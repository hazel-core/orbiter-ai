# Ralph Progress Log
Started: Mon Feb 17 2026

## Codebase Patterns
- New packages follow hatchling pattern: `packages = ["src/orbiter"]` in pyproject.toml, no top-level `orbiter/__init__.py` needed (orbiter-core's `pkgutil.extend_path()` handles namespace)
- Add new packages to root pyproject.toml in 3 places: `[tool.uv.workspace] members`, `[dependency-groups] dev`, and `[tool.uv.sources]`
- Streaming event types are frozen Pydantic BaseModels in `packages/orbiter-core/src/orbiter/types.py`
- Use `model_config = {"frozen": True}` for all event types
- `StreamEvent` is a Union type alias at the bottom of `types.py` — all new event types must be added to it
- Existing `TextEvent` and `ToolCallEvent` use `agent_name: str = ""` as default
- The stale `test_streaming_events.py` was rewritten in US-002 with correct PRD field names — future event types (US-003) should add tests to this file
- `_stream()` in `runner.py` uses `detailed=True` to enable rich events — when adding Swarm streaming (US-005), pass `detailed` through to the per-agent stream call
- `StreamChunk` has a `usage` field (populated on final chunk with `total_tokens > 0`) — use this to build `UsageEvent`
- `agent._execute_tools()` wraps errors with "Tool 'X' failed: " prefix — test error messages with `in` not `==`
- Stream tests use `_FakeStreamChunk`, `_FakeToolCallDelta`, `_make_stream_provider` helpers in `test_runner.py` (also duplicated in `test_swarm.py`)
- Agent model string is at `agent.model` (may be empty string or None) — use `getattr(agent, "model", "") or ""`
- Swarm streaming collects text from `TextEvent` objects for output chaining — avoids double LLM execution
- `run.stream()` detects Swarm via `hasattr(agent, 'flow_order')` and delegates to `Swarm.stream()`, same pattern as `run()`
- Swarm `stream()` imports `run` inside methods to avoid circular imports (runner.py → swarm.py cycle)
- Event filtering via `event_types: set[str] | None` — Swarm workflow/handoff must filter at yield points (not inner `run.stream()`) to preserve TextEvent collection for output chaining
- Distributed models use `StrEnum` (not `str, Enum`) per ruff UP042, and frozen Pydantic BaseModels with `Field(default_factory=...)` for mutable defaults
- Distributed package exports from `orbiter.distributed.__init__` — update `__all__` when adding new public types
- Use `# pyright: ignore[reportMissingImports]` on all `from orbiter.distributed.*` imports in source and test files — pyright can't resolve namespace packages across workspace members (same pattern as orbiter-observability)
- fakeredis.aioredis.FakeRedis(decode_responses=True) for Redis integration tests — supports Streams (XADD/XREADGROUP/XACK/XRANGE)
- TaskBroker uses Redis Streams consumer groups: `{queue_name}:group` as group name, worker_id as consumer name, `>` for new messages
- TaskBroker._pending_ids maps task_id → stream message_id for ack/nack correlation
- Redis hash/set async operations (HSET, HGETALL, EXPIRE, SADD, SMEMBERS) need `# type: ignore[misc]` for pyright — type stubs don't properly annotate these as awaitable (unlike Streams operations which work fine)
- Agent/Swarm serialization: `to_dict()`/`from_dict()` in agent.py/swarm.py — tools as dotted paths, handoffs recursive, non-serializable items (closures, hooks, memory, context) raise ValueError
- Tool serialization helpers: `_serialize_tool()`, `_deserialize_tool()`, `_import_object()` in agent.py — reuse these for any import-by-dotted-path needs
- `_EVENT_TYPE_MAP` in `events.py` maps type discriminator → event class for deserialization; `_deserialize_event()` reconstructs `StreamEvent` from JSON dict
- EventPublisher publishes to both Pub/Sub (`orbiter:events:{task_id}`) and Stream (`orbiter:stream:{task_id}`) — Pub/Sub for live, Stream for replay
- EventSubscriber `subscribe()` terminates on terminal StatusEvent (completed/error/cancelled)
- Worker imports `Agent`, `Swarm`, `run`, `TextEvent` inside methods (local imports) to avoid circular deps and pyright issues — patch at source (`orbiter.agent.Agent`, `orbiter.swarm.Swarm`, `orbiter.runner.run`) not at `orbiter.distributed.worker.X` in tests
- Worker heartbeat uses a separate Redis connection — published to `orbiter:workers:{worker_id}` hash with TTL
- `contextlib.suppress(asyncio.CancelledError)` preferred over try/except/pass per ruff SIM105
- `distributed()` in client.py is the main public API — connects broker/store/subscriber, submits TaskPayload, returns TaskHandle
- `TaskHandle` wraps all distributed components (broker, store, subscriber) — provides result(), stream(), cancel(), status() methods
- CLI subcommand groups: `sub = typer.Typer()` + `app.add_typer(sub, name="name")` — lazy-import heavy deps inside command functions
- CLI Redis URL resolution: use `_resolve_redis_url()` helper — checks `--redis-url` flag then `ORBITER_REDIS_URL` env var, prints error and exits on missing
- Rich tables available via typer dependency — use `from rich.table import Table` without extra installs
- Worker health data is at `orbiter:workers:{worker_id}` Redis hash (set by heartbeat loop) — read via HGETALL, scan via `SCAN match=orbiter:workers:*`
- `HealthCheck` protocol from `orbiter.observability.health` requires sync `check()` method — use sync `redis.from_url()` (not async) inside implementations
- Tests checking in-memory metrics collector must `patch("module.HAS_OTEL", False)` since OTel is installed in dev — use autouse fixture with `yield` for module-wide patching
- Distributed metric constants live in `orbiter.observability.semconv` (prefixed `METRIC_DIST_*` and `DIST_*`), recording helpers live in `orbiter.distributed.metrics`
- Streaming event metric constants also in `semconv.py` (prefixed `METRIC_STREAM_*` and `STREAM_*`), recording in `events.py` and `runner.py`
- `time.monotonic()` for measuring durations (not `time.time()`) — monotonic clock can't go backwards
- Event counting in `_stream()` is integrated into `_passes_filter()` — avoids modifying every yield point
- Optional deps use `try/except ImportError` with `HAS_X` flag pattern (e.g., `HAS_TEMPORAL` in temporal.py, `HAS_OTEL` in observability) — guard constructors with `if not HAS_X: raise ImportError(msg)`
- `temporalio` imports need `# pyright: ignore[reportMissingImports]` since pyright can't resolve optional deps in try/except blocks
- For multi-line imports with `# pyright: ignore`, put the comment on the `from` line (not the imported name line) — ruff I001 reformats single-name imports to multi-line
- Worker `executor` param: `Literal["local", "temporal"]` — `"local"` for direct execution, `"temporal"` delegates to `TemporalExecutor`
- Temporal workflow/activity pattern: `@workflow.defn` class with `@workflow.run` method, `@activity.defn` async function with `activity.heartbeat()` for liveness

---

## 2026-02-17 - US-001
- Added `StepEvent` frozen Pydantic BaseModel to `orbiter/types.py`
- Fields: `type: Literal["step"]`, `step_number: int`, `agent_name: str`, `status: Literal["started", "completed"]`, `started_at: float`, `completed_at: float | None`, `usage: Usage | None`
- Updated `StreamEvent` union to include `StepEvent`
- Files changed: `packages/orbiter-core/src/orbiter/types.py`
- **Learnings for future iterations:**
  - Follow the exact frozen BaseModel pattern: `model_config = {"frozen": True}`, type literal with default, required fields first, optional fields with defaults
  - `StreamEvent` union is a simple type alias (not Annotated/Discriminated) — just add `| NewType` at the end
  - There's a pre-existing `packages/orbiter-core/tests/test_streaming_events.py` that uses wrong field names (e.g., `step_num` instead of `step_number`). It needs to be rewritten to match PRD specs in a later story.
  - All 682 existing orbiter-core tests pass with the change — no downstream breakage
---

## 2026-02-17 - US-002
- Added `ToolResultEvent` frozen Pydantic BaseModel to `orbiter/types.py`
- Fields: `type: Literal["tool_result"]`, `tool_name: str`, `tool_call_id: str`, `arguments: dict[str, Any]`, `result: str`, `error: str | None`, `success: bool`, `duration_ms: float`, `agent_name: str`
- Updated `StreamEvent` union to include `ToolResultEvent`
- Rewrote stale `test_streaming_events.py` with correct PRD field names (11 tests)
- Files changed: `packages/orbiter-core/src/orbiter/types.py`, `packages/orbiter-core/tests/test_streaming_events.py`
- **Learnings for future iterations:**
  - The stale test file has been rewritten — US-003 should add new test classes to `test_streaming_events.py` for ReasoningEvent, ErrorEvent, StatusEvent, UsageEvent
  - Required fields for ToolResultEvent are only `tool_name` and `tool_call_id` — all others have sensible defaults
  - `arguments` uses `Field(default_factory=dict)` to avoid mutable default sharing
  - All 693 orbiter-core tests pass (11 new + 682 existing)
---

## 2026-02-17 - US-003
- Added four new streaming event types to `orbiter/types.py`: `ReasoningEvent`, `ErrorEvent`, `StatusEvent`, `UsageEvent`
- `ReasoningEvent`: `type: Literal["reasoning"]`, `text: str`, `agent_name: str`
- `ErrorEvent`: `type: Literal["error"]`, `error: str`, `error_type: str`, `agent_name: str`, `step_number: int | None`, `recoverable: bool`
- `StatusEvent`: `type: Literal["status"]`, `status: Literal["starting", "running", "waiting_for_tool", "completed", "cancelled", "error"]`, `agent_name: str`, `message: str`
- `UsageEvent`: `type: Literal["usage"]`, `usage: Usage`, `agent_name: str`, `step_number: int`, `model: str`
- Updated `StreamEvent` union to include all 8 event types
- Added 24 new tests across 4 test classes + updated union test class (32 total in file)
- Files changed: `packages/orbiter-core/src/orbiter/types.py`, `packages/orbiter-core/tests/test_streaming_events.py`
- **Learnings for future iterations:**
  - `StatusEvent.status` uses a Literal union with 6 values — invalid values correctly rejected by Pydantic validation
  - `UsageEvent` embeds the existing `Usage` model — `.model_dump()` correctly nests it as a dict
  - All required fields for new types: `ReasoningEvent(text)`, `ErrorEvent(error, error_type)`, `StatusEvent(status)`, `UsageEvent(usage)` — rest have defaults
  - Total orbiter-core tests now: 714 (32 streaming + 682 existing)
---

## 2026-02-17 - US-004
- Added `detailed=True` parameter to `_stream()` in `runner.py`
- When `detailed=False` (default): only `TextEvent` and `ToolCallEvent` emitted (backward compatible)
- When `detailed=True`: emits `StatusEvent('starting')` at start, `StepEvent(status='started')` before each LLM call, `UsageEvent` after each LLM call, `ToolCallEvent` for each tool call, `ToolResultEvent` after each tool execution, `StepEvent(status='completed')` at step end, `StatusEvent('completed')` at finish
- `ErrorEvent` emitted on errors regardless of `detailed` flag, re-raises after yielding
- When `detailed=True` and error occurs, also emits `StatusEvent(status='error')`
- Added 18 new tests across 6 test classes: `TestRunStreamDetailedFalse` (2), `TestRunStreamDetailedText` (2), `TestRunStreamDetailedToolCalls` (4), `TestRunStreamDetailedUsage` (2), `TestRunStreamDetailedErrors` (3), `TestRunStreamDetailedStepNumbers` (1)
- Files changed: `packages/orbiter-core/src/orbiter/runner.py`, `packages/orbiter-core/tests/test_runner.py`
- **Learnings for future iterations:**
  - `StreamChunk.usage` is a `Usage` object (not None) — check `total_tokens > 0` to detect actual usage data
  - Usage is typically only on the final chunk — capture it during the stream loop and emit `UsageEvent` after the loop ends
  - Tools execute in parallel via `asyncio.TaskGroup()` — individual tool timing isn't available, so duration is split evenly across tools
  - Error handling in `_stream()` wraps each step in try/except — yields `ErrorEvent` then re-raises to let caller handle
  - `agent.model` attribute holds the model string for `UsageEvent.model` field
  - Total orbiter-core tests now: 728 (37 runner + 32 streaming + 659 other)
---

## 2026-02-17 - US-005
- Added `stream()` method to `Swarm` class with streaming variants for all three modes: workflow, handoff, and team
- Workflow streaming: agents run sequentially, text is collected from `TextEvent` objects for output→input chaining (avoids double execution)
- Handoff streaming: agents delegate dynamically, text collected from `TextEvent` for handoff detection via lightweight `RunResult`
- Team streaming: lead agent streams with delegate tools added temporarily, tools restored after execution (including on error)
- `StatusEvent(status='running')` emitted for each agent transition (workflow steps, handoff transitions, team lead start)
- Handoff streaming emits `StatusEvent` with "Handoff from 'X' to 'Y'" message for agent transitions
- Updated `_stream()` in `runner.py` to detect Swarm via `hasattr(agent, 'flow_order')` and delegate to `Swarm.stream()`
- All events include the correct `agent_name` of the sub-agent that produced them
- Added 14 new test cases across 5 test classes: `TestSwarmStreamWorkflow` (4), `TestSwarmStreamHandoff` (4), `TestSwarmStreamTeam` (4), `TestSwarmStreamUnsupportedMode` (1), `TestSwarmStreamViaRun` (1)
- Files changed: `packages/orbiter-core/src/orbiter/swarm.py`, `packages/orbiter-core/src/orbiter/runner.py`, `packages/orbiter-core/tests/test_swarm.py`
- **Learnings for future iterations:**
  - Key design: collect text from `TextEvent` objects during streaming to avoid double LLM execution (no need for separate non-streaming `call_runner` call)
  - `_detect_handoff()` works with a lightweight `RunResult(output=text)` — just needs `.output` for string matching
  - Swarm's `stream()` uses `from orbiter.runner import run` inside methods to avoid circular imports (runner.py imports from swarm indirectly)
  - Team mode streaming must restore lead tools in a `try/finally` block, same as the non-streaming `_run_team()`
  - Stream test helpers (`_FakeStreamChunk`, `_FakeToolCallDelta`, `_make_stream_provider`) duplicated from `test_runner.py` — consider extracting to a shared conftest.py if more tests need them
  - Total orbiter-core tests now: 742 (37 runner + 32 streaming + 62 swarm + 611 other)
---

## 2026-02-17 - US-006
- Added `event_types: set[str] | None` parameter to `_stream()` in `runner.py`
- When `event_types` is provided, only events whose `type` field matches are yielded
- When `event_types` is `None` (default), all events pass through (respecting `detailed` flag)
- Added `event_types` parameter to `Swarm.stream()` and all three mode methods (`_stream_workflow`, `_stream_handoff`, `_stream_team`)
- Swarm workflow/handoff modes: filtering applied at yield points while still collecting TextEvent text for output chaining (inner `run.stream()` called unfiltered)
- Swarm team mode: `event_types` passed through to inner `run.stream()` (no text collection needed)
- Added 8 new tests in `TestRunStreamEventFiltering` class covering: single type filter, tool_result filter, multiple types, None passthrough, empty set, without detailed, error filtering, error inclusion
- Files changed: `packages/orbiter-core/src/orbiter/runner.py`, `packages/orbiter-core/src/orbiter/swarm.py`, `packages/orbiter-core/tests/test_runner.py`
- **Learnings for future iterations:**
  - Swarm workflow/handoff modes need unfiltered access to TextEvents for output chaining — don't pass `event_types` to inner `run.stream()`, filter at the Swarm level instead
  - Swarm team mode doesn't collect text, so it's safe to pass `event_types` through to inner `run.stream()`
  - All event types have a `.type` string attribute (e.g., `"text"`, `"tool_call"`, `"status"`) which is the discriminator used for filtering
  - The `_passes_filter` helper pattern: `event_types is None or event.type in event_types` is clean and reusable
  - Total orbiter-core tests now: 750 (45 runner + 32 streaming + 62 swarm + 611 other)
---

## 2026-02-17 - US-007
- Created `orbiter-distributed` package scaffold at `packages/orbiter-distributed/`
- `pyproject.toml` follows hatchling pattern from orbiter-observability: `packages = ["src/orbiter"]`
- Dependencies: `orbiter-core`, `redis[hiredis]>=5.0`
- Optional deps: `[temporal]` with `temporalio>=1.7`, `[test]` with `pytest`, `pytest-asyncio`, `fakeredis[lua]`
- Created `src/orbiter/distributed/__init__.py` with `__all__` placeholder
- No separate `orbiter/__init__.py` needed — orbiter-core's `pkgutil.extend_path()` handles namespace sharing
- Created `README.md` placeholder (required by hatchling)
- Added to root `pyproject.toml`: workspace member, dev dependency, uv source, and `fakeredis[lua]` to dev deps
- Created `tests/` directory for future test files
- Package installs cleanly with `uv sync`, pyright passes, ruff passes, all 750 existing tests pass
- Files changed: `packages/orbiter-distributed/pyproject.toml`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/README.md`, `pyproject.toml`
- **Learnings for future iterations:**
  - Namespace packages in this monorepo work via hatchling's `packages = ["src/orbiter"]` — no top-level `orbiter/__init__.py` needed in sub-packages
  - `orbiter-core`'s `orbiter/__init__.py` has `pkgutil.extend_path()` which enables namespace package splitting across packages
  - Root `pyproject.toml` needs updates in 3 sections when adding a new package: `[tool.uv.workspace] members`, `[dependency-groups] dev`, `[tool.uv.sources]`
  - `fakeredis[lua]` added to root dev deps since it's needed for Redis Stream tests (Streams use Lua scripting internally)
---

## 2026-02-17 - US-009
- Created `TaskPayload`, `TaskResult`, and `TaskStatus` models in `orbiter/distributed/models.py`
- `TaskStatus` is a `StrEnum` with 6 values: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, RETRYING
- `TaskPayload` is a frozen Pydantic BaseModel with auto-generated `task_id` (uuid4().hex), `agent_config`, `input`, `messages`, `model`, `detailed`, `metadata`, `created_at`, `timeout_seconds`
- `TaskResult` is a frozen Pydantic BaseModel with `task_id`, `status`, `result`, `error`, `started_at`, `completed_at`, `worker_id`, `retries`
- Updated `__init__.py` to export `TaskPayload`, `TaskResult`, `TaskStatus`
- Added 15 tests covering defaults, custom fields, frozen enforcement, JSON serialization round-trips, and all statuses
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/models.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_models.py`
- **Learnings for future iterations:**
  - Use `StrEnum` (not `str, Enum`) per ruff UP042 rule
  - `uuid4().hex` gives a 32-char hex string without dashes — good for task IDs
  - `Field(default_factory=dict)` and `Field(default_factory=list)` for mutable defaults in frozen models
  - All distributed models are in `orbiter.distributed.models` and re-exported from `orbiter.distributed.__init__`
  - Tests at `packages/orbiter-distributed/tests/test_models.py` — use `uv run pytest` to execute
  - Total tests: 765 (15 new distributed + 750 existing)
---

## 2026-02-17 - US-008
- Created `TaskBroker` class in `orbiter/distributed/broker.py` backed by Redis Streams
- Constructor: `TaskBroker(redis_url, *, queue_name='orbiter:tasks', max_retries=3)`
- `connect()` creates Redis client via `redis.asyncio.from_url()` and ensures consumer group exists (XGROUP CREATE with MKSTREAM)
- `disconnect()` closes the Redis connection
- `submit(task)` serializes TaskPayload to JSON and adds to stream via XADD, returns task_id
- `claim(worker_id, *, timeout=5.0)` uses XREADGROUP with blocking to pop next task from consumer group
- `ack(task_id)` acknowledges processing via XACK
- `nack(task_id)` reads original message via XRANGE, acks it, then re-adds to stream for retry by any consumer
- Updated `__init__.py` to export `TaskBroker`
- Added `# pyright: ignore[reportMissingImports]` to all distributed package cross-imports (source + tests) following orbiter-observability pattern
- Added 18 tests across 3 test classes: `TestTaskBrokerInit` (3), `TestTaskBrokerConnect` (4), `TestTaskBrokerWithFakeRedis` (11)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/broker.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_broker.py`, `packages/orbiter-distributed/tests/test_models.py`
- **Learnings for future iterations:**
  - Use `# pyright: ignore[reportMissingImports]` for all `orbiter.distributed.*` imports — pyright can't resolve namespace packages via .pth editable installs
  - `fakeredis.aioredis.FakeRedis(decode_responses=True)` provides full Redis Streams support for integration tests
  - Redis Streams consumer group pattern: XADD for submit, XREADGROUP with `>` for new messages, XACK for acknowledgment
  - For nack/retry: XRANGE to read original message → XACK to remove from PEL → XADD to re-enqueue
  - `_pending_ids` dict maps task_id → stream message_id for ack/nack correlation — must be initialized in `__init__`
  - `aioredis.ResponseError` with "BUSYGROUP" indicates consumer group already exists — safe to ignore on reconnect
  - Total tests: 783 (33 distributed + 750 existing)
---

## 2026-02-17 - US-010
- Created `TaskStore` class in `orbiter/distributed/store.py` backed by Redis hashes
- Constructor: `TaskStore(redis_url, *, prefix='orbiter:task:', ttl_seconds=86400)`
- `connect()` creates Redis client via `redis.asyncio.from_url()`
- `disconnect()` closes the Redis connection
- `set_status(task_id, status, **kwargs)` stores task state as Redis hash with TTL, maintains secondary index via SADD
- `get_status(task_id)` retrieves task state via HGETALL and parses into `TaskResult`
- `list_tasks(status=None, limit=100)` uses SMEMBERS on secondary index set, filters by status
- `_parse_result()` handles type conversion from Redis strings (floats, ints, JSON-encoded dicts, None via empty string)
- Updated `__init__.py` to export `TaskStore`
- Added 16 tests across 2 test classes: `TestTaskStoreInit` (3), `TestTaskStoreWithFakeRedis` (13)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/store.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_store.py`
- **Learnings for future iterations:**
  - Redis hash operations (HSET, HGETALL, EXPIRE) and set operations (SADD, SMEMBERS) need `# type: ignore[misc]` for pyright — the redis async type stubs don't properly annotate these as awaitable
  - Secondary index pattern: SADD `{prefix}index` task_id on every `set_status()`, SMEMBERS for `list_tasks()` — simple but effective for listing
  - Redis hashes store everything as strings — need type conversion in `_parse_result()`: empty string → None, JSON.loads for dicts, float() for timestamps, int() for retries
  - `set_status()` uses HSET (not HMSET) with `mapping=` param — HSET with mapping is the modern Redis way
  - Calling `set_status()` multiple times preserves existing fields (HSET merges, doesn't replace the whole hash)
  - Total tests: 799 (49 distributed + 750 existing)
---

## 2026-02-17 - US-015
- Added `to_dict()` and `from_dict()` methods to `Agent` class for serialization/deserialization
- Added `to_dict()` and `from_dict()` methods to `Swarm` class for serialization/deserialization
- Tools serialized as importable dotted paths (`module.qualname`) — FunctionTool uses wrapped `_fn`, custom Tool subclasses use the class path
- Deserialization resolves paths via `importlib.import_module()` + `getattr()` with progressive fallback
- Non-serializable components raise `ValueError`: callable instructions, hooks, memory, context, closures/lambdas
- Handoffs serialized/deserialized recursively
- `output_type` (Pydantic BaseModel subclass) serialized as importable dotted path
- Added 36 tests across 9 test classes covering: to_dict, from_dict, round-trips (including JSON), error cases, Swarm serialization, and helper functions
- Files changed: `packages/orbiter-core/src/orbiter/agent.py`, `packages/orbiter-core/src/orbiter/swarm.py`, `packages/orbiter-core/tests/test_serialization.py`
- **Learnings for future iterations:**
  - `FunctionTool._fn` gives access to the wrapped function for `__module__` and `__qualname__` extraction
  - Closures/lambdas have `<` in their `__qualname__` (e.g., `make_tool.<locals>.my_closure`) — use this to detect non-serializable tools
  - Custom `Tool` subclasses serialize via their class path, not function path — `_deserialize_tool` checks `isinstance(obj, Tool)` first, then `issubclass(obj, Tool)` for classes
  - `_import_object()` tries `rsplit('.', 1)` first, then progressively shorter module paths for nested attributes
  - Agent hooks check uses `any(self.hook_manager.has_hooks(hp) for hp in HookPoint)` — iterates all hook points
  - Swarm `to_dict()` is thin: just agents list + flow/mode/max_handoffs; `from_dict()` delegates agent reconstruction to `Agent.from_dict()`
  - Total tests: 835 (49 distributed + 786 orbiter-core)
---

## 2026-02-17 - US-011
- Created `EventPublisher` and `EventSubscriber` classes in `orbiter/distributed/events.py`
- `EventPublisher` publishes streaming events to both Redis Pub/Sub (live) and Redis Streams (persistent replay)
- `publish(task_id, event)` serializes event as JSON, publishes to `orbiter:events:{task_id}` Pub/Sub channel and appends to `orbiter:stream:{task_id}` Stream with configurable TTL
- `EventSubscriber` provides two consumption modes:
  - `subscribe(task_id)` — live Pub/Sub `AsyncIterator[StreamEvent]`, terminates on terminal StatusEvent (completed/error/cancelled)
  - `replay(task_id, from_id="0")` — reads persisted events from Redis Stream
- Discriminated union deserialization via `_deserialize_event()` using `_EVENT_TYPE_MAP` dict mapping type string → concrete class
- `_deserialize_event()` raises `ValueError` for unknown event types
- Stream TTL configurable (default 3600s = 1 hour), set via EXPIRE on each publish
- Updated `__init__.py` to export `EventPublisher` and `EventSubscriber`
- Added 30 tests across 7 test classes: `TestDeserializeEvent` (10), `TestEventPublisherInit` (3), `TestEventPublisherConnect` (2), `TestEventPublisherWithFakeRedis` (5), `TestEventSubscriberInit` (2), `TestEventSubscriberReplay` (5), `TestEventSubscriberSubscribe` (3)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/events.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_events.py`
- **Learnings for future iterations:**
  - `_EVENT_TYPE_MAP` dict maps type discriminator strings to concrete event classes — extend this if new event types are added
  - Redis Pub/Sub via `r.pubsub()` + `subscribe()` + `get_message(ignore_subscribe_messages=True)` for async listening; must `unsubscribe()` + `aclose()` in finally block
  - `r.publish()` and `r.expire()` need `# type: ignore[misc]` for pyright (same as HSET/SADD pattern in store.py)
  - fakeredis supports Pub/Sub — shared FakeRedis instance between publisher and subscriber in tests enables end-to-end Pub/Sub testing with `asyncio.gather(consumer, producer)`
  - `subscribe()` terminates on terminal StatusEvent (completed/error/cancelled) — callers don't need to manually stop iteration
  - `replay()` is a simple XRANGE read — no consumer groups needed since it's read-only replay
  - Pub/Sub `get_message()` returns None when no message available — need `await asyncio.sleep(0.01)` to avoid busy-loop
  - Total tests: 865 (79 distributed + 786 orbiter-core)
---

## 2026-02-17 - US-012
- Created `Worker` class in `orbiter/distributed/worker.py` — the core task execution loop
- Constructor: `Worker(redis_url, *, worker_id=None, concurrency=1, queue_name='orbiter:tasks', heartbeat_ttl=30)`
- `start()` connects broker/store/publisher, registers SIGINT/SIGTERM handlers, runs concurrent claim loops + heartbeat
- `stop()` signals graceful shutdown via `asyncio.Event`
- `_claim_loop()` claims tasks with 2s timeout and delegates to `_execute_task()`
- `_execute_task()` lifecycle: set RUNNING → reconstruct agent → run.stream() → publish events → set COMPLETED/FAILED → ack/nack
- `_reconstruct_agent()` detects Agent vs Swarm via `"agents"` key in config dict
- `_run_agent()` streams with `detailed=task.detailed`, publishes each event, collects TextEvent text for result
- Heartbeat: separate Redis connection publishes worker health to `orbiter:workers:{worker_id}` hash with TTL every `heartbeat_ttl/3` seconds
- Retry logic: on failure checks retries vs max_retries — nacks if retries remain, acks if exhausted
- Auto-generated worker_id: `{hostname}-{pid}-{random_hex}`
- Updated `__init__.py` to export `Worker`
- Added 17 tests across 8 test classes: `TestGenerateWorkerId` (2), `TestWorkerInit` (3), `TestWorkerReconstructAgent` (2), `TestWorkerExecuteTask` (4), `TestWorkerRunAgent` (2), `TestWorkerClaimLoop` (2), `TestWorkerStop` (1), `TestWorkerStart` (1)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/worker.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_worker.py`
- **Learnings for future iterations:**
  - Worker uses local imports for `Agent`, `Swarm`, `run`, `TextEvent` inside methods — avoids circular imports and pyright issues across namespace packages
  - When testing local-import code, patch at the real source module (`orbiter.agent.Agent`, `orbiter.runner.run`) not at `orbiter.distributed.worker.X` — the module-level attribute doesn't exist for local imports
  - `contextlib.suppress(asyncio.CancelledError)` is required by ruff SIM105 instead of try/except/pass
  - `random.randbytes(4).hex()` gives 8-char random hex — good for worker ID suffix
  - Heartbeat uses a separate Redis connection from broker/store to avoid contention
  - `asyncio.Event` for shutdown signaling — checked in `_claim_loop()` while-condition
  - Signal handlers registered via `loop.add_signal_handler()` — must be patched in tests since pytest doesn't support real signal handlers
  - Cancel listener uses separate Redis connection + Pub/Sub on `orbiter:cancel:{task_id}` — sets `CancellationToken.cancelled = True` on signal
  - Worker `_execute_task` creates a cancel listener task per active task — cancelled in `finally` block via `task.cancel()` + `contextlib.suppress(asyncio.CancelledError)`
  - `_run_agent` checks `token.cancelled` between event yields (cooperative cancellation) — emits `StatusEvent(status='cancelled')` and breaks
  - `TaskBroker.cancel()` publishes to Pub/Sub and directly updates task hash status to CANCELLED (uses `orbiter:task:` prefix convention from TaskStore)
  - Total tests: 894 (108 distributed + 786 orbiter-core)
---

## 2026-02-17 - US-013
- Created `CancellationToken` class in `orbiter/distributed/cancel.py` with `cancelled` property and `cancel()` method
- Added `cancel(task_id)` method to `TaskBroker` in `broker.py` — publishes cancel signal to `orbiter:cancel:{task_id}` Pub/Sub and sets task status to CANCELLED in Redis hash
- Updated `Worker._execute_task()` to create a `_listen_for_cancel()` background task per active task — subscribes to Pub/Sub cancel channel and sets token on signal
- Updated `Worker._run_agent()` to accept `CancellationToken` — checks `token.cancelled` between event yields, emits `StatusEvent(status='cancelled')` and breaks when cancelled
- Worker sets task status to CANCELLED (not COMPLETED) when token is cancelled
- Cancel listener uses separate Redis connection (like heartbeat) to avoid contention
- Updated existing `test_worker.py` tests for new `_run_agent(agent, task, token)` signature and `_listen_for_cancel` patching
- Exported `CancellationToken` from `orbiter.distributed.__init__`
- Added 12 new tests across 5 test classes: `TestCancellationToken` (3), `TestTaskBrokerCancel` (3), `TestWorkerListenForCancel` (2), `TestWorkerRunAgentCancellation` (2), `TestWorkerExecuteTaskCancellation` (2)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/cancel.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/broker.py`, `packages/orbiter-distributed/src/orbiter/distributed/worker.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_cancel.py` (new), `packages/orbiter-distributed/tests/test_worker.py`
- **Learnings for future iterations:**
  - Cancel listener uses separate Redis connection + `r.pubsub()` — `pubsub()` is a sync method on Redis (returns PubSub object), not async
  - When testing with mock Redis, use `MagicMock` for `pubsub` method (sync) and `AsyncMock` for the PubSub object methods (subscribe, get_message, unsubscribe, aclose are async)
  - `TaskBroker.cancel()` directly updates the Redis hash using TaskStore's key convention (`orbiter:task:{task_id}`) — avoids needing TaskStore dependency in broker
  - Worker `_execute_task` always cancels the listener task in `finally` block — important for cleanup even on exceptions
  - Cooperative cancellation: token checked between event yields in `_run_agent`, not between steps — gives finer-grained cancellation response
  - When adding params to `_run_agent`, update ALL existing tests that mock it (both in test_worker.py and test_cancel.py)
  - Total tests: 894 (108 distributed + 786 orbiter-core)
---

## 2026-02-17 - US-014
- Created `distributed()` function and `TaskHandle` class in `orbiter/distributed/client.py`
- `distributed(agent, input, *, messages, redis_url, detailed, timeout, metadata)` serializes agent via `to_dict()`, creates `TaskPayload`, submits via `TaskBroker`, returns `TaskHandle`
- `TaskHandle` wraps `TaskBroker`, `TaskStore`, `EventSubscriber` with methods:
  - `task_id` property — unique task identifier
  - `result(poll_interval=0.5)` — polls TaskStore until terminal status, returns result dict or raises RuntimeError
  - `stream()` — yields live `StreamEvent` via `EventSubscriber.subscribe()`
  - `cancel()` — delegates to `TaskBroker.cancel()`
  - `status()` — returns current `TaskResult` from `TaskStore`
- `redis_url` defaults to `ORBITER_REDIS_URL` environment variable; raises `ValueError` if neither provided
- Works with both `Agent` and `Swarm` instances (both have `to_dict()`)
- Updated `__init__.py` to export `TaskHandle` and `distributed`
- Added 18 tests across 7 test classes: `TestTaskHandleInit` (2), `TestTaskHandleStatus` (2), `TestTaskHandleCancel` (1), `TestTaskHandleResult` (5), `TestTaskHandleStream` (1), `TestDistributed` (7)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/client.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_client.py` (new)
- **Learnings for future iterations:**
  - `distributed()` connects broker/store/subscriber before submitting — all three must be connected for the TaskHandle to work
  - `TaskHandle.result()` uses polling with `asyncio.sleep(poll_interval)` — terminal statuses are COMPLETED, FAILED, CANCELLED
  - On FAILED: `RuntimeError` message includes the `error` field from `TaskResult`; on CANCELLED: simpler "was cancelled" message
  - `result()` returns empty dict `{}` when `TaskResult.result` is None (e.g., completed without explicit result)
  - `patch.dict("os.environ", {}, clear=True)` in tests to ensure env var fallback works correctly
  - Ruff SIM117 requires combining nested `with` statements — `patch.dict` + `pytest.raises` can be combined with `,` inside `with()`
  - Ruff doesn't flag `A002` (builtin shadowing) for `input` parameter — the `# noqa: A002` is unnecessary
  - Total tests: 912 (126 distributed + 786 orbiter-core)
---

## 2026-02-17 - US-016
- Added `orbiter start worker` CLI command to `orbiter_cli/main.py` using Typer subcommand group
- `start_app = typer.Typer()` + `app.add_typer(start_app, name="start")` for subcommand group
- `start_worker` command with options: `--redis-url` (default: ORBITER_REDIS_URL env var), `--concurrency` (default: 1), `--queue` (default: orbiter:tasks), `--worker-id` (auto-generated)
- Startup banner shows: Worker ID, masked Redis URL (host + port only, password hidden), queue name, concurrency
- `_mask_redis_url()` helper parses URL with `urllib.parse.urlparse()` and shows only host:port
- Uses `asyncio.run(worker.start())` to run the Worker event loop
- Worker class lazily imported from `orbiter.distributed.worker` inside command function
- Added `orbiter-distributed` as dependency in `orbiter-cli/pyproject.toml` (both `dependencies` and `[tool.uv.sources]`)
- Added 11 new tests across 2 test classes: `TestMaskRedisUrl` (4), `TestStartWorkerCommand` (7)
- Files changed: `packages/orbiter-cli/src/orbiter_cli/main.py`, `packages/orbiter-cli/pyproject.toml`, `packages/orbiter-cli/tests/test_cli_main.py`, `uv.lock`
- **Learnings for future iterations:**
  - Typer subcommand groups: `sub = typer.Typer()` + `app.add_typer(sub, name="name")` — commands added via `@sub.command("cmd")`
  - Worker import is lazy (inside command function) to avoid loading Redis/distributed deps when using other CLI commands
  - `urlparse()` from `urllib.parse` is reliable for masking — `parsed.hostname` and `parsed.port` extract cleanly
  - Tests mock `orbiter.distributed.worker.Worker` — the import path in `patch()` matches the actual module path, not the lazy import location
  - `CliRunner` from `typer.testing` captures Rich console output — search for plain text in `result.output`, Rich markup is stripped
  - `monkeypatch.delenv("ORBITER_REDIS_URL", raising=False)` for clean env in tests — `raising=False` prevents error if var not set
  - Total tests: 1154 (44 CLI + 126 distributed + 786 orbiter-core + others)
---

## 2026-02-17 - US-017
- Added `task` subcommand group to `orbiter_cli/main.py` with three commands: `status`, `cancel`, `list`
- `orbiter task status <task_id>` shows task details: status (color-coded), worker ID, started/completed timestamps, duration, retries, error, and result preview (truncated at 200 chars)
- `orbiter task cancel <task_id>` cancels a running task via `TaskBroker.cancel()`
- `orbiter task list` shows a Rich table of recent tasks with columns: Task ID, Status, Worker, Started, Duration
- `--status` filter validates against `TaskStatus` enum values, shows error with valid values on invalid input
- `--limit` option controls max tasks displayed (default 100)
- All three commands accept `--redis-url` or fall back to `ORBITER_REDIS_URL` env var via shared `_resolve_redis_url()` helper
- Helper functions: `_format_timestamp()` (UTC), `_format_duration()` (ms/s/m), `_status_color()` (Rich color per status)
- Lazy imports for `TaskStore`, `TaskBroker`, `TaskStatus` inside command functions
- Added 29 new tests across 7 test classes: `TestFormatTimestamp` (2), `TestFormatDuration` (4), `TestStatusColor` (2), `TestTaskCommandRegistered` (4), `TestTaskStatusCommand` (6), `TestTaskCancelCommand` (3), `TestTaskListCommand` (7)
- Files changed: `packages/orbiter-cli/src/orbiter_cli/main.py`, `packages/orbiter-cli/tests/test_cli_main.py`
- **Learnings for future iterations:**
  - `Rich.Table` is included via typer dependency — no additional install needed
  - `datetime.UTC` alias preferred over `datetime.timezone.utc` per ruff UP017
  - `raise typer.Exit(code=1) from err` required in except blocks per ruff B904
  - Lazy imports inside async inner functions work well for CLI commands — avoids loading heavy Redis/distributed deps at import time
  - `TaskStore` and `TaskBroker` need `connect()`/`disconnect()` lifecycle — use try/finally in async inner functions
  - `_resolve_redis_url()` extracted as shared helper for redis URL resolution across task commands — reuse for future commands
  - Tests mock at the real module path (`orbiter.distributed.store.TaskStore`, `orbiter.distributed.broker.TaskBroker`) for lazy imports
  - Total tests: 1172 (73 CLI + 126 distributed + 786 orbiter-core + others)
---

## 2026-02-17 - US-019
- Created `WorkerHealth` frozen dataclass in `orbiter/distributed/health.py` with fields: worker_id, status, tasks_processed, tasks_failed, current_task_id, started_at, last_heartbeat, concurrency, hostname, alive
- Created `WorkerHealthCheck` class implementing `orbiter.observability.health.HealthCheck` protocol — reads worker heartbeat hash from Redis via sync `redis.from_url()`, reports HEALTHY/DEGRADED/UNHEALTHY based on heartbeat freshness
- Created `get_worker_fleet_status(redis_url)` async function — scans `orbiter:workers:*` keys via async Redis SCAN, parses each into `WorkerHealth`, detects dead workers (>60s since heartbeat)
- Created `_parse_worker_health(worker_id, data)` helper for Redis hash → `WorkerHealth` conversion
- Added `orbiter worker list` CLI command to `orbiter_cli/main.py` via `worker_app = typer.Typer()` subcommand group — shows Rich table with worker ID, status, hostname, tasks, failed, current task, concurrency, last heartbeat
- Dead workers displayed with "dead" status in red color
- Updated `orbiter.distributed.__init__` to export `WorkerHealth`, `WorkerHealthCheck`, `get_worker_fleet_status`
- Added 16 tests in `test_health.py`: `TestWorkerHealth` (3), `TestWorkerHealthCheck` (6), `TestParseWorkerHealth` (4), `TestGetWorkerFleetStatus` (3)
- Added 7 tests in `test_cli_main.py`: `TestWorkerCommandRegistered` (2), `TestWorkerListCommand` (5)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/health.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_health.py` (new), `packages/orbiter-cli/src/orbiter_cli/main.py`, `packages/orbiter-cli/tests/test_cli_main.py`
- **Learnings for future iterations:**
  - `WorkerHealthCheck.check()` uses sync `redis.from_url()` (not async) because the `HealthCheck` protocol requires a sync `check()` method
  - For patching sync `redis` in tests of `WorkerHealthCheck.check()`, patch at `redis.from_url` (top-level module) since the import is local (`import redis` inside method)
  - `get_worker_fleet_status()` uses async Redis SCAN (`r.scan(cursor=..., match=..., count=...)`) — returns `(cursor, keys)` tuple, loop until cursor is 0
  - Redis SCAN and HGETALL in async need `# type: ignore[misc]` for pyright (same pattern as store.py)
  - Worker alive detection: `time.time() - last_heartbeat < 60.0` — consistent with heartbeat TTL convention
  - CLI `worker` subcommand group follows same pattern as `task` group: `worker_app = typer.Typer()` + `app.add_typer(worker_app, name="worker")`
  - Total tests: 1195 (80 CLI + 142 distributed + 786 orbiter-core + others)
---

## 2026-02-17 - US-020
- Added distributed task metric constants to `orbiter/observability/semconv.py`: `METRIC_DIST_TASKS_SUBMITTED`, `METRIC_DIST_TASKS_COMPLETED`, `METRIC_DIST_TASKS_FAILED`, `METRIC_DIST_TASKS_CANCELLED`, `METRIC_DIST_QUEUE_DEPTH`, `METRIC_DIST_TASK_DURATION`, `METRIC_DIST_TASK_WAIT_TIME`
- Added distributed attribute constants: `DIST_TASK_ID`, `DIST_WORKER_ID`, `DIST_QUEUE_NAME`, `DIST_TASK_STATUS`
- Created `orbiter/distributed/metrics.py` with recording helpers: `record_task_submitted()`, `record_task_completed()`, `record_task_failed()`, `record_task_cancelled()`, `record_queue_depth()`
- All helpers support both OTel and in-memory fallback (same dual-path pattern as `record_agent_run`/`record_tool_step`)
- Updated `Worker._execute_task()` to automatically record metrics: `record_task_completed` on success, `record_task_failed` on failure, `record_task_cancelled` on cancellation
- Worker tracks wait time (`started_at - task.created_at`) and execution duration for histogram metrics
- Added `orbiter-observability` as dependency to `orbiter-distributed/pyproject.toml`
- Updated `orbiter.observability.__init__` `_SEMCONV_NAMES` list with new constants for lazy-loading
- Updated `orbiter.distributed.__init__` to export all 5 recording helpers
- Added 20 tests across 8 test classes: `TestBuildAttributes` (3), `TestRecordTaskSubmitted` (2), `TestRecordTaskCompleted` (5), `TestRecordTaskFailed` (3), `TestRecordTaskCancelled` (2), `TestRecordQueueDepth` (2), `TestWorkerMetricsIntegration` (3)
- Files changed: `packages/orbiter-observability/src/orbiter/observability/semconv.py`, `packages/orbiter-observability/src/orbiter/observability/__init__.py`, `packages/orbiter-distributed/src/orbiter/distributed/metrics.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/worker.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/pyproject.toml`, `packages/orbiter-distributed/tests/test_metrics.py` (new), `uv.lock`
- **Learnings for future iterations:**
  - OTel is installed in the dev environment (`HAS_OTEL=True`) — tests that check in-memory collector must `patch("orbiter.distributed.metrics.HAS_OTEL", False)` to force the fallback path
  - Pattern for test fixture: use `with patch(...)` inside an autouse fixture that yields, so all tests in the module automatically get `HAS_OTEL=False`
  - `_build_attributes()` helper builds attribute dicts with only non-empty fields — avoids cluttering metrics with empty strings
  - `record_queue_depth()` uses `set_gauge()` (in-memory) / `create_up_down_counter()` (OTel) — gauges represent current values, not cumulative
  - Worker wait time calculation: `started_at - task.created_at` — `task.created_at` is set by `TaskPayload` default factory at submission time
  - Distributed metrics import chain: `orbiter.distributed.metrics` → `orbiter.observability.metrics` + `orbiter.observability.semconv` — needed `orbiter-observability` added to distributed deps
  - Total tests: 1215 (80 CLI + 162 distributed + 786 orbiter-core + others)
---

## 2026-02-17 - US-021
- Added distributed task tracing with trace context propagation from client to worker
- Client (`distributed()` in `client.py`):
  - Creates `orbiter.distributed.submit` span wrapping task submission with `dist.task_id` and `dist.agent_name` attributes
  - Injects current baggage context into `TaskPayload.metadata['trace_context']` via `BaggagePropagator.inject()` + `DictCarrier`
  - Preserves any existing user-provided metadata alongside trace_context
- Worker (`_execute_task()` in `worker.py`):
  - Extracts trace context from `task.metadata['trace_context']` via `BaggagePropagator.extract()` + `DictCarrier`
  - Creates `orbiter.distributed.execute` child span wrapping entire task execution with `dist.task_id` and `dist.worker_id` attributes
  - Span records exceptions on task failure via `s.record_exception(exc)`
  - Gracefully handles missing or invalid trace_context (non-dict values ignored)
- Added 12 tests across 5 test classes: `TestDistributedSubmissionSpan` (2), `TestDistributedTraceContextInjection` (3), `TestWorkerTraceContextExtraction` (3), `TestWorkerExecutionSpan` (2), `TestTraceContextPropagation` (2)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/client.py`, `packages/orbiter-distributed/src/orbiter/distributed/worker.py`, `packages/orbiter-distributed/tests/test_tracing.py` (new)
- **Learnings for future iterations:**
  - `aspan` from `orbiter.observability.tracing` is the async span context manager — use it with `async with aspan(name, attributes=...) as s:`
  - `BaggagePropagator` + `DictCarrier` from `orbiter.observability.propagation` handle W3C Baggage serialization — inject sets `baggage` header, extract reads it back
  - Trace context propagation via metadata: inject into `metadata['trace_context']` (a `dict[str, str]`), extract on worker side — clean separation without changing TaskPayload schema
  - `carrier.headers` is empty dict `{}` when no baggage is set — check before adding to metadata to avoid unnecessary `trace_context` key
  - Worker must check `isinstance(trace_context, dict)` before extraction — metadata values could be any JSON type after deserialization
  - The `aspan` context manager yields a real OTel span or `NullSpan` — both support `record_exception()` and `set_attribute()` safely
  - Import `aspan` and propagation classes with `# pyright: ignore[reportMissingImports]` — same pattern as all cross-namespace imports
  - Total tests: 1227 (80 CLI + 174 distributed + 786 orbiter-core + others)
---

## 2026-02-17 - US-022
- Added streaming event metric constants to `orbiter/observability/semconv.py`: `METRIC_STREAM_EVENTS_EMITTED`, `METRIC_STREAM_EVENT_PUBLISH_DURATION`, `STREAM_EVENT_TYPE`
- Added `_record_event_published()` helper function to `orbiter/distributed/events.py` — records event count (counter) and publish duration (histogram) with event type attribute
- Updated `EventPublisher.publish()` to record metrics on each publish (event count by type, publish duration via `time.monotonic()`)
- Updated `run.stream()` in `runner.py` to count total events emitted per run when `detailed=True` — counts tracked via `events_emitted` dict, recorded at stream completion or error via `_record_stream_metrics()` helper
- Event counting integrated into `_passes_filter()` — only events that pass the filter are counted (consistent with what the consumer actually receives)
- Added `_SEMCONV_NAMES` entries for new constants in `orbiter.observability.__init__` for lazy loading
- Added 11 new tests across 4 test classes:
  - `test_event_metrics.py`: `TestRecordEventPublished` (4), `TestEventPublisherMetrics` (4)
  - `test_runner.py`: `TestRunStreamEventMetrics` (3)
- Files changed: `packages/orbiter-observability/src/orbiter/observability/semconv.py`, `packages/orbiter-observability/src/orbiter/observability/__init__.py`, `packages/orbiter-distributed/src/orbiter/distributed/events.py`, `packages/orbiter-distributed/tests/test_event_metrics.py` (new), `packages/orbiter-core/src/orbiter/runner.py`, `packages/orbiter-core/tests/test_runner.py`
- **Learnings for future iterations:**
  - `_record_event_published()` follows the same dual-path pattern (HAS_OTEL vs in-memory) as `record_task_submitted` and friends in `orbiter.distributed.metrics`
  - `time.monotonic()` is better than `time.time()` for measuring durations — monotonic clock can't go backwards
  - Event counting in `_stream()` is integrated into `_passes_filter()` to avoid modifying every yield point — the filter already gates all yields
  - Swarm delegation path in `_stream()` doesn't record metrics — each inner `run.stream()` call records its own metrics to avoid double-counting
  - `_record_stream_metrics()` batches per-type counts into single counter.add() calls — more efficient than per-event recording
  - Tests must patch `HAS_OTEL` at the module path where it's used (e.g., `orbiter.distributed.events.HAS_OTEL` for events, `orbiter.runner.HAS_OTEL` for runner)
  - Total tests: 1695 (all passing)
---

## 2026-02-17 - US-023
- Created `register_distributed_alerts()` function in `orbiter/distributed/alerts.py`
- Pre-defined alert rules:
  - Queue depth > 100: WARNING (`dist_queue_depth_warning`)
  - Queue depth > 500: CRITICAL (`dist_queue_depth_critical`)
  - Task failure rate > 10%: WARNING (`dist_failure_rate_warning`)
  - Worker count = 0: CRITICAL (`dist_worker_count_critical`)
  - Task wait time > 60s: WARNING (`dist_wait_time_warning`)
- Uses `AlertManager`, `AlertRule`, `AlertSeverity`, `Comparator` from `orbiter.observability.alerts`
- Rules reference metric constants from `orbiter.observability.semconv` (METRIC_DIST_QUEUE_DEPTH, METRIC_DIST_TASKS_FAILED, METRIC_DIST_TASK_WAIT_TIME)
- Worker count uses a custom `METRIC_WORKER_COUNT` constant (evaluated externally, not a recorded metric)
- Exported `register_distributed_alerts` from `orbiter.distributed.__init__`
- Rule name constants exported for external reference and testing
- Added 19 tests across 7 test classes: `TestRegisterDistributedAlerts` (5), `TestQueueDepthRules` (4), `TestFailureRateRule` (3), `TestWorkerCountRule` (2), `TestWaitTimeRule` (3), `TestAlertCallbacks` (2)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/alerts.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_alerts.py` (new)
- **Learnings for future iterations:**
  - `AlertRule` uses `Comparator.GT` for "greater than" thresholds — boundary value exactly at threshold does NOT fire (strict comparison)
  - `Comparator.EQ` used for "worker count = 0" — fires when metric_value equals threshold exactly
  - `get_manager()` returns a global singleton `AlertManager` — rules persist across calls, so `register_distributed_alerts()` is idempotent (rules overwrite by name)
  - `reset()` clears the global manager — use in test fixtures for isolation
  - Worker count is not a recorded metric — it needs to be evaluated externally (e.g., by counting active workers via `get_worker_fleet_status()`)
  - Alert rules use default `cooldown_seconds=300.0` — tests in the alert module use `cooldown_seconds=0` to avoid timing issues, but distributed alert registration uses the default
  - Total tests: 3463 (all passing)
---

## 2026-02-17 - US-018
- Created `TemporalExecutor` class in `orbiter/distributed/temporal.py` as alternative execution backend
- Temporal workflow wraps agent execution: `AgentExecutionWorkflow` (`@workflow.defn`) delegates to `execute_agent_activity` (`@activity.defn`)
- Activity runs `run.stream()` with periodic heartbeating (every 10 events) for Temporal liveness detection
- Activity reconstructs Agent/Swarm from serialized config (same `"agents"` key detection as Worker)
- Workflow reads `timeout_seconds` from payload for `start_to_close_timeout`, with 30s `heartbeat_timeout`
- `HAS_TEMPORAL` flag via `try/except ImportError` — same pattern as `HAS_OTEL` in orbiter-observability
- `TemporalExecutor.__init__` raises `ImportError` with install instructions when temporalio not available
- `TemporalExecutor.connect()` creates Temporal client, `execute_task()` starts workflow and waits for result
- `TemporalExecutor.start_temporal_worker()` / `stop_temporal_worker()` for running Temporal worker process
- Updated `Worker.__init__` with `executor: Literal["local", "temporal"] = "local"` parameter
- When `executor="temporal"`: creates `TemporalExecutor`, connects/disconnects in `start()`, delegates `_execute_task` to `executor.execute_task()`
- When `executor="temporal"` and `HAS_TEMPORAL=False`: raises clear `ImportError` immediately
- Added `temporalio>=1.7` to root dev dependencies in `pyproject.toml`
- Updated `orbiter.distributed.__init__` to export `HAS_TEMPORAL` and `TemporalExecutor`
- Added 30 tests across 9 test classes: `TestHasTemporal` (2), `TestTemporalEnvHelpers` (4), `TestTemporalExecutorInit` (5), `TestTemporalExecutorLifecycle` (2), `TestTemporalExecutorExecuteTask` (4), `TestTemporalExecutorWorkerManagement` (3), `TestExecuteAgentActivity` (3), `TestAgentExecutionWorkflow` (2), `TestWorkerExecutorParam` (5)
- Files changed: `packages/orbiter-distributed/src/orbiter/distributed/temporal.py` (new), `packages/orbiter-distributed/src/orbiter/distributed/worker.py`, `packages/orbiter-distributed/src/orbiter/distributed/__init__.py`, `packages/orbiter-distributed/tests/test_temporal.py` (new), `pyproject.toml`, `uv.lock`
- **Learnings for future iterations:**
  - `temporalio` optional dep: use `try/except ImportError` + `HAS_TEMPORAL` flag, same as OTel pattern
  - Guard `TemporalExecutor.__init__` on `HAS_TEMPORAL` so error is raised at construction time, not at method call
  - Temporal workflow/activity are only defined inside `if HAS_TEMPORAL:` block — they reference `activity.defn`/`workflow.defn` decorators that don't exist otherwise
  - `TemporalClient` and `TemporalWorker` set to `None` in ImportError fallback — methods on `TemporalExecutor` that use them need `# pyright: ignore[reportOptionalMemberAccess]` or `# pyright: ignore[reportOptionalCall]`
  - For multi-line imports, pyright comment must be on the `from` line: `from module import (  # pyright: ignore[reportMissingImports]` — ruff reformats single imports to multi-line, moving the comment
  - `activity.heartbeat(detail)` sends heartbeat with arbitrary detail — useful for progress tracking
  - `workflow.execute_activity()` takes `start_to_close_timeout` and `heartbeat_timeout` as `timedelta`
  - Worker `executor` parameter uses `Literal["local", "temporal"]` type — clean API for switching backends
  - Tests mock `TemporalClient` at `orbiter.distributed.temporal.TemporalClient` for Worker creation with temporal executor
  - Total tests: 1287 across key packages (all passing) — 231 distributed, 786 orbiter-core, 80+ CLI
---

## 2026-02-17 - US-024
- Updated placeholder `packages/orbiter-distributed/README.md` with full content
- Package description, installation instructions (base + temporal optional)
- Quick start guide: Redis setup, worker startup, task submission
- Agent execution example with `distributed()` API
- Swarm execution example with workflow mode
- Live streaming events example with pattern matching on all event types
- Task management: status checking, cancellation, result waiting
- CLI commands reference: `orbiter start worker`, `orbiter task list/status/cancel`, `orbiter worker list`
- Environment variables table: `ORBITER_REDIS_URL`, `TEMPORAL_HOST`, `TEMPORAL_NAMESPACE`
- Temporal integration section with durable execution explanation
- ASCII architecture diagram showing data flow: Client → Redis Stream → Workers → Events → Client
- Component descriptions: TaskBroker, Worker, EventPublisher/Subscriber, TaskStore, TaskHandle
- Files changed: `packages/orbiter-distributed/README.md`
- **Learnings for future iterations:**
  - README is a documentation-only change — no pyright/ruff/test impact
  - Pre-existing pyright errors (9 `reportMissingImports` in test_temporal.py) are unrelated to README changes
  - All 231 distributed tests continue to pass
---

## 2026-02-17 - US-025
- Created `docs/distributed/architecture.md` with comprehensive architecture documentation
- System architecture Mermaid diagram showing Client → Redis → Worker data flow with all Redis data structures
- Component descriptions for all modules: TaskBroker, Worker, EventPublisher/EventSubscriber, TaskStore, TaskHandle/distributed(), CancellationToken, Models, Health, Metrics, Alerts, Tracing
- Worker lifecycle state diagram (Mermaid)
- Task submission sequence diagram (Mermaid)
- Cancellation flow sequence diagram (Mermaid)
- Event publishing flow diagram (Mermaid)
- Tables documenting Redis commands per component, metrics, alert rules, Redis data structures
- Data flow diagrams for both Redis-only and Redis+Temporal modes
- Redis data structures summary with key patterns, types, purposes, and TTLs
- Covers both Redis-only and Redis+Temporal execution modes
- Files changed: `docs/distributed/architecture.md` (new), `prd.json`
- **Learnings for future iterations:**
  - Documentation-only stories don't affect pyright/ruff/tests — just verify pre-existing errors haven't changed
  - Mermaid diagrams work well for sequence diagrams, state diagrams, and component graphs
  - All 3493 tests continue to pass
---

## 2026-02-17 - US-026
- Created `docs/distributed/workers.md` with comprehensive worker setup and scaling guide
- Worker startup: CLI (`orbiter start worker`) and Python API with all constructor parameters documented
- Environment variables reference: `ORBITER_REDIS_URL`, `TEMPORAL_HOST`, `TEMPORAL_NAMESPACE`
- Concurrency tuning guidelines: CPU-bound vs I/O-bound agents, horizontal vs vertical scaling
- Multi-worker deployment: multiple processes, systemd service template
- Graceful shutdown behavior (SIGINT/SIGTERM handling)
- Heartbeat and health monitoring: Redis hash fields, CLI commands, Python health check API
- Task retry behavior and monitoring
- Execution backends: local (default) and Temporal (durable) with configuration
- Redis configuration recommendations: memory, persistence, connections
- Docker deployment: Dockerfile and docker-compose.yml with Redis, workers, and optional Temporal
- Monitoring and alerting: built-in alert rules, metrics table, task management CLI
- Scaling recommendations table by workload size
- Troubleshooting section for common issues
- Files changed: `docs/distributed/workers.md` (new), `prd.json`
- **Learnings for future iterations:**
  - Documentation-only stories don't affect pyright/ruff/tests — just verify pre-existing errors haven't changed
  - Worker constructor params well documented in worker.py docstring — reference source code for accurate defaults
  - All 3493 tests continue to pass
---

## 2026-02-17 - US-027
- Created `docs/distributed/migration.md` with comprehensive migration guide from local to distributed execution
- Side-by-side code comparisons: `run()` vs `distributed()`, `run.stream()` vs `handle.stream()`, Swarm streaming
- Step-by-step migration walkthrough (6 steps): install, start Redis, set URL, start worker, replace run(), replace stream()
- Event consumption patterns: local direct iterator, distributed submit+subscribe, event replay via Redis Streams
- Error handling differences: local exceptions vs distributed `RuntimeError` from `handle.result()`, `ErrorEvent` via streaming
- Task cancellation section (distributed-only feature)
- Decision table: when to use local vs distributed (8 scenarios)
- Parameter mapping table: `run()`/`run.stream()` params → `distributed()` params with notes
- Event filtering comparison: server-side (`event_types` param) vs client-side filtering
- Files changed: `docs/distributed/migration.md` (new), `prd.json`
- **Learnings for future iterations:**
  - Documentation-only stories don't affect pyright/ruff/tests — just verify pre-existing errors haven't changed
  - All 3493 tests continue to pass
---

## 2026-02-17 - US-028
- Created `docs/streaming-events.md` with comprehensive streaming events documentation
- All 8 event types documented with field tables: TextEvent, ToolCallEvent, StepEvent, ToolResultEvent, ReasoningEvent, ErrorEvent, StatusEvent, UsageEvent
- `detailed=True` usage guide with parameter comparison table
- Event emission order diagram showing step lifecycle
- Event filtering examples with `event_types` parameter
- Multi-agent streaming (Swarm) section covering workflow, handoff, and team modes
- SSE integration examples for FastAPI and Django
- Frontend consumption patterns: JavaScript EventSource and Fetch API with ReadableStream
- JSON serialization/deserialization examples with discriminated union pattern
- Distributed streaming section linking to `orbiter-distributed` architecture docs
- Files changed: `docs/streaming-events.md` (new), `prd.json`
- **Learnings for future iterations:**
  - Documentation-only stories don't affect pyright/ruff/tests — just verify pre-existing errors haven't changed
  - All 3493 tests continue to pass
---

## 2026-02-17 - US-029
- Created four example applications in `examples/distributed/`
- `simple_chat.py` — single agent with a tool, submitted via `distributed()`, streams all rich event types with pattern matching
- `multi_agent.py` — three-agent workflow Swarm (researcher >> drafter >> reviewer), streams events with agent name tracking
- `docker-compose.yml` — Docker Compose with Redis 7, 2 workers, and optional Temporal (via `--profile temporal`)
- `Dockerfile` — worker container image using uv for dependency resolution
- `fastapi_sse.py` — FastAPI endpoint that submits distributed tasks and streams SSE events to frontend, includes `/chat` (SSE stream) and `/task/{id}/status` (polling) endpoints
- All examples have inline comments explaining the code, prerequisites, and usage instructions
- All examples use `# pyright: ignore[reportMissingImports]` for `orbiter.distributed` imports (standard pattern)
- Files changed: `examples/distributed/simple_chat.py` (new), `examples/distributed/multi_agent.py` (new), `examples/distributed/docker-compose.yml` (new), `examples/distributed/Dockerfile` (new), `examples/distributed/fastapi_sse.py` (new), `prd.json`
- **Learnings for future iterations:**
  - Example files don't need tests but must pass ruff + pyright
  - ruff catches f-strings without placeholders (F541) and unsorted imports (I001) — use `--fix` for auto-correction
  - `orbiter.distributed` imports in examples need `# pyright: ignore[reportMissingImports]` — same pattern as all cross-namespace imports
  - FastAPI `StreamingResponse` works well with async generators for SSE — use `text/event-stream` media type
  - Docker Compose `profiles` feature allows optional services (Temporal) without cluttering default startup
  - All 3493 tests continue to pass
---
