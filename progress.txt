# Ralph Progress Log
Started: Sun Feb 15 04:58:56 PM IST 2026

## Codebase Patterns
- UV workspace monorepo: 13 packages under `packages/` (including orbiter-context), root pyproject.toml has NO build-system
- New namespace package setup: create `packages/orbiter-<name>/` dir, add to root pyproject.toml workspace members + dev deps + uv.sources, then `uv sync`
- Namespace packages: `orbiter-core/__init__.py` uses `pkgutil.extend_path()` so other packages add to the `orbiter` namespace
- Meta-package at `packages/orbiter/` uses `_orbiter_meta` dummy package for hatchling compatibility
- Quality checks: `uv run ruff check packages/`, `uv run ruff format --check packages/`, `uv run pyright packages/orbiter-core/`, `uv run pytest`
- Use `Sequence[Message]` (not `list[Message]`) for function params that accept subtype lists — avoids pyright invariance errors
- Use `ClassVar[dict[...]]` for mutable class attributes on dataclass-like test fixtures to avoid RUF012 lint errors
- Use `StrEnum` (not `str, Enum`) for string enums — ruff UP042 enforces this
- Test file names must be unique across all packages (pytest collects `tests/` from multiple packages into one `tests` module)
- pytest-asyncio `asyncio_mode = "auto"` configured in root pyproject.toml — no need for `@pytest.mark.asyncio`
- Import sorting: ruff enforces I001 import block sorting — use `uv run ruff check --fix` to auto-sort
- Forward-compatible params: use `Any` type with `None` default for params whose concrete types come from future packages
- Cross-package error handling: since orbiter-core can't import ModelError from orbiter-models, check error attributes (e.g., `.code`) via `getattr()` on caught exceptions
- Mock provider responses MUST include valid `Usage` objects (not `None`) — `parse_response(usage=None)` causes Pydantic validation failure, silently consumed by retry logic
- pyright ignore comments: put `# pyright: ignore[reportMissingImports]` on the import line itself (not a separate comment line) to avoid ruff I001 import sorting issues
- Within-package namespace imports also need `# pyright: ignore[reportMissingImports]` — e.g., `context.py` importing `config.py` in the same `orbiter.context` package
- Ruff RUF023: `__slots__` entries must be sorted alphabetically — run `uv run ruff check --fix` to auto-sort
- `__slots__` makes instance attributes read-only for `patch.object(instance, "method")` — use `patch.object(ClassName, "method")` instead
- `@asynccontextmanager` returns single-use CMs — when mocking, use `side_effect=lambda: factory()` not `return_value=cm` for multi-call scenarios
---

## 2026-02-15 - US-001
- What was implemented: Message builder with build_messages, validate_message_order, extract_last_assistant_tool_calls, merge_usage. Initial commit of entire Orbiter framework infrastructure (Phases 1-4 + partial Phase 5).
- Files changed: packages/ (65 files — all package infrastructure, types, config, registry, events, hooks, tool, models, agent, message_builder, output_parser, and tests)
- **Learnings for future iterations:**
  - The codebase had pre-existing code from prior interactive sessions (Phases 1-4) that was never committed — this first commit captures everything
  - `list[SubType]` is not assignable to `list[BaseType]` in pyright due to list invariance — use `Sequence[BaseType]` for read-only function params
  - `test_agent.py` had RUF012 (mutable class default) and I001 (import sorting) lint errors that needed fixing
  - All 349 tests pass across orbiter-core (222) and orbiter-models (127)
---

## 2026-02-15 - US-002
- What was implemented: Output parser was already fully implemented and committed as part of the US-001 batch commit. Verified all acceptance criteria are met.
- Files: `packages/orbiter-core/src/orbiter/_internal/output_parser.py` (~155 lines), `packages/orbiter-core/tests/test_output_parser.py` (19 tests)
- Key functions: `parse_response()`, `parse_tool_arguments()`, `parse_structured_output()`, `OutputParseError`
- All quality checks pass: ruff check, ruff format, pyright (0 errors), pytest (349 tests)
- **Learnings for future iterations:**
  - US-001's initial commit included code for multiple user stories — check if the next story's code already exists before implementing
  - The PRD's `ParsedOutput` is implemented as `AgentOutput` in the actual code — the types module defines `AgentOutput` with text, tool_calls, usage fields
---

## 2026-02-15 - US-003
- What was implemented: Agent class was already mostly implemented from prior sessions. Added missing `memory` and `context` parameters to `Agent.__init__` (typed as `Any` since orbiter-context and orbiter-memory packages are not yet implemented). Updated test to assert defaults are `None`.
- Files changed: `packages/orbiter-core/src/orbiter/agent.py`, `packages/orbiter-core/tests/test_agent.py`
- **Learnings for future iterations:**
  - Agent class already had 21 tests covering: minimal/full creation, keyword-only enforcement, model parsing, tool/handoff registration with duplicate detection, hook integration, callable instructions, describe(), __repr__
  - `memory` and `context` params use `Any` type since their implementations are in future phases (orbiter-memory Phase 9, orbiter-context Phase 8)
  - Pattern: when PRD references types from future packages, use `Any` with `None` default as a forward-compatible placeholder
---

## 2026-02-15 - US-004
- What was implemented: `Agent.run()` async method with single-turn LLM execution and retry logic. Added `code` parameter to `ModelError` for error classification.
- Files changed: `packages/orbiter-core/src/orbiter/agent.py` (added ~85 lines for run() + _is_context_length_error()), `packages/orbiter-core/tests/test_agent.py` (added 15 tests in 3 new test classes), `packages/orbiter-models/src/orbiter/models/types.py` (added `code` param to ModelError)
- Key implementation details:
  - `run(input, *, messages, provider, max_retries=3)` wires message_builder → LLM call → output_parser
  - PRE_LLM_CALL / POST_LLM_CALL hooks fire at correct points
  - Retry with exponential backoff (2^attempt seconds): 1s, 2s, 4s...
  - Context-length errors (via `ModelError.code == "context_length"` or message text) fail immediately
  - Provider typed as `Any` since orbiter-core doesn't depend on orbiter-models
- **Learnings for future iterations:**
  - orbiter-core can't import from orbiter-models directly — use `Any` type for provider parameter and catch base exceptions
  - `_is_context_length_error()` checks both `.code` attribute and error message text for robustness
  - pyright inline ignore comments on import lines don't break ruff I001 sorting (put `# pyright: ignore[...]` on the same import line, not a separate comment line)
  - All 364 tests pass (15 new: 7 run tests, 4 retry tests, 4 hook tests)
---

## 2026-02-15 - US-005
- What was implemented: Tool execution loop in `Agent.run()` — when LLM returns tool calls, agent executes them in parallel, feeds results back, and re-calls LLM until text response or max_steps.
- Files changed: `packages/orbiter-core/src/orbiter/agent.py` (refactored run() into run() + _call_llm() + _execute_tools(), ~120 new lines), `packages/orbiter-core/tests/test_agent.py` (8 new tests in TestAgentToolLoop class)
- Key implementation details:
  - `run()` now has a `for _step in range(self.max_steps)` loop around `_call_llm()`
  - `_call_llm()` extracted from old `run()` — single LLM call with retry logic and hooks
  - `_execute_tools()` uses `asyncio.TaskGroup` for parallel tool execution
  - Tool errors caught per-tool and returned as `ToolResult(error=...)`, not propagated
  - Unknown tools return `ToolResult(error="Unknown tool '...'")`
  - PRE_TOOL_CALL / POST_TOOL_CALL hooks fire for each tool
  - `parse_tool_arguments()` from output_parser converts ToolCall JSON args to ActionModel
  - Updated existing test `test_run_with_tool_calls_in_response` to account for the tool loop (provider now needs to return text on second call)
- **Learnings for future iterations:**
  - When adding tool loop, existing tests that mock a single tool-call response break because the agent now tries to execute the tool and re-call. Update mocks to return text on the follow-up call.
  - `asyncio.TaskGroup` requires Python 3.11+ — already satisfied by this project
  - The placeholder list pattern `[ToolResult(...)] * len(actions)` creates shared references — must assign by index in the async task, not append
  - All 372 tests pass (8 new tool loop tests)
---

## 2026-02-15 - US-006
- What was implemented: 8 new edge case tests in `TestAgentEdgeCases` class covering: retry during tool loop, agent with handoffs runs normally, handoffs don't appear as tools, sequential tool calls accumulate messages, max_steps=1, empty input, usage from final response, tool with no arguments.
- Files changed: `packages/orbiter-core/tests/test_agent.py` (+161 lines)
- **Learnings for future iterations:**
  - Most edge cases were already covered by earlier test classes (TestAgentToolLoop had 8 tests for the core scenarios)
  - The key gap was retry behavior mid-tool-loop and handoff interaction — these are important to test because they exercise separate code paths in the agent
  - 52 agent tests total across 9 test classes, 380 tests across the full suite
---

## 2026-02-15 - US-007
- What was implemented: Human-in-the-loop tool with `HumanInputHandler` ABC, `ConsoleHandler` (stdin-based), and `HumanInputTool` (Tool subclass with timeout support).
- Files changed: `packages/orbiter-core/src/orbiter/human.py` (~120 lines), `packages/orbiter-core/tests/test_human.py` (21 tests in 4 classes)
- Key implementation details:
  - `HumanInputHandler` ABC with `async get_input(prompt, choices)` — extensible to console, web, Slack, etc.
  - `ConsoleHandler` — reads from stdin via `asyncio.to_thread()`, validates choices, defaults to first choice on invalid input
  - `HumanInputTool` — `Tool` subclass with manually defined JSON schema (prompt required, choices optional array)
  - Timeout via `asyncio.wait_for()` — raises `ToolError` on timeout
  - Tests use `MockHandler` and `SlowHandler` fixtures, plus `monkeypatch` for `ConsoleHandler._read_line`
- **Learnings for future iterations:**
  - `HumanInputTool` manually defines its JSON schema rather than using `_generate_schema()` since it's not a FunctionTool wrapper — this is the pattern for custom Tool subclasses
  - `asyncio.wait_for()` raises `TimeoutError` (Python 3.11+) — catch that and convert to `ToolError` for consistent error handling
  - 401 tests total across full suite (21 new)
---

## 2026-02-15 - US-008
- What was implemented: Run state tracking with `RunNodeStatus` (StrEnum: INIT, RUNNING, SUCCESS, FAILED, TIMEOUT), `RunNode` (Pydantic model with lifecycle transitions: start/succeed/fail/timeout, timing, usage, metadata), and `RunState` (mutable execution tracker with message accumulation, node management, usage aggregation, terminal state detection).
- Files changed: `packages/orbiter-core/src/orbiter/_internal/state.py` (~145 lines), `packages/orbiter-core/tests/test_state.py` (29 tests in 3 classes)
- **Learnings for future iterations:**
  - Use `StrEnum` instead of `str, Enum` — ruff UP042 enforces this pattern
  - `Sequence[Message]` still needed for function params that accept `list[SubType]` — pyright invariance on `list[T]`
  - `RunNode` uses mutable Pydantic model (no `frozen=True`) since state transitions mutate fields — this is the right pattern for stateful objects vs. value objects
  - 430 tests total across full suite (29 new)
---

## 2026-02-15 - US-009
- What was implemented: Core call runner (`call_runner()`) that orchestrates Agent.run() with RunState tracking, node lifecycle management, usage accumulation, and endless loop detection via tool-call signature comparison.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/call_runner.py` (~145 lines), `packages/orbiter-core/tests/test_call_runner.py` (15 tests in 6 classes)
- Key implementation details:
  - `call_runner(agent, input, *, state, messages, provider, max_retries, loop_threshold)` — wraps Agent.run() in RunState lifecycle
  - Creates RunNode per call, tracks INIT → RUNNING → SUCCESS/FAILED transitions
  - Loop detection: `_tool_call_signature()` creates deterministic string from sorted tool name:args pairs; `_check_loop()` counts consecutive matching signatures in node metadata
  - Errors from Agent.run() wrapped in `CallRunnerError`, state/node marked FAILED
  - Agent.instructions resolved via callable check + str() for pyright compatibility (agent param typed as Any)
  - Returns `RunResult` with output text, accumulated messages, total usage, step count
- **Learnings for future iterations:**
  - Since `agent` is typed as `Any` in call_runner, accessing `agent.instructions` returns `object | Any` — must use `str()` cast or explicit type narrowing, not direct assignment to `str` variable
  - Loop detection works across consecutive `call_runner()` invocations with shared state by storing tool signatures in `RunNode.metadata`
  - The call_runner delegates the actual LLM→tool→LLM loop entirely to Agent.run() — it's a thin orchestration layer for state tracking, not a replacement for Agent's internal loop
  - 445 tests total across full suite (15 new)
---

## 2026-02-15 - US-010
- What was implemented: Public `run()` async entry point and `run.sync()` blocking wrapper as the primary API for executing agents. Auto-provider resolution via orbiter.models registry when available.
- Files changed: `packages/orbiter-core/src/orbiter/runner.py` (~100 lines), `packages/orbiter-core/tests/test_runner.py` (13 tests in 5 classes)
- Key implementation details:
  - `run(agent, input, *, messages, provider, max_retries, loop_threshold) -> RunResult` — async, delegates to `call_runner()`
  - `run.sync(...)` — attached as attribute on `run` function, uses `asyncio.run()` internally
  - `_resolve_provider(agent)` — tries `orbiter.models.provider.get_provider()` with graceful fallback to `None`
  - Provider typed as `Any` since orbiter-core doesn't depend on orbiter-models
- **Learnings for future iterations:**
  - Python functions are objects — you can attach attributes like `run.sync = _sync` with `# type: ignore[attr-defined]` for pyright
  - `asyncio.run()` creates a new event loop, so `run.sync()` can't be called from within an existing async context — this is the expected behavior
  - Auto-provider resolution uses try/except to gracefully handle missing orbiter-models package
  - 458 tests total across full suite (13 new)
---

## 2026-02-15 - US-011
- What was implemented: `run.stream()` async generator attached to the `run` function, yielding `TextEvent` for text deltas and `ToolCallEvent` for tool invocations. Supports full tool execution loop with re-streaming after tool results.
- Files changed: `packages/orbiter-core/src/orbiter/runner.py` (added `_stream()` ~110 lines), `packages/orbiter-core/tests/test_runner.py` (added 10 tests in 3 new test classes + streaming helpers)
- Key implementation details:
  - `run.stream(agent, input, ...)` uses `provider.stream()` for chunk-by-chunk text delivery
  - Tool call deltas accumulated from `StreamChunk.tool_call_deltas` by index, then assembled into `ToolCall` objects
  - After tool calls: executes tools via `agent._execute_tools()`, appends results to conversation, re-streams
  - Loop continues until text-only response or `max_steps` reached
  - Error in no-provider case raises `AgentError` immediately
- **Learnings for future iterations:**
  - `_resolve_provider` passes `agent.provider_name` to `get_provider()`, which calls `parse_model_string()` again — a bare name like "nonexistent" gets parsed as `("openai", "nonexistent")` and resolves to OpenAI provider. To test no-provider scenarios, monkeypatch `_resolve_provider` to return `None`
  - Same pyright issue with `agent.instructions` returning `object | Any` — use `str()` cast pattern (instr/raw_instr) consistent with call_runner.py
  - Streaming doesn't use `call_runner()` or `Agent.run()` — it operates at a lower level, directly calling `provider.stream()` and managing the tool loop itself. This is by design for real-time event delivery
  - 468 tests total across full suite (10 new streaming tests)
---

## 2026-02-15 - US-012
- What was implemented: Handler system with `Handler[IN, OUT]` ABC (generic async generator base), `AgentHandler` (routes between agents in swarms), `SwarmMode` enum (workflow/handoff/team), handoff detection, and topology-aware stop checks.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/handlers.py` (~130 lines), `packages/orbiter-core/tests/test_handlers.py` (28 tests in 7 classes)
- Key implementation details:
  - `Handler[IN, OUT]` is a generic ABC with abstract `handle(input) -> AsyncIterator[OUT]` — non-async def returning AsyncIterator avoids pyright type issues with abstract async generators
  - `AgentHandler` implements workflow (sequential, output→input chaining), handoff (follow handoff chains with max_handoffs guard), and team (run lead agent) modes
  - Handoff detection: exact match of `result.output.strip()` against agent's handoff target names, with additional check that the target exists in the swarm's agents dict
  - Stop checks are separate methods (`_check_workflow_stop`, `_check_handoff_stop`, `_check_team_stop`) for composability
  - Delegates actual agent execution to `call_runner()` from the existing call_runner module
- **Learnings for future iterations:**
  - Abstract async generator methods in ABCs: use `def handle(...) -> AsyncIterator[T]` (not `async def`) to avoid pyright issues with `yield` in abstract methods yielding `None` instead of `T`
  - Handoff cycle test: when testing A→B→A→B cycles, the mock provider must alternate responses ("b", "a", "b", "a") since each agent checks handoff targets against its own handoffs dict
  - The handler system is the bridge between the runner layer (run/call_runner) and the swarm layer (US-017+). AgentHandler will be used by Swarm to orchestrate multi-agent execution
  - 496 tests total across full suite (28 new handler tests)
---

## 2026-02-15 - US-013
- What was implemented: ToolHandler (dynamic tool loading, parallel execution via asyncio.TaskGroup, result aggregation) and GroupHandler (parallel and serial agent group execution with dependency resolution via Kahn's topological sort).
- Files changed: `packages/orbiter-core/src/orbiter/_internal/handlers.py` (added ~130 lines for ToolHandler + GroupHandler), `packages/orbiter-core/tests/test_handlers.py` (added 23 tests in 6 new test classes)
- Key implementation details:
  - `ToolHandler` — receives dict of `{tool_call_id: {"name": str, "arguments": dict}}`, resolves tools from registry, executes in parallel, yields `ToolResult` objects. Also has `register()`, `register_many()`, `aggregate()` methods
  - `GroupHandler` — `parallel=True` runs all agents concurrently (same input, TaskGroup), `parallel=False` runs serially with output→input chaining and dependency resolution
  - `_resolve_order()` implements Kahn's algorithm for topological sort with cycle detection — raises `HandlerError` on cyclic dependencies
  - Both handlers follow the `Handler[IN, OUT]` ABC pattern from US-012
- **Learnings for future iterations:**
  - When testing with `_make_provider`, response count must match total LLM calls across all agents — each `call_runner()` invocation triggers at least one `provider.complete()` call
  - Custom mock providers in tests need to return objects with `content`, `tool_calls`, and `usage` attributes matching what `parse_response()` expects — `usage=None` will fail
  - To test "missing agent" in serial mode, either override `_resolve_order()` in a subclass or manipulate the agents dict after construction (since `_resolve_order` only includes keys from `self.agents`)
  - 519 tests total across full suite (23 new handler tests)
---

## 2026-02-15 - US-014
- What was implemented: BackgroundTaskHandler with hot-merge and wake-up-merge patterns. BackgroundTask lifecycle (INIT→RUNNING→SUCCESS/FAILED), PendingQueue for async result queuing, merge callbacks, state node integration.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/background.py` (~230 lines), `packages/orbiter-core/tests/test_background.py` (28 tests in 8 classes)
- Key implementation details:
  - `BackgroundTask` — tracks task_id, parent_task_id, payload, result, error, status, merge_mode
  - `PendingQueue` — async-aware queue with `push()`, `pop_all()`, `wait(timeout)` using `asyncio.Event`
  - `BackgroundTaskHandler` — `submit()` registers tasks, `handle_result()` routes to hot or wake-up merge, `handle_error()` for failures, `drain_pending()` async iterator for wake-up pattern
  - Hot-merge: fires registered callbacks when main task is still running
  - Wake-up-merge: queues to PendingQueue when main task completed; drain via `drain_pending()`
  - State integration: creates RunNode with `bg:{task_id}` agent name, marks SUCCESS/FAILED on completion
  - `on_merge(callback)` for registering async merge callbacks
  - `list_tasks(status=...)` for filtered task listing
- **Learnings for future iterations:**
  - BackgroundTaskHandler is NOT a Handler[IN, OUT] subclass — it's a standalone manager rather than a pipeline handler, since background tasks have their own lifecycle separate from the main execution flow
  - `asyncio.TimeoutError` is deprecated in favor of builtin `TimeoutError` (ruff UP041) — just catch `TimeoutError`
  - Unused imports (ruff F401) must be removed even from type-only usage — `RunNode` was imported but only `RunNodeStatus` and `RunState` were needed
  - 547 tests total across full suite (28 new background handler tests)
---

## 2026-02-15 - US-015
- What was implemented: Runner integration tests covering end-to-end Agent + @tool + run() flows, handler pipelines (workflow, handoff, team), ToolHandler/GroupHandler integration, and background task scenarios (hot-merge, wake-up-merge, error handling, callbacks). Updated `packages/orbiter-core/src/orbiter/__init__.py` with public API exports (Agent, run, tool, Tool, FunctionTool).
- Files changed: `packages/orbiter-core/src/orbiter/__init__.py` (added imports + __all__), `packages/orbiter-core/tests/test_runner_integration.py` (22 new tests in 8 test classes)
- **Learnings for future iterations:**
  - `run.sync()` uses `asyncio.run()` which can't be called from within an async test function (already has a running event loop) — make sync tests use `def test_...` (not `async def`)
  - Ruff N817 flags `import X as A` when `A` is a CamelCase name used as an acronym — use longer aliases like `AgentImport` instead of `A`
  - The `__init__.py` for orbiter-core already uses `pkgutil.extend_path()` for namespace packages — imports of `Agent`, `run`, `tool`, `Tool` go after that line
  - 569 tests total across full suite (22 new integration tests)
---

## 2026-02-15 - US-016
- What was implemented: Graph utilities module with `Graph` dataclass (adjacency list), `topological_sort()` (Kahn's algorithm with deterministic ordering), cycle detection, and `parse_flow_dsl()` for parsing flow DSL strings like `"a >> b >> c"` and `"(a | b) >> c"` into Graph objects.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/graph.py` (~130 lines), `packages/orbiter-core/tests/test_graph.py` (31 tests in 4 classes)
- Key implementation details:
  - `Graph` is a `@dataclass` with `_adjacency: dict[str, list[str]]` — add_node (idempotent), add_edge (duplicate-safe, auto-creates nodes), nodes/edges properties, successors(), in_degree()
  - `topological_sort()` uses Kahn's algorithm with `deque` — sorts queue entries for deterministic ordering
  - `parse_flow_dsl()` tokenizes by `>>`, handles `(a | b)` parallel groups via regex, creates edges between all members of consecutive stages
  - `GraphError` raised for cycles, unknown nodes, empty/malformed DSL
- **Learnings for future iterations:**
  - GroupHandler in `handlers.py` already has an inline topo sort (`_resolve_order`) — could be refactored to use graph.py in a future iteration, but not required now
  - `deque.popleft()` is O(1) vs `list.pop(0)` which is O(n) — prefer deque for BFS-style queue operations
  - The `parse_flow_dsl` returns a `Graph` (not just edges) so callers can use `topological_sort()` directly on the result
  - 600 tests total across full suite (31 new graph tests)
---

## 2026-02-15 - US-017
- What was implemented: Swarm class with workflow mode (`packages/orbiter-core/src/orbiter/swarm.py`, ~120 lines). Workflow mode runs agents sequentially with output→input chaining. Uses `parse_flow_dsl()` and `topological_sort()` from graph.py for execution ordering. Updated `runner.py` to detect Swarm (via `hasattr(agent, "flow_order")`) and delegate to `swarm.run()`.
- Files changed: `packages/orbiter-core/src/orbiter/swarm.py` (new, ~120 lines), `packages/orbiter-core/tests/test_swarm.py` (new, 20 tests in 5 classes), `packages/orbiter-core/src/orbiter/runner.py` (added Swarm detection in `run()`)
- Key implementation details:
  - `Swarm.__init__` accepts `agents` (list), `flow` (DSL string, optional), `mode` (default "workflow")
  - Without flow DSL, agents execute in list order. With flow DSL, topological sort determines order
  - `Swarm.run()` delegates to `call_runner()` per agent, chaining output→input
  - Validates: no duplicate agent names, all flow DSL nodes are known agents, no cycles
  - `SwarmError` for all swarm-level errors
  - `describe()` and `__repr__()` for introspection
  - Swarm has a `name` attribute for runner compatibility
- **Learnings for future iterations:**
  - Swarm detection in `run()` uses duck-typing (`hasattr(agent, "flow_order")`) rather than isinstance to avoid circular imports — this works because Agent doesn't have a `flow_order` attribute
  - Swarm.run() has its own `provider` and `max_retries` params, but doesn't take `loop_threshold` (that's Agent-level via call_runner)
  - The AgentHandler from handlers.py already has workflow mode — Swarm is a higher-level API that wraps the same call_runner() directly without going through AgentHandler. Future handoff/team modes (US-018, US-019) may leverage AgentHandler's existing implementations
  - 620 tests total across full suite (20 new swarm tests)
---

## 2026-02-15 - US-018
- What was implemented: Swarm handoff mode (`mode='handoff'`) with dynamic agent-to-agent delegation. Added `_run_handoff()` (handoff chain execution with conversation history transfer), `_detect_handoff()` (output-based handoff detection matching agent's declared handoff targets), and `max_handoffs` parameter for loop detection.
- Files changed: `packages/orbiter-core/src/orbiter/swarm.py` (added ~70 lines for handoff mode), `packages/orbiter-core/tests/test_swarm.py` (added 15 tests in 4 new test classes)
- Key implementation details:
  - `_run_handoff()` starts with the first agent in flow_order, runs it via `call_runner()`, checks output against handoff targets
  - `_detect_handoff()` matches `result.output.strip()` against agent's `handoffs` dict keys, also requires target exists in `swarm.agents`
  - `max_handoffs` uses `>` semantics (default 10): allows exactly `max_handoffs` transitions before raising `SwarmError`
  - Conversation history from each agent's run (`result.messages`) is passed to the next agent as `all_messages`
  - Handoff target not in swarm's agents dict → no handoff (agent output returned as-is)
- **Learnings for future iterations:**
  - `max_handoffs` boundary: use `>` not `>=` so that `max_handoffs=N` allows exactly N handoffs. The AgentHandler in handlers.py uses `>=` which is slightly different semantics
  - `call_runner` builds messages internally per agent — the `result.messages` from RunState are minimal (system + user + assistant). Passing them as `all_messages` to the next call_runner allows conversation history transfer, but each agent still builds its own message list
  - Ruff RUF043: regex metacharacters in `pytest.raises(match=...)` require raw strings (e.g., `match=r"Max handoffs.*3.*exceeded"`)
  - 635 tests total across full suite (15 new handoff tests)
---

## 2026-02-15 - US-019
- What was implemented: Swarm team mode (`mode='team'`) with lead-worker delegation pattern. Lead agent gets auto-generated `_DelegateTool` instances (`delegate_to_{worker_name}`) that invoke worker agents when called. Workers run via `call_runner()` and return their output as tool results. Lead synthesizes final output after receiving worker results.
- Files changed: `packages/orbiter-core/src/orbiter/swarm.py` (added `_run_team()` ~50 lines, `_DelegateTool` class ~50 lines), `packages/orbiter-core/tests/test_swarm.py` (added 13 tests in 4 new test classes)
- Key implementation details:
  - `_DelegateTool` is a `Tool` subclass with manually defined JSON schema (task parameter), calls `call_runner()` on the worker
  - `_run_team()` temporarily adds delegate tools to lead's `self.tools`, runs `call_runner(lead, ...)`, then restores original tools in a `finally` block
  - Team mode requires at least 2 agents (lead + at least one worker)
  - Workers receive the lead's tool-call argument `task` as their input, not the original user input
- **Learnings for future iterations:**
  - Mock providers for team mode need `Usage` objects (not `None`) in dict-form responses — `parse_response(usage=None)` causes `AgentOutput` Pydantic validation to fail, which gets caught by `_call_llm`'s retry loop, consuming the next response instead
  - The provider `complete()` call count is shared across lead + worker agents since the mock is a single closure — responses must be ordered accounting for worker calls interleaved with lead calls
  - `_DelegateTool` follows the same pattern as `HumanInputTool` — custom `Tool` subclass with manually defined JSON schema rather than `_generate_schema()` from a function
  - Using `finally` block to restore tools ensures cleanup even on errors (important for tool-loop exceptions propagating up)
  - 648 tests total across full suite (13 new team mode tests)
---

## 2026-02-15 - US-020
- What was implemented: ParallelGroup (concurrent agent execution via `asyncio.TaskGroup` with configurable result aggregation) and SerialGroup (sequential output→input chaining) in `packages/orbiter-core/src/orbiter/_internal/agent_group.py`. Both integrate as nodes in Swarm workflow via `is_group` marker attribute. Swarm's `_run_workflow()` updated to detect groups and delegate to their `run()` method.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/agent_group.py` (new, ~236 lines), `packages/orbiter-core/tests/test_agent_group.py` (new, 27 tests in 7 classes), `packages/orbiter-core/src/orbiter/swarm.py` (updated `_run_workflow` for group detection)
- Key implementation details:
  - `ParallelGroup` — concurrent execution via `asyncio.TaskGroup`, default join with `\n\n` separator, optional custom `aggregate_fn(list[RunResult]) -> str`, usage/steps summed across all agents
  - `SerialGroup` — sequential execution with output→input chaining, accumulated usage/steps, final agent's output is the group output
  - Both have `is_group = True` marker, `name` attribute, `run()` method matching Agent/Swarm interface, and `describe()`/`__repr__()` for introspection
  - Swarm detects groups via `getattr(agent, "is_group", False)` in `_run_workflow()` — similar duck-typing pattern as Swarm detection in `runner.py`
  - Groups can be used standalone (without Swarm) or as nodes in Swarm flow DSL
- **Learnings for future iterations:**
  - Groups need the same interface as agents from Swarm's perspective: `name` attribute and `run(input, *, messages, provider, max_retries)` method
  - Duck-typing via `is_group` marker is cleaner than isinstance checks since it avoids circular imports
  - The existing `GroupHandler` in `handlers.py` has similar parallel/serial logic but operates at the handler layer — `ParallelGroup`/`SerialGroup` are the public-facing API while `GroupHandler` is the internal handler abstraction
  - 675 tests total across full suite (27 new agent group tests)
---

## 2026-02-15 - US-021
- What was implemented: Nested swarm support via `SwarmNode` wrapper class in `packages/orbiter-core/src/orbiter/_internal/nested.py`. `SwarmNode` wraps a Swarm so it can be used as a node in another Swarm's agent list. Context isolation: inner swarm gets clean message history (outer messages NOT forwarded). Updated `Swarm._run_workflow()` to detect `is_swarm` marker via duck-typing.
- Files changed: `packages/orbiter-core/src/orbiter/_internal/nested.py` (new, ~107 lines), `packages/orbiter-core/tests/test_nested.py` (new, 14 tests in 5 classes), `packages/orbiter-core/src/orbiter/swarm.py` (1-line change in `_run_workflow`)
- Key implementation details:
  - `SwarmNode` has `is_swarm = True` marker, `name` attribute, and `run()` method — same interface pattern as `ParallelGroup`/`SerialGroup` with `is_group`
  - `_run_workflow()` condition: `getattr(agent, "is_group", False) or getattr(agent, "is_swarm", False)` — both groups and nested swarms use `agent.run()` directly
  - Context isolation: `SwarmNode.run()` does NOT forward outer `messages` to inner swarm — each level maintains its own conversation context
  - Inner swarm can use any mode (workflow, handoff, team) — tested with both workflow and handoff modes
  - `NestedSwarmError` for validation errors, `describe()` includes inner swarm metadata
- **Learnings for future iterations:**
  - The duck-typing pattern (`is_group`, `is_swarm`) is consistent across the codebase for detecting node types without isinstance checks
  - `Agent.__init__` expects `handoffs` as `list[Agent]`, NOT `dict` — the dict is built internally by `_register_handoff()`
  - SwarmNode's context isolation is a design choice: inner swarm starts fresh each time, which prevents message pollution between nesting levels
  - 689 tests total across full suite (14 new nested swarm tests)
---

## 2026-02-15 - US-022
- What was implemented: Swarm integration with the public `run()` API and public API exports. Updated `packages/orbiter-core/src/orbiter/__init__.py` to export `Swarm`, `SwarmNode`, `ParallelGroup`, `SerialGroup`. Created comprehensive integration tests covering all swarm modes and features via the public `run()` entry point.
- Files changed: `packages/orbiter-core/src/orbiter/__init__.py` (added 6 new exports), `packages/orbiter-core/tests/test_swarm_integration.py` (new, 21 tests in 8 classes)
- Integration test coverage:
  - Workflow mode: 2-agent, 3-agent pipeline, no-flow-DSL, sync wrapper
  - Handoff mode: simple handoff, A→B→C chain, no-handoff-returns-directly
  - Team mode: lead-delegates-to-worker, lead-no-delegation
  - Groups: ParallelGroup in workflow, SerialGroup in workflow, custom aggregation
  - Nested swarms: nested workflow, nested handoff-inside-workflow
  - Public API: Swarm/SwarmNode/ParallelGroup/SerialGroup importable from `orbiter`, run() detects Agent vs Swarm
  - Tools in workflows: agent uses @tool then chains to next agent
- **Learnings for future iterations:**
  - Swarm was already wired into `run()` via `hasattr(agent, "flow_order")` from US-017 — this story only needed to add public exports and integration tests
  - The `_make_provider` mock must account for ALL LLM calls across ALL agents in a swarm (e.g., 3-agent pipeline = 3 responses, team with delegation = lead + worker + lead = 3 responses)
  - `__init__.py` imports must be sorted alphabetically by module path for ruff I001 compliance
  - 710 tests total across full suite (21 new swarm integration tests)
---

## 2026-02-15 - US-023
- What was implemented: Created `orbiter-context` package from scratch — added workspace registration, pyproject.toml, directory structure. Implemented `ContextConfig` (Pydantic v2 frozen model with `AutomationMode` enum: pilot/copilot/navigator, history_rounds, summary_threshold, offload_threshold, enable_retrieval, neuron_names tuple, extra dict, threshold validation) and `make_config()` factory with preset defaults per automation level. Implemented `ContextState` (hierarchical key-value store with parent chain lookup, write isolation, get/set/update/delete/pop/clear/to_dict/local_dict/keys).
- Files changed:
  - `pyproject.toml` (root — added orbiter-context to workspace members, dev deps, uv.sources)
  - `packages/orbiter-context/pyproject.toml` (new package config)
  - `packages/orbiter-context/src/orbiter/context/__init__.py` (new)
  - `packages/orbiter-context/src/orbiter/context/config.py` (~120 lines — AutomationMode, ContextConfig, make_config)
  - `packages/orbiter-context/src/orbiter/context/state.py` (~130 lines — ContextState)
  - `packages/orbiter-context/tests/test_context_config.py` (22 tests: defaults, frozen, validation, neuron coercion, serialization, factory)
  - `packages/orbiter-context/tests/test_context_state.py` (35 tests: read/write, delete/pop/clear, update, parent inheritance, introspection, bool, repr)
- **Learnings for future iterations:**
  - Creating a new namespace package requires adding it to 3 places in root pyproject.toml: `[tool.uv.workspace] members`, `[dependency-groups] dev`, and `[tool.uv.sources]`
  - Same pyright `reportMissingImports` issue applies to orbiter-context test files — use `# pyright: ignore[reportMissingImports]` on import lines
  - `ContextState` uses `__slots__` for memory efficiency — all instance attributes (`_data`, `_parent`) must be declared there
  - Pydantic v2 frozen models: use `model_validator(mode="after")` for cross-field validation (e.g., summary_threshold <= offload_threshold)
  - Use `tuple[str, ...]` (not `list`) for frozen model fields that should be immutable; accept `list` via `model_validator(mode="before")` coercion
  - 767 tests total across full suite (57 new context tests)
---

## 2026-02-15 - US-024
- What was implemented: Context class with fork/merge for hierarchical task decomposition. `ContextError` exception. `Context.__init__` accepts task_id (required), config, parent, state. Properties: task_id, config, parent, state, children, token_usage. `add_tokens()` for token tracking. `fork(task_id)` creates child context with state inheritance and token snapshot. `merge(child)` consolidates child local state and net token deltas back into parent.
- Files changed: `packages/orbiter-context/src/orbiter/context/context.py` (new, ~160 lines), `packages/orbiter-context/tests/test_context.py` (new, 35 tests in 8 classes), `packages/orbiter-context/tests/test_context_config.py` (import sorting fix from ruff)
- Key implementation details:
  - `_token_snapshot` stores parent's usage at fork time (immutable copy), separate from `_token_usage` which tracks current usage. Merge computes net = current - snapshot
  - `fork()` auto-sets child's config from parent (shared immutable reference), creates ContextState with parent chain, copies token usage as starting point
  - `merge()` validates child.parent identity, merges child's `local_dict()` state, adds net-positive token deltas
  - `__slots__` for memory efficiency, sorted alphabetically per ruff RUF023
- **Learnings for future iterations:**
  - Within-package imports (e.g., `context.py` importing from `config.py` in the same `orbiter.context` namespace package) also need `# pyright: ignore[reportMissingImports]` due to `.pth`-based editable installs
  - Ruff RUF023 requires `__slots__` entries to be sorted alphabetically — use `--fix` to auto-sort
  - For net token merge calculation, storing a separate `_token_snapshot` at fork time is essential — the `_token_usage` dict gets mutated by `add_tokens()` and can't serve as its own baseline
  - 802 tests total across full suite (35 new context tests)
---

## 2026-02-15 - US-025
- What was implemented: TokenTracker — per-agent, per-step token tracking for cost analysis and budget enforcement. `TokenStep` (frozen dataclass with agent_id, step index, prompt/output tokens, total_tokens property). `TokenUsageSummary` (frozen dataclass for aggregated usage). `TokenTracker` (add_step with auto step indexing, get_trajectory, total_usage, agent_usage, agent_ids, steps, len/repr).
- Files changed: `packages/orbiter-context/src/orbiter/context/token_tracker.py` (new, ~130 lines), `packages/orbiter-context/tests/test_token_tracker.py` (new, 24 tests in 7 classes)
- **Learnings for future iterations:**
  - TokenTracker is standalone (no dependencies on Context/ContextState) — it can be used independently or composed into Context
  - `dataclass(frozen=True, slots=True)` is the right pattern for immutable value objects like TokenStep/TokenUsageSummary
  - Step index is per-agent (computed by counting existing steps for the agent), not global — this makes trajectories self-contained
  - The `field` import from dataclasses wasn't needed (no mutable defaults) — ruff F401 catches unused imports
  - 826 tests total across full suite (24 new token tracker tests)
---

## 2026-02-15 - US-026
- What was implemented: Neuron ABC with `__slots__`, `neuron_registry` (Registry[Neuron]), and three built-in neurons: `SystemNeuron` (priority 100 — date/time/platform via `datetime.UTC`), `TaskNeuron` (priority 1 — task_id, input, output, subtask plan from context state), `HistoryNeuron` (priority 10 — windowed conversation history using `ctx.config.history_rounds`).
- Files changed: `packages/orbiter-context/src/orbiter/context/neuron.py` (new, ~130 lines), `packages/orbiter-context/tests/test_neuron.py` (new, 32 tests in 6 classes)
- Key implementation details:
  - Neuron ABC has `name` (str) and `priority` (int) properties, abstract `async format(ctx, **kwargs) -> str`
  - XML-like tags wrap neuron output (e.g., `<system_info>`, `<task_info>`, `<conversation_history>`) for LLM clarity
  - HistoryNeuron windowing: `history_rounds * 2` messages from end (each round = user + assistant)
  - SystemNeuron uses `datetime.UTC` (not `timezone.utc`) per ruff UP017
  - Built-in neurons registered at module level: `neuron_registry.register("system", SystemNeuron())`
- **Learnings for future iterations:**
  - `datetime.UTC` is the preferred alias over `timezone.utc` — ruff UP017 enforces this
  - Registry[Neuron] from orbiter-core works well for neuron discovery across packages
  - The two-level design from old AWorld (format_items → format) was simplified to single `format()` — the old pattern was over-engineered for most use cases
  - 858 tests total across full suite (32 new neuron tests)
---

## 2026-02-15 - US-027
- What was implemented: Six extended neurons (TodoNeuron, KnowledgeNeuron, WorkspaceNeuron, SkillNeuron, FactNeuron, EntityNeuron) added to `neuron.py` with proper priorities. Dynamic variable system (`DynamicVariableRegistry`) with nested path resolution, callable/static resolver registration, decorator form, and `resolve_template()` for `${path}` substitution in `variables.py`.
- Files changed: `packages/orbiter-context/src/orbiter/context/neuron.py` (+180 lines — 6 new neuron classes + registration), `packages/orbiter-context/src/orbiter/context/variables.py` (new, ~132 lines), `packages/orbiter-context/tests/test_extended_neurons.py` (new, 46 tests in 11 classes)
- Key implementation details:
  - All extended neurons follow the same pattern: read from `ctx.state`, return empty string if no data, format with XML-like tags
  - TodoNeuron reads `todos` (list of {item, done}), KnowledgeNeuron reads `knowledge_items` (list of {source, content}), WorkspaceNeuron reads `workspace_artifacts` (list of {name, type, size}), SkillNeuron reads `skills` (list of {name, description, active}), FactNeuron reads `facts` (list of strings), EntityNeuron reads `entities` (list of {name, type})
  - DynamicVariableRegistry: registered resolvers take priority over nested path lookup. `resolve()` checks resolvers first, then falls back to `_resolve_nested()` which walks dot-separated paths through dicts/ContextState/objects. `resolve_template()` uses regex to substitute `${path}` placeholders
  - All 9 neurons are registered at module level in neuron_registry
- **Learnings for future iterations:**
  - Extended neurons are intentionally simple — each reads from a well-known state key and formats it. Complex logic (populating those state keys) belongs in processors/tools, not neurons
  - DynamicVariableRegistry is separate from neuron system — neurons produce prompt fragments, variables resolve individual values. The PromptBuilder (US-028) will likely use both
  - `re.sub` with a callable replacement function is clean for template resolution with error handling per-variable
  - 904 tests total across full suite (46 new: 32 neuron tests, 14 variable tests)
---

## 2026-02-15 - US-028
- What was implemented: PromptBuilder — composes neurons in priority order to build rich system prompts. `PromptBuilder.__init__(ctx, *, variables, separator)`, `add(neuron_name, **kwargs)` for registry lookup, `add_neuron(neuron, **kwargs)` for direct instances, `async build()` resolves all neurons by priority, filters empty results, joins with separator, and optionally resolves `${path}` template variables via DynamicVariableRegistry. Also: `clear()`, `__len__`, `__repr__`, method chaining. Internal `_NeuronEntry` pairs neuron with kwargs.
- Files changed: `packages/orbiter-context/src/orbiter/context/prompt_builder.py` (new, ~130 lines), `packages/orbiter-context/tests/test_prompt_builder.py` (new, 30 tests in 9 classes)
- Test coverage: init/repr (3), add neurons by name/instance/chaining (5), build with empty/single/multiple/custom separator/filtered empty (6), priority ordering/stability/registered neurons (3), variable resolution/unresolvable/no-vars/multiple/across-neurons (5), kwargs forwarding (1), clear (2), context traversal with parent state/overridden state (2), integration with built-in neurons and full composition (3)
- **Learnings for future iterations:**
  - Ruff I001 import sorting and pyright `reportMissingImports` ignore comments conflict on multi-line parenthesized imports: ruff reformats `from X import Y  # pyright: ignore[...]` into `from X import (\n    Y,  # pyright: ignore[...]\n)`, but pyright reports the error on the `from` line, not the import name line. Solution: put `# pyright: ignore[reportMissingImports]` on the `from` line: `from X import (  # pyright: ignore[reportMissingImports]\n    Y,\n)`
  - PromptBuilder's `build()` uses Python's stable sort (`sorted()`) to preserve insertion order for neurons with equal priority — important for deterministic prompt construction
  - Template variable resolution happens AFTER joining all neuron fragments — this means variables can span across neuron outputs but resolution applies to the final composed string
  - 934 tests total across full suite (30 new prompt builder tests)
---

## 2026-02-15 - US-029
- What was implemented: ContextProcessor pipeline — event-driven context processing system. `ContextProcessor` ABC with `event` (str) and `async process(ctx, payload)`. `ProcessorPipeline` with `register()` (chaining), `unregister()`, `fire(event, ctx, payload)`, `has_processors()`, `list_processors()`, `clear()`, `__len__`, `__repr__`. Two built-in processors: `SummarizeProcessor` (`pre_llm_call` — marks context for summarization when history exceeds `ctx.config.summary_threshold`, stores excess messages as `summary_candidates`), `ToolResultOffloader` (`post_tool_call` — offloads large tool results exceeding `max_size` to `offloaded_results` in state, replaces payload with truncated reference).
- Files changed: `packages/orbiter-context/src/orbiter/context/processor.py` (new, ~190 lines), `packages/orbiter-context/tests/test_context_processor.py` (new, 39 tests in 7 classes)
- Test coverage: ABC tests (5: create, name, empty event error, repr, abstract enforcement), Pipeline registration (10: register, multiple same event, different events, unregister, unregister-not-registered, chaining, clear, list-all, list-filtered, repr), Pipeline fire (7: calls processor, no-match, default payload, sequential order, state mutation, error propagation, stops on first error), SummarizeProcessor (5: defaults, no history, below threshold, at threshold, exceeds threshold + pipeline), ToolResultOffloader (7: defaults, custom max_size, no result, small result, large result offload, multiple accumulate, default tool name + pipeline), Integration (3: multiple events, chain mutation, full lifecycle)
- **Learnings for future iterations:**
  - Ruff SIM105 requires `contextlib.suppress(ValueError)` instead of try/except/pass — already consistent with HookManager pattern in hooks.py
  - ProcessorPipeline follows similar patterns to HookManager (sequential execution, error propagation) but uses string event types instead of enum values, which is more flexible for user-defined events
  - ToolResultOffloader mutates the `payload` dict in-place to replace tool results — this is by design since the caller needs to see the modified payload
  - 973 tests total across full suite (39 new processor tests)
---

## 2026-02-15 - US-030
- What was implemented: Workspace + artifact system — persistent artifact storage with versioning and observer notifications. `WorkspaceError`, `ArtifactType` (StrEnum: TEXT, CODE, MARKDOWN, JSON, CSV, IMAGE), `ArtifactVersion` (immutable snapshot with content + timestamp), `Artifact` (named artifact with type, content, version history), `ObserverCallback` type alias, `Workspace` (workspace_id, storage_path, write/read/get/list/delete, version_history/revert_to_version, observer on/notify, filesystem persistence).
- Files changed: `packages/orbiter-context/src/orbiter/context/workspace.py` (new, ~210 lines), `packages/orbiter-context/tests/test_workspace.py` (new, 40 tests in 8 classes)
- Test coverage: ArtifactType (2: values, StrEnum), ArtifactVersion (3: creation, auto-timestamp, repr), Artifact (4: creation, default type, add version, repr), Workspace init (4: creation, empty id, storage path, repr), CRUD (10: write/read, read missing, get, get missing, empty name, list all, list by type, delete, delete missing, overwrite), Versioning (6: history, missing history, revert, revert missing, invalid version, negative version), Observers (6: on_create, on_update, on_delete, multiple, chaining, no observer), Filesystem (5: write persists, delete removes, overwrite updates, no storage path, revert persists)
- **Learnings for future iterations:**
  - `shutil.rmtree()` is the clean way to recursively delete artifact directories on delete
  - Observer pattern uses `dict[str, list[ObserverCallback]]` — simple and extensible for custom event names beyond on_create/on_update/on_delete
  - `Workspace.on()` returns `self` for chaining — consistent with ProcessorPipeline.register() pattern
  - Artifact versioning uses append-only `_versions` list — revert creates a NEW version with old content rather than truncating history
  - 1013 tests total across full suite (40 new workspace tests)
---

## 2026-02-15 - US-031
- What was implemented: Workspace-retriever integration — `KnowledgeStore` with in-memory artifact indexing, text chunking with overlap, keyword search (TF-IDF-like scoring), and chunk range queries. Workspace auto-indexes artifacts on write/update and de-indexes on delete via `knowledge_store` parameter.
- Files changed:
  - `packages/orbiter-context/src/orbiter/context/_internal/__init__.py` (new, empty)
  - `packages/orbiter-context/src/orbiter/context/_internal/knowledge.py` (new, ~195 lines — KnowledgeError, Chunk, SearchResult, chunk_text(), KnowledgeStore with add/remove/get/get_range/search)
  - `packages/orbiter-context/src/orbiter/context/workspace.py` (modified — added `knowledge_store` param, `_index_artifact`, `_deindex_artifact` methods, auto-indexing in write/delete)
  - `packages/orbiter-context/tests/test_knowledge.py` (new, 41 tests in 11 classes)
- Test coverage: chunk_text (8: short/empty/exact/overlap/no-overlap/invalid-size/invalid-overlap/negative), Chunk (2: creation, immutability), SearchResult (1), KnowledgeStore init (3: defaults, custom, repr), add (5: short/long/empty-name/re-add/multiple), remove (2: existing/missing), get (2: existing/missing), get_range (3: range/missing/no-overlap), search (7: keyword/multi-term/no-results/empty/top_k/ranking/empty-store), workspace integration (8: auto-index/update-re-index/delete-removes/no-store/round-trip/multi-artifact/property/default)
- **Learnings for future iterations:**
  - `_internal/` directory pattern: US-033 will enhance KnowledgeStore with more features — the module is in `_internal/knowledge.py` so US-033 can extend it
  - Workspace `knowledge_store` typed as `Any` to avoid circular import — duck-typing: any object with `add(name, content)` and `remove(name)` methods works
  - chunk_text's loop uses `end >= len(text)` break condition, so the last chunk may be full-size (not always short) when end exactly equals text length
  - TF-IDF scoring uses `log(1 + tf)` per term, which dampens high-frequency terms — simple but effective for keyword search
  - 1054 tests total across full suite (41 new knowledge/integration tests)
---

## 2026-02-15 - US-032
- What was implemented: Checkpoint system — save and restore complete execution state for long-running tasks. `Checkpoint` (frozen dataclass with task_id, version, values, token_usage, metadata, created_at; includes `to_dict()`/`from_dict()` serialization). `CheckpointStore` (per-session versioned store with `save()`, `get(version)`, `latest`, `list_versions()`). `Context.snapshot()` captures state + token usage into a versioned checkpoint. `Context.restore()` classmethod creates a fresh context from a checkpoint.
- Files changed:
  - `packages/orbiter-context/src/orbiter/context/checkpoint.py` (new, ~160 lines — CheckpointError, Checkpoint, CheckpointStore)
  - `packages/orbiter-context/src/orbiter/context/context.py` (added ~70 lines — `_checkpoint_store` slot, `checkpoints` property, `snapshot()`, `restore()` classmethod)
  - `packages/orbiter-context/tests/test_checkpoint.py` (new, 35 tests in 7 classes)
- Test coverage: Checkpoint creation/metadata/immutable/repr/to_dict/from_dict/roundtrip (7), CheckpointStore creation/empty-id/save/version-increment/metadata/get/invalid-version/latest/list-versions/repr (10), Context.snapshot basic/empty/metadata/version-increment/captures-current (5), Context.restore basic/config/fresh-context/no-parent/no-checkpoints/invalid-type (6), Checkpoints property/per-context/forked-own-store (3), Full roundtrip/old-version/serialization/fork-snapshot (4)
- **Learnings for future iterations:**
  - Checkpoint is a frozen dataclass (not Pydantic) since it's a simple value object — no need for validation beyond what the store provides
  - CheckpointStore uses 1-based versioning (version starts at 1, not 0) — consistent with user-facing version numbering
  - Context.restore() is a classmethod that creates a completely fresh context with no parent/children/checkpoint-history — it's a clean slate with restored state values
  - Forked child's `snapshot()` captures `to_dict()` which includes inherited parent values — so restoring captures the full merged view, not just local state
  - 1089 tests total across full suite (35 new checkpoint tests)
---

## 2026-02-15 - US-033
- What was implemented: US-033 acceptance criteria were already fully satisfied by US-031's implementation. Verified: `_internal/__init__.py` exists, `_internal/knowledge.py` has complete KnowledgeStore (234 lines) with `add`, `search`, `get`, `get_range`, `remove`, `artifact_names`, `total_chunks`. Basic chunking via `chunk_text()` with overlap support. TF-IDF keyword search serving as the in-memory search for testing. 41 existing tests cover all criteria.
- Files: No changes needed — all code already in place from US-031
- Quality checks: ruff check ✓, ruff format ✓, pyright 0 errors ✓, pytest 1089 passed ✓
- **Learnings for future iterations:**
  - US-031 (workspace-retriever integration) already implemented the full KnowledgeStore that US-033 was supposed to create — check for pre-existing implementations before coding
  - When a PRD story's acceptance criteria are already met by prior work, verify quality checks and mark as passing rather than duplicating code
---

## 2026-02-15 - US-034
- What was implemented: Context tools — planning, knowledge, and file tools for agent self-management. `_ContextTool` subclass of `Tool` with `bind(ctx)` pattern to inject context at execution time while hiding `ctx` from JSON schema. Planning tools (`add_todo`, `complete_todo`, `get_todo`) for task checklist management via `ctx.state`. Knowledge tools (`get_knowledge`, `grep_knowledge`, `search_knowledge`) for artifact retrieval via workspace and knowledge store. File tool (`read_file`) with path traversal protection via `resolve()` + prefix check.
- Files changed:
  - `packages/orbiter-context/src/orbiter/context/tools.py` (new, ~255 lines — _ContextTool, 7 tools, 4 factory functions)
  - `packages/orbiter-context/tests/test_context_tools.py` (new, 38 tests in 11 classes)
- Key implementation details:
  - `_ContextTool` extends `Tool` ABC with `bind(ctx)` method — context injected at runtime, stripped from JSON schema via post-processing of `_generate_schema()` output
  - Planning tools store todos as `list[dict]` in `ctx.state["todos"]` with `{item, done}` shape
  - Knowledge tools read from `ctx.state["workspace"]` (Workspace) and `ctx.state["knowledge_store"]` (KnowledgeStore)
  - `grep_knowledge` uses `re.compile(pattern, re.IGNORECASE)` with invalid regex error handling
  - `read_file` prevents path traversal via `Path.resolve()` + `str.startswith()` prefix check against working directory
  - Factory functions: `get_planning_tools()`, `get_knowledge_tools()`, `get_file_tools()`, `get_context_tools()` (all 7)
- **Learnings for future iterations:**
  - orbiter-context depends on orbiter-core, so `@tool` decorator and `Tool` ABC can be imported directly — no need for cross-package workarounds
  - `_ContextTool` pattern: custom Tool subclass that strips `ctx` from schema and injects it via `bind()` — this avoids exposing internal context to the LLM while keeping tools testable
  - Module-level tool singletons (e.g., `planning_tool_add`) are mutable via `bind()` — tests must call `bind(ctx)` before each test to set the right context
  - ruff auto-fix may move `# pyright: ignore[reportMissingImports]` from the `from` line to individual import names inside parenthesized imports — the ignore must stay on the `from` line for pyright to see it
  - 1127 tests total across full suite (38 new context tools tests)
---

## 2026-02-15 - US-035
- What was implemented: Context package public API (`__init__.py` with 23 exports) and 26 integration tests. All exports organized in `__all__`. Agent context wiring already in place from US-003 (verified by integration tests).
- Files changed:
  - `packages/orbiter-context/src/orbiter/context/__init__.py` (rewritten — 23 public exports: Context, ContextConfig, ContextState, ContextError, AutomationMode, make_config, PromptBuilder, Neuron, neuron_registry, ContextProcessor, ProcessorPipeline, SummarizeProcessor, ToolResultOffloader, Workspace, ArtifactType, Checkpoint, CheckpointStore, TokenTracker, get_context_tools, get_planning_tools, get_knowledge_tools, get_file_tools)
  - `packages/orbiter-context/tests/test_context_integration.py` (new, 26 tests in 7 test classes)
- Integration test coverage:
  - Public API imports (8 tests): core classes, prompt building, processors, workspace, checkpoint, config, token tracker, tool factories
  - Context+PromptBuilder E2E (4 tests): basic prompt building, history, todos, full composition with priority ordering
  - Context+Processor E2E (3 tests): SummarizeProcessor trigger, ToolResultOffloader, multi-processor pipeline
  - Context+Workspace E2E (3 tests): write + knowledge search round-trip, versioning via context state, filesystem persistence
  - Full lifecycle (3 tests): create→populate→process→build prompt→checkpoint→restore, fork/merge with workspace, context tools with workspace
  - Custom processor (2 tests): custom processor mutates state, mixed with built-in processors
  - Agent context wiring (3 tests): Agent accepts context, default None, describe() stability
- **Learnings for future iterations:**
  - TaskNeuron reads from `task_input` state key (not `input`) — use the correct state key names as defined in each neuron's docstring
  - Agent.context is already wired as `Any` type from US-003 — no additional code changes needed in agent.py for this story
  - The `__init__.py` pattern for namespace packages: use `# pyright: ignore[reportMissingImports]` on each `from` line since editable installs use `.pth` files
  - 1153 tests total across full suite (26 new integration tests)
---

## 2026-02-15 - US-036
- What was implemented: Memory interface + types for the orbiter-memory package. `MemoryStatus` (StrEnum: draft/accepted/discard) with valid transition map. `MemoryMetadata` (frozen Pydantic: user_id, session_id, task_id, agent_id, extra). `MemoryItem` base class with id (auto UUID), content, memory_type, status, metadata, timestamps, `transition()` method with validation. Four subtypes: `SystemMemory` (system prompts), `HumanMemory` (user messages), `AIMemory` (assistant + tool_calls list), `ToolMemory` (tool results + tool_call_id, tool_name, is_error). `MemoryStore` runtime_checkable Protocol with async add/get/search/clear methods.
- Files changed: `packages/orbiter-memory/src/orbiter/memory/base.py` (new, ~120 lines), `packages/orbiter-memory/tests/test_memory_base.py` (new, 38 tests in 10 classes)
- Test coverage: MemoryStatus (2), MemoryMetadata (4: defaults, values, extra, frozen), MemoryItem (5: creation, custom id, default metadata, with metadata, draft status), Status transitions (7: valid transitions, invalid transitions, timestamp update), SystemMemory (2), HumanMemory (2), AIMemory (3), ToolMemory (3), Protocol conformance (10: isinstance, add/get, search by type/query/metadata/status/limit, clear all, clear with filter)
- **Learnings for future iterations:**
  - orbiter-memory package stub already existed (pyproject.toml, __init__.py, backends/__init__.py) — just needed base.py
  - For test classes that aren't dataclasses, ruff RUF012 does NOT apply to mutable class attributes — no need for ClassVar. But pyright DOES flag `ClassVar` + `__init__` reassignment as `reportAttributeAccessIssue`. Best to avoid ClassVar in regular classes.
  - `runtime_checkable` Protocol allows `isinstance()` checks — useful for verifying that store implementations satisfy the protocol
  - MemoryItem uses mutable Pydantic model (no frozen=True) since `transition()` mutates status and updated_at — same pattern as RunNode from state.py
  - Status transitions use an explicit transition map dict rather than methods per status — cleaner and more maintainable
  - 1191 tests total across full suite (38 new memory tests)
---

## 2026-02-15 - US-037
- What was implemented: ShortTermMemory — in-memory conversation store implementing MemoryStore protocol with scope-based filtering, configurable windowing, and tool call/response integrity filtering.
- Files changed: `packages/orbiter-memory/src/orbiter/memory/short_term.py` (new, ~130 lines), `packages/orbiter-memory/tests/test_short_term.py` (new, 32 tests in 8 classes)
- Key implementation details:
  - `ShortTermMemory(scope, max_rounds)` — scope: "user"|"session"|"task" (default "task"), max_rounds: 0 = unlimited
  - `_matches_metadata()` — scope-aware metadata matching: user scope ignores session/task, session scope ignores task, task scope checks all
  - `_window()` — finds human message positions, cuts at Nth-from-last, always preserves system messages before the cut
  - `_filter_incomplete_pairs()` — walks backwards removing trailing AI messages with unmatched tool_calls and orphan ToolMemory without matching AI call
  - All MemoryStore protocol methods implemented: add, get, search, clear
- Test coverage: init (5), add/get (3), basic search (5), scope filtering (4), windowing (4), tool call integrity (5), clear (3), integration (3)
- **Learnings for future iterations:**
  - Test helper functions with `**kw: str` conflict with other keyword args (e.g., `tool_calls: list`) when called with `**dict` — pyright treats the dict as potentially containing any string key. Fix: use explicit `meta` parameter instead of `**kw` pattern
  - Windowing by counting rounds from the end: use forward scan to find human message positions, then slice at `positions[-max_rounds]` — much simpler than backward counting which gets the boundary wrong
  - `_filter_incomplete_pairs` walks backwards in a `while` loop, popping trailing items that violate integrity — this handles both dangling AI tool calls and orphan tool results
  - 1223 tests total across full suite (32 new short-term memory tests)
---

## 2026-02-15 - US-038
- What was implemented: Summary trigger logic and multi-template summary generation. `SummaryTemplate` (StrEnum: conversation/facts/profiles) with default prompt templates. `SummaryConfig` (frozen dataclass: message_threshold, token_threshold, templates, prompts, keep_recent, token_estimate_ratio) with `get_prompt()` fallback. `check_trigger()` detects when message count or estimated token count exceeds thresholds. `Summarizer` runtime_checkable Protocol for pluggable LLM backends. `generate_summary()` splits items into compress-vs-keep-recent, formats conversation text, runs each configured template through the summarizer. `SummaryResult` with summaries dict, compressed_items, original_count. Helper: `_estimate_tokens()` (char/ratio), `_format_items()` (role-tagged lines).
- Files changed: `packages/orbiter-memory/src/orbiter/memory/summary.py` (new, ~130 lines), `packages/orbiter-memory/tests/test_summary.py` (new, 33 tests in 10 classes)
- Test coverage: SummaryTemplate values + default prompts (2), SummaryConfig defaults/custom/frozen/get_prompt (6), _estimate_tokens empty/single/multiple/custom-ratio (4), TriggerResult creation (1), check_trigger below/message/token/exact/empty/priority (6), _format_items empty/single/multi-type (3), generate_summary empty/single/multi-template/keep-recent/larger-than/custom-prompt/formatted-items (7), SummaryResult creation/defaults (2), Summarizer protocol conformance (2)
- **Learnings for future iterations:**
  - `Sequence` import: ruff UP035 requires `from collections.abc import Sequence` instead of `from typing import Sequence`
  - `SummaryConfig` uses frozen dataclass (not Pydantic) since it's a simple value object with no validation beyond defaults — consistent with `TokenStep` and `Checkpoint` patterns
  - The `Summarizer` protocol is intentionally minimal (single `async summarize(prompt) -> str` method) — callers wrap their LLM provider in this interface, keeping the summary module decoupled from any specific provider
  - `generate_summary` always generates summaries for all configured templates, even when `keep_recent >= len(items)` — this is by design since the summaries capture information from the conversation regardless of compression
  - 1256 tests total across full suite (33 new summary tests)
---

## 2026-02-15 - US-039
- What was implemented: Long-term memory orchestrator with `LongTermMemory` (MemoryStore-compatible persistent store with content deduplication and namespace isolation), `MemoryOrchestrator` (async LLM extraction of UserProfile, AgentExperience, Facts), `ExtractionTask` (lifecycle: PENDING→RUNNING→COMPLETED/FAILED), `Extractor` protocol, `OrchestratorConfig`, and processing task queue with status tracking.
- Files changed: `packages/orbiter-memory/src/orbiter/memory/long_term.py` (new, ~290 lines), `packages/orbiter-memory/tests/test_long_term.py` (new, 47 tests in 12 classes)
- Key implementation details:
  - `LongTermMemory` stores items in a dict[str, MemoryItem] for O(1) lookup by ID. Deduplicates by content+memory_type match (skips duplicate adds). Search sorts by created_at descending (newest first).
  - `ExtractionType` (StrEnum): user_profile, agent_experience, facts — each with default extraction prompts containing `{content}` placeholder.
  - `ExtractionTask` dataclass with start()/complete(result)/fail(error) lifecycle methods and completion timestamps.
  - `Extractor` runtime_checkable Protocol (single `async extract(prompt) -> str` method) — decoupled from LLM provider, similar to `Summarizer` protocol in summary.py.
  - `MemoryOrchestrator` manages task queue: `submit()` creates tasks (one per extraction type), `process()` runs single task through extractor and stores result in LongTermMemory, `process_all()` processes all pending tasks, `get_task()`/`list_tasks()` for task introspection.
  - Failed extraction tasks mark FAILED with error message but don't store anything in memory — clean failure handling.
- **Learnings for future iterations:**
  - `Extractor` protocol follows the same pattern as `Summarizer` — minimal single-method protocol for LLM integration. Callers wrap their provider.
  - `LongTermMemory` deduplication is by exact content+type match — intentionally simple. More sophisticated deduplication (semantic similarity) would require vector embeddings (US-042).
  - `_format_extraction_items` reuses the same `[ROLE]: content` format as `_format_items` in summary.py — could be shared but kept separate to avoid cross-module coupling.
  - `MemoryOrchestrator.process()` stores extracted knowledge with the extraction type as `memory_type` (e.g., "facts", "user_profile") — this allows `search(memory_type="facts")` to filter by extraction type.
  - 1303 tests total across full suite (47 new long-term memory tests)
---

## 2026-02-15 - US-040
- What was implemented: SQLiteMemoryStore — SQLite-backed persistent memory store implementing the MemoryStore protocol. Uses `aiosqlite` for async database access. Features: JSON metadata indexes via `json_extract()`, soft deletes (deleted flag), upsert with version bumping (`ON CONFLICT DO UPDATE SET version = version + 1`), async context manager lifecycle (`async with SQLiteMemoryStore(...) as store:`), and full MemoryItem subtype reconstruction on read (dispatches to SystemMemory/HumanMemory/AIMemory/ToolMemory based on `memory_type` column).
- Files changed:
  - `packages/orbiter-memory/src/orbiter/memory/backends/sqlite.py` (new, ~230 lines — SQLiteMemoryStore, _extra_fields, _row_to_item, SQL schema with indexes)
  - `packages/orbiter-memory/tests/test_sqlite.py` (new, 34 tests in 9 classes)
- Test coverage: ProtocolConformance (1: isinstance MemoryStore check), Lifecycle (5: init/close, double init idempotent, context manager, pre-init RuntimeError, repr), AddGet (6: add+get, get nonexistent, upsert, SystemMemory, AIMemory with tool_calls, ToolMemory), Search (9: all, query, memory_type, status, metadata user_id, limit, newest-first ordering, session+task metadata, agent_id metadata), Clear (5: clear all, clear with metadata filter, soft-delete hides from get, soft-delete preserved in DB, re-add after soft-delete), Version (2: initial version=1, version increments on upsert), Count (3: empty, active, with deleted), FilePersistence (2: data persists across connections, metadata persists), CustomMemoryType (1: unknown type returns base MemoryItem)
- **Learnings for future iterations:**
  - `aiosqlite` needs to be installed separately — it's an optional dep (`orbiter-memory[sqlite]`), but `uv pip install aiosqlite` for dev
  - SQLite `json_extract()` works well for metadata filtering — no need for separate metadata columns
  - Soft delete pattern: `deleted INTEGER DEFAULT 0` with `WHERE deleted = 0` in all queries, `SET deleted = 1` for clear — allows `count(include_deleted=True)` for auditing
  - Upsert via `ON CONFLICT(id) DO UPDATE` is cleaner than separate INSERT/UPDATE logic
  - Row factory `aiosqlite.Row` enables dict-like access (`row["column_name"]`)
  - Subtype reconstruction from DB: dispatch on `memory_type` column to construct the right Pydantic model class, extra fields (tool_calls, tool_call_id, etc.) stored in `extra_json` column
  - ruff S608 (SQL injection) is not enabled in this project — no need for `# noqa: S608` comments
  - 1337 tests total across full suite (34 new SQLite backend tests)
---

## 2026-02-15 - US-041
- What was implemented: PostgresMemoryStore — asyncpg-backed persistent memory store implementing the MemoryStore protocol. Uses `asyncpg.create_pool()` for connection pooling, JSONB columns for metadata (with expression indexes), soft deletes, upsert with version bumping via `ON CONFLICT`, `$N` positional parameter binding, ILIKE for case-insensitive text search, and full MemoryItem subtype reconstruction on read.
- Files changed:
  - `packages/orbiter-memory/src/orbiter/memory/backends/postgres.py` (new, ~230 lines — PostgresMemoryStore, _parse_rowcount, _extra_fields, _row_to_item, SQL schema with JSONB indexes)
  - `packages/orbiter-memory/tests/test_postgres.py` (new, 39 tests — 33 unit + 6 integration)
- Test coverage: ProtocolConformance (1), Lifecycle (5: init/close, double init, context manager, pre-init RuntimeError, repr), AddGet (2: add+get, get nonexistent), Search (6: all, query/ILIKE, memory_type, status, metadata/JSONB, limit), Clear (2: all, with metadata filter), Count (3: empty, active, includes deleted), Helpers (7: _parse_rowcount x4, _extra_fields x3), RowToItem (7: human, system, ai+tool_calls, tool, unknown type, metadata from dict, metadata from JSON string), Integration (6: skipped — add+get, search, clear+count, upsert, metadata search, subtype reconstruction)
- **Learnings for future iterations:**
  - asyncpg `Pool.acquire()` returns a synchronous context manager (not a coroutine) — mock it with `MagicMock()` (not `AsyncMock()`) for `acquire`, but use `AsyncMock` for `__aenter__`/`__aexit__`
  - asyncpg uses `$N` positional parameters (not `?` like SQLite) — build parameter index counter (`idx`) when constructing dynamic WHERE clauses
  - asyncpg `conn.execute()` for UPDATE returns a status string like `"UPDATE 3"` — parse with `_parse_rowcount()` to get affected row count
  - asyncpg JSONB columns return Python dicts directly (not JSON strings) — `_row_to_item` handles both `str` and `dict` for metadata/extra_json via `isinstance` check
  - PostgreSQL uses `ILIKE` for case-insensitive LIKE (vs SQLite's case-insensitive `LIKE` by default)
  - `_mock_pool()` returns `(pool, conn)` tuple — pool uses `MagicMock` (sync `.acquire()`), conn uses `AsyncMock` (async `.execute()`, `.fetchrow()`, etc.)
  - 1370 tests total across full suite (33 new unit tests, 6 skipped integration tests)
---

## 2026-02-15 - US-042
- What was implemented: Embeddings ABC + VectorMemoryStore for semantic memory retrieval. `Embeddings` ABC with `embed()` (sync), `aembed()` (async), and `dimension` property. `OpenAIEmbeddings` (OpenAI-compatible provider with lazy import, dimension support, `asyncio.to_thread()` for async). `_cosine_similarity()` for vector comparison. `VectorMemoryStore` implementing MemoryStore protocol with in-memory vector storage, cosine similarity ranking, and post-filtering by metadata/memory_type/status.
- Files changed:
  - `packages/orbiter-memory/src/orbiter/memory/backends/vector.py` (new, ~195 lines — Embeddings ABC, OpenAIEmbeddings, VectorMemoryStore, _cosine_similarity, _matches_metadata)
  - `packages/orbiter-memory/tests/test_vector.py` (new, 35 tests in 9 classes)
- Test coverage: EmbeddingsABC (5: instantiation, implements, sync/async, empty text), OpenAIEmbeddings (2: subclass, dimension), CosineSimilarity (5: identical, opposite, orthogonal, zero, known value), VectorProtocol (1: isinstance MemoryStore), VectorLifecycle (2: init, repr), VectorAddGet (5: add+get, nonexistent, computes embedding, multiple, upsert), VectorSearch (8: semantic ranking, no-query newest, limit, empty, memory_type, status, metadata, combined filters), VectorClear (4: all, metadata, removes vectors, empty), EmbeddingCalls (3: add calls embed, search calls for query, no query no embed)
- **Learnings for future iterations:**
  - `zip()` requires `strict=` parameter — ruff B905 enforces this. Use `strict=False` for embedding cosine similarity where vectors should be same length but we don't want to crash on mismatch
  - ruff SIM103 requires inlining final `if X: return False; return True` as `return not X` — cleaner for metadata matching
  - MockEmbeddings pattern for tests: deterministic vectors from char-code hashing, plus FixedEmbeddings with pre-set lookup table for precise similarity testing
  - VectorMemoryStore is in-memory only — production use would integrate with chromadb (already in optional deps as `orbiter-memory[vector]`). The in-memory implementation serves as the base pattern and test interface
  - 1405 tests total across full suite (35 new vector store tests)
---

## 2026-02-15 - US-043
- What was implemented: Memory package public API (`__init__.py` with 28 exports and `__all__`) and `MemoryEventEmitter` for event-driven memory integration. Updated `__init__.py` to export all memory types, stores, summary utilities, and event constants. Created `events.py` with `MemoryEventEmitter` wrapping any MemoryStore + EventBus to emit `memory:added`, `memory:searched`, `memory:cleared` events on operations. Agent already accepts `memory` param from US-003 — integration tests verify wiring.
- Files changed:
  - `packages/orbiter-memory/src/orbiter/memory/__init__.py` (rewritten — 28 public exports: base types, short-term, long-term, summary, events)
  - `packages/orbiter-memory/src/orbiter/memory/events.py` (new, ~85 lines — MEMORY_ADDED/SEARCHED/CLEARED constants, MemoryEventEmitter)
  - `packages/orbiter-memory/tests/test_memory_integration.py` (new, 25 tests in 10 classes)
- Test coverage: PublicAPIImports (6: base types, subtypes, short-term, long-term, summary, events), EventEmitterInit (3: defaults, custom bus, repr), EventEmitterAdd (2: emits event, persists to store), EventEmitterGet (2: delegates, missing), EventEmitterSearch (2: emits event, filters), EventEmitterClear (2: emits event, metadata filter), AgentMemoryWiring (3: store, emitter, default none), EndToEnd (5: short→summary pipeline, short→long extraction, event-driven pipeline, protocol conformance, multi-handler)
- **Learnings for future iterations:**
  - RUF022 requires `__all__` to be sorted alphabetically (isort-style) — ruff `--unsafe-fixes` auto-sorts but moves comment groups around. Best to just sort alphabetically from the start and skip grouping comments.
  - `MemoryEventEmitter` follows a decorator/wrapper pattern rather than subclassing MemoryStore — this avoids Protocol subclassing issues while still satisfying the protocol interface duck-typing
  - Agent.memory was already wired as `Any` from US-003 — no additional code changes needed in agent.py. The memory param accepts any MemoryStore or MemoryEventEmitter
  - The `events.py` module imports `EventBus` from orbiter-core (which orbiter-memory depends on) — no additional workspace deps needed
  - 1430 tests total across full suite (25 new integration tests)
---

## 2026-02-15 - US-044
- What was implemented: MCP client with multiple transport support (stdio, SSE, streamable-http) and server instance caching/reuse. `MCPClientError` exception, `MCPTransport` (StrEnum: stdio/sse/streamable_http), `MCPServerConfig` (validated config with transport-specific fields), `MCPServerConnection` (live connection with connect/list_tools/call_tool/cleanup, async context manager, tool caching with invalidation), `MCPClient` (high-level multi-server manager with auto-connect, caching/reuse, connect_all/disconnect_all).
- Files changed:
  - `packages/orbiter-mcp/src/orbiter/mcp/client.py` (new, ~280 lines — MCPClientError, MCPTransport, MCPServerConfig, MCPServerConnection, MCPClient)
  - `packages/orbiter-mcp/tests/test_mcp_client.py` (new, 45 tests in 14 classes)
- Test coverage: MCPTransport (2), MCPServerConfig (9: creation, validation, defaults, custom, repr), MCPServerConnectionInit (2), MCPServerConnectionConnect (4: success, idempotent, failure, validation), MCPServerConnectionListTools (4: list, not-connected, cached, invalidate), MCPServerConnectionCallTool (2: call, not-connected), MCPServerConnectionCleanup (2: cleanup, async-ctx), MCPServerConnectionTransports (3: stdio, sse, streamable-http), MCPClientInit (6: creation, add, multiple, remove, remove-nonexistent, validates), MCPClientConnect (4: single, unknown, cached, connect-all), MCPClientDisconnect (2: single, disconnect-all), MCPClientToolOperations (3: list, call, auto-connect), MCPClientContextManager (2: async-ctx, repr)
- **Learnings for future iterations:**
  - `__slots__` makes attributes read-only at instance level — `patch.object(instance, "method")` fails. Use `patch.object(ClassName, "method")` instead to patch at class level
  - `@asynccontextmanager` functions return single-use context managers — when mocking a method that returns an async context manager and the method is called multiple times, use `side_effect=lambda: factory()` instead of `return_value=single_cm` so each call gets a fresh context manager
  - MCP SDK `mcp>=1.0` provides `stdio_client`, `sse_client`, `streamablehttp_client` — all return async context managers yielding `(read_stream, write_stream)` or `(read_stream, write_stream, get_session_id)` for streamable-http
  - `ClientSession` is itself an async context manager — enter it after entering the transport to get initialize → session lifecycle management
  - 1475 tests total across full suite (45 new MCP client tests)
---

## 2026-02-15 - US-045
- What was implemented: MCP tool schema extraction, conversion to Orbiter Tool format, namespace mapping, and filtering. `MCPToolError` exception. `MCPToolFilter` with include/exclude whitelist/blacklist (exclude takes priority). `namespace_tool_name()` creates `{ns}__{server}__{tool}` names with special char sanitization. `parse_namespaced_name()` for reverse parsing. `extract_schema()` extracts JSON Schema from MCPTool inputSchema. `MCPToolWrapper` (Tool subclass) wraps MCP tools for Orbiter agent use — delegates `execute()` to a `call_fn(tool_name, arguments)` async callable, handles error results and multi-content responses. `convert_mcp_tools()` batch conversion with optional filtering. `load_tools_from_connection()` and `load_tools_from_client()` high-level loaders.
- Files changed:
  - `packages/orbiter-mcp/src/orbiter/mcp/tools.py` (new, ~240 lines — MCPToolError, MCPToolFilter, namespace functions, extract_schema, MCPToolWrapper, convert/load functions)
  - `packages/orbiter-mcp/tests/test_mcp_tools.py` (new, 44 tests in 10 classes)
- Test coverage: MCPToolFilterInit (5: defaults, include-only, exclude-only, exclude-priority, repr), MCPToolFilterApply (3: empty, include, exclude), NamespaceToolName (4: basic, custom-ns, sanitize-special, preserve-underscores), ParseNamespacedName (5: basic, roundtrip, nested-underscores, invalid-format, invalid-one-separator), ExtractSchema (3: basic, empty, returns-copy), MCPToolWrapperInit (5: creation, custom-ns, default-desc, schema-extraction, to_schema), MCPToolWrapperExecute (5: success, empty-args, error-result, call-fn-exception, multi-content), ConvertMCPTools (5: basic, with-filter, custom-ns, empty, all-filtered), LoadFromConnection (5: basic, filter, error, custom-ns, execution-wired), LoadFromClient (4: single, multiple, filter, empty)
- **Learnings for future iterations:**
  - `CallToolResult.content` is a union of `TextContent | ImageContent | AudioContent | ResourceLink | EmbeddedResource` — pyright errors on `hasattr(item, "text")` since some union members lack `.text`. Use `getattr(item, "text", None)` instead
  - `MCPToolWrapper.execute()` passes `kwargs or None` to the call_fn — when kwargs is empty `{}`, MCP `call_tool` expects `None` (not `{}`) for no-argument tools
  - `MCPToolFilter` uses sets internally for O(1) lookup — simple and fast for typical tool counts
  - Namespace format `mcp__server__tool` uses double underscore as separator with `split("__", 2)` to handle tool names that may contain `__`
  - 1519 tests total across full suite (44 new MCP tools tests)
---

## 2026-02-15 - US-046
- What was implemented: `@mcp_server()` class decorator that converts Python classes into MCP servers via FastMCP. `MCPServerError` exception. `MCPServerRegistry` (singleton class/instance store with register, get_class, get_instance, has, clear, names). `_register_methods()` helper that discovers public methods (excluding private, `run`, `stop`) and registers them as MCP tools via `FastMCP.tool()`. Decorator adds `_mcp` (FastMCP instance), `_tool_names` (list of registered tool names), `run(transport=)`, and `stop()` to the decorated class. Module-level `server_registry` global singleton.
- Files changed: `packages/orbiter-mcp/src/orbiter/mcp/server.py` (new, ~120 lines), `packages/orbiter-mcp/tests/test_mcp_server.py` (new, 32 tests in 10 classes)
- Test coverage: MCPServerRegistryInit (2), MCPServerRegistryRegister (4: register, multiple, overwrite, missing), MCPServerRegistryInstance (4: get, singleton, args, missing), MCPServerRegistryClear (2: clear, has), DecoratorRegistration (3: global registry, default name, custom name), DecoratorInit (3: creates mcp, uses docstring, preserves init), DecoratorMethods (4: registers public, skips private, skips run/stop, tool_names attr), DecoratorRun (4: run added, default transport, not-initialized raises, stop added), RegisterMethods (3: sync, async, empty), GlobalRegistry (3: exists, decorator uses, cleared)
- **Learnings for future iterations:**
  - FastMCP.__init__ uses `instructions=` not `description=` — the old AWorld code used `description=` which was valid in older mcp versions but mcp>=1.26 uses `instructions`
  - pyright may not see overloaded parameters that exist at runtime (e.g., `FastMCP.tool(description=...)`) — use `# pyright: ignore[reportCallIssue]` for known-valid calls that pyright's stubs miss
  - Dynamically added attributes via class decorator (`_mcp`, `run`, `stop`) are invisible to pyright — test files need `# pyright: ignore[reportAttributeAccessIssue]`
  - The `_register_methods` helper uses default argument capture (`_m=method`) to avoid closure variable binding issues in for-loops — same pattern as old AWorld's `create_tool_wrapper(method_to_call)`
  - 1551 tests total across full suite (32 new MCP server tests)
---

## 2026-02-15 - US-047
- What was implemented: MCP execution utilities — retry logic with configurable timeout, env var substitution for mcp.json config files, config file loading, and integration tests. `MCPExecutionError` (subclass of `MCPClientError`). `call_tool_with_retry()` with exponential backoff (`backoff_base * 2^attempt`), per-attempt timeout via `asyncio.wait_for()`, non-retryable `MCPClientError` pass-through, and client vs connection dispatch via duck-typing. `substitute_env_vars()` with `${VAR}` regex pattern, `_substitute_recursive()` for nested dicts/lists. `load_mcp_config()` parses mcp.json format (`mcpServers` key), applies env var substitution to all string values, creates `MCPServerConfig` instances. `load_mcp_client()` convenience wrapper.
- Files changed: `packages/orbiter-mcp/src/orbiter/mcp/execution.py` (new, ~180 lines), `packages/orbiter-mcp/tests/test_mcp_execution.py` (new, 31 tests in 9 classes)
- Test coverage: RetrySuccess (4: first try, arguments, retry, connection dispatch), RetryFailure (3: exhausted, no retries, MCPClientError not retried), RetryTimeout (2: triggers retry, exhausts retries), RetryBackoff (1: custom base), SubstituteEnvVars (6: simple, multiple, unset, no vars, empty, nested braces), SubstituteRecursive (3: dict, list, passthrough), LoadMCPConfig (8: stdio, multiple, nested env, missing file, invalid json, invalid type, empty, defaults), LoadMCPClient (1: creates client), Integration (3: retry E2E, config E2E, error hierarchy)
- **Learnings for future iterations:**
  - `MCPExecutionError` extends `MCPClientError` so callers can catch both specific and general MCP errors with a single except clause
  - `call_tool_with_retry` distinguishes MCPClient (has `server_names`) from MCPServerConnection (no `server_names`) via `hasattr` — MCPClient.call_tool takes `(server_name, tool_name, args)` while connection takes `(tool_name, args)`
  - mcp.json format uses `"mcpServers"` key (matching VS Code / Claude Code convention) — each value is a server config dict with transport/command/args/env/url/headers fields
  - `asyncio.wait_for()` wraps each individual attempt, not the total retry sequence — this prevents a single slow attempt from blocking all retries
  - Within-package imports in orbiter-mcp source also need `# pyright: ignore[reportMissingImports]` (same namespace package issue as orbiter-core/orbiter-models)
  - 1582 tests total across full suite (31 new execution tests)
---

## 2026-02-15 - US-048
- What was implemented: YAML agent & swarm loader with variable substitution and factory dispatch. `LoaderError` exception. `_substitute()` recursive variable replacement for `${ENV_VAR}` (from os.environ) and `${vars.KEY}` (from YAML's `vars` section), with full-match type preservation (int, float, bool). `_build_agent()` factory with builtin Agent construction and custom class dispatch via `register_agent_class()`. `load_yaml()` loads + substitutes. `load_agents()` returns named agent instances. `load_swarm()` creates Swarm with mode (workflow/handoff/team), flow DSL, explicit order, handoff edge wiring.
- Files changed: `packages/orbiter-core/src/orbiter/loader.py` (new, ~150 lines), `packages/orbiter-core/tests/test_loader.py` (37 tests in 9 classes), `packages/orbiter-core/pyproject.toml` (added `pyyaml>=6.0` dependency)
- Key implementation details:
  - Two substitution modes: `fullmatch()` preserves original type (e.g., `${vars.TEMP}` → 0.7 as float), `sub()` for partial matches always returns string
  - `_build_agent()` supports `system_prompt` alias for `instructions` (matching old AWorld convention)
  - `load_swarm()` handles 3 topology patterns: workflow (default, with optional explicit order or flow DSL), handoff (with `edges` for wiring handoff targets), team (lead + workers via order)
  - `register_agent_class(name, cls)` for custom agent types via YAML `type:` field
  - Missing env vars / vars keys return the original `${...}` string unchanged (no error, matches old AWorld behavior)
- **Learnings for future iterations:**
  - `pyyaml>=6.0` needed to be added as a dependency to orbiter-core's pyproject.toml — previously not needed by any orbiter-core module
  - `_VAR_RE.fullmatch()` vs `_VAR_RE.sub()` distinction is critical for type preservation: full match returns the resolved value directly (preserving int/float/bool), partial match always converts to string
  - YAML `vars:` section is popped from data before substitution to avoid leaking into the returned dict — consistent with old AWorld's approach
  - ruff auto-format adjusts whitespace in function signatures — run format before committing
  - sqlite/postgres tests have collection errors due to missing deps (aiosqlite/asyncpg not installed) — this is pre-existing, not caused by this change
  - 1552 tests pass (37 new loader tests), excluding sqlite/postgres collection errors
---
