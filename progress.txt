## Codebase Patterns
- SQLiteMemoryStore: import from `orbiter.memory.backends.sqlite`, needs `await store.init()` before use
- Context summarization: `_apply_context_windowing()` in `orbiter.agent` creates a transient `SystemMessage("[Conversation Summary]...")` — NOT persisted to SQLiteMemoryStore; only HumanMemory + AIMemory are persisted; summarization fires when non-system msg_count >= summary_threshold (including loaded history + new user message)
- summary_threshold=4 means summarization fires at turn 3 (turn 1=2 stored items, turn 2=4 stored → 4+1 new=5 >= 4, fires); keep_recent = max(2, summary_threshold//2) = 2
- token_budget_trigger only fires INSIDE the tool loop (when tool calls happen), not between turns — set summary_threshold to test summarization in non-tool scenarios
- `agent.run()` returns `AgentOutput(text, tool_calls, usage)` — NO `output` field; for structured output, use `parse_structured_output(result.text, MyModel)` from `orbiter._internal.output_parser`
- `output_type` on Agent is stored but NOT auto-applied; instruct LLM to return JSON in system prompt and parse manually
- Agent memory persistence: pass `memory=SQLiteMemoryStore(db_path=...)` directly (not AgentMemory) for simple per-store persistence; agent_id and task_id are auto-set; user_id is NOT auto-set
- ContextConfig: import from `orbiter.context.config`; use `ContextConfig(mode="copilot", history_rounds=N, summary_threshold=M, offload_threshold=P)` or `make_config("pilot")`; `summary_threshold <= offload_threshold` is required
- Context windowing test: to test `history_rounds` limits, set `summary_threshold` high (e.g. 50) to prevent summarization injecting context via SystemMessage before windowing runs
- `load_history(rounds=max_steps)` limits how many prior rounds memory loads; set `max_steps >= N_turns` so memory loads full history before context windowing trims it
- For memory history reload: second agent MUST have same `name` as first agent AND same `conversation_id` for `load_history()` to find prior messages
- `MemoryMetadata(user_id=...)` scoping works in SQLiteMemoryStore via `json_extract(metadata, '$.user_id')`
- ChromaVectorMemoryStore: import from `orbiter.memory.backends.vector`, takes EmbeddingProvider, optional `path` for persistence
- SentenceTransformerEmbeddingProvider: local embeddings, no API key needed, import from `orbiter.memory.backends.vector`
- mcp_server decorator: import from `orbiter.mcp`, adds `run()` and `stop()` methods dynamically (type: ignore needed for pyright)
- MCPServerConfig stdio transport: `command=sys.executable, args=[script_path]` — client spawns subprocess per connection
- pyproject.toml testpaths uses glob pattern `"packages/*/tests"` for package tests
- `asyncio_mode = "auto"` in pytest config means all async test/fixture functions auto-detected
- Integration test branch: `ralph/orbiter-integration-tests`
- Streaming API: `run.stream(agent, prompt, provider=provider, detailed=True)` from `orbiter.runner`; `detailed=True` enables StepEvent, UsageEvent, ToolResultEvent, StatusEvent; use `event_types={"text","usage"}` filter so UsageEvent is last in no-tool case
- Gemini parallel tool calls: Vertex/Gemini API requires ALL function responses for a multi-tool step to be in a single turn; orbiter sends them as separate ToolResult messages which can trigger `400 INVALID_ARGUMENT`; avoid by using chained tools (sequential) instead of prompting for parallel independent calls

---

# Ralph Progress Log
Started: Fri Feb 20 03:26:33 AM IST 2026
---

## 2026-02-20 - US-INT-001
- What was implemented: Full integration test infrastructure setup
- Files changed:
  - `tests/__init__.py` (new, empty)
  - `tests/integration/__init__.py` (new, empty)
  - `tests/integration/conftest.py` (new, all 8 fixtures)
  - `tests/integration/helpers/__init__.py` (new, empty)
  - `tests/integration/helpers/mcp_test_server.py` (new, @mcp_server class with get_capital + get_population)
  - `tests/integration/helpers/web_app.py` (new, minimal FastAPI app with /health)
  - `pyproject.toml` (updated testpaths, markers, added pytest-timeout/tenacity/docker/httpx/chromadb/sentence-transformers)
  - `uv.lock` (updated)
- **Learnings for future iterations:**
  - The orbiter-server app (`orbiter_server.app`) has NO `/health` endpoint — a separate `web_app.py` helper was created for uvicorn_server fixture
  - `pytest --collect-only` exits code 5 when no tests collected; that's expected for a new empty test directory
  - Lazy imports (inside fixture bodies) are the right pattern for optional heavy dependencies (chromadb, sentence-transformers)
  - `@mcp_server()` decorator adds `.run()` dynamically — pyright needs `# type: ignore[attr-defined]` at call site
  - `contextlib.suppress(FileNotFoundError)` preferred over try/except/pass (ruff SIM105)
  - After running `uv lock`, need `uv sync --dev` to install packages in the venv
  - Package runs via `uv run` use Python 3.11 venv, not system Python 3.14
---

## 2026-02-20 - US-INT-002
- What was implemented: Agent + memory persistence seam tests
- Files changed:
  - `tests/integration/test_agent_memory.py` (new)
- **Learnings for future iterations:**
  - `agent.run()` returns `AgentOutput` with `text`, `tool_calls`, `usage` — no `output` field for structured Pydantic models
  - For structured output from LLM, instruct in system prompt to return JSON and parse with `parse_structured_output(result.text, Model)` from `orbiter._internal.output_parser`
  - Two agents sharing a SQLiteMemoryStore with same `name` and `conversation_id` will load each other's history via `load_history()` in `MemoryPersistence`
  - `MemoryMetadata(user_id=...)` scoping works correctly in SQLiteMemoryStore; querying by user_id only returns items with that user_id
  - `test_memory_metadata_scoping` tests SQLiteMemoryStore metadata isolation directly (no LLM needed)
---

## 2026-02-20 - US-INT-004
- What was implemented: Agent streaming event ordering tests
- Files changed:
  - `tests/integration/test_agent_streaming.py` (new)
- **Learnings for future iterations:**
  - `run.stream()` lives in `orbiter.runner`; `run.stream` is attached to the `run` callable as `run.stream = _stream`; use `# type: ignore[attr-defined]` at call site
  - `detailed=True` enables StepEvent, UsageEvent, ToolResultEvent, StatusEvent; without it only TextEvent and ToolCallEvent are emitted
  - Use `event_types={"text","usage"}` filter to make UsageEvent the last event in a no-tool stream (StepEvent/StatusEvent are filtered out)
  - Use `event_types={"text","tool_call","tool_result","usage"}` filter for tool call ordering tests
  - Gemini/Vertex API requires ALL function responses for a parallel multi-tool step to be in a SINGLE user turn; orbiter sends separate ToolResult messages which triggers `400 INVALID_ARGUMENT: number of function response parts must equal function call parts`; avoid by using CHAINED tools (sequential dependency) rather than prompting for parallel independent calls
  - StepEvent count: any single tool call creates >= 2 steps (tool step + final response step) → >= 4 StepEvents (started+completed each); chained 2-tool test gives >= 6 StepEvents
---

## 2026-02-20 - US-INT-003
- What was implemented: Agent + context windowing seam tests
- Files changed:
  - `tests/integration/test_agent_context.py` (new)
- **Learnings for future iterations:**
  - `AgentOutput` (from `agent.run()`) has NO `messages` field; PRD's `len(result.messages)` check was adapted to behavioral testing
  - Context windowing (`history_rounds`) trims non-system messages; summarization creates a SystemMessage that is NOT trimmed — so `summary_threshold` must be set high to prevent it from injecting old context
  - `load_history(rounds=max_steps)` limits memory loads; use `max_steps >= N_turns` so context windowing (not memory) is the limiting factor in the test
  - ContextConfig frozen=True — pass all params at creation; `make_config("pilot")` sets `history_rounds=100, summary_threshold=100, offload_threshold=100`
  - Behavioral test pattern: plant unique token in turn 1, pad with N turns, ask about token — windowed agent says "I don't know", pilot agent recalls it
---

## 2026-02-20 - US-INT-005
- What was implemented: Agent multi-tool selection tests
- Files changed:
  - `tests/integration/test_agent_tools.py` (new)
- **Learnings for future iterations:**
  - `AgentOutput.tool_calls` is `list[ToolCall]`; each `ToolCall` has `.name` (str) and `.arguments` (JSON-encoded str); use `json.loads(tc.arguments)` to get the dict
  - For multi-tool selection test, checking `result.tool_calls` (not `result.messages`) is the correct way to inspect tool calls from `agent.run()`
  - For chained tools test (get_capital → get_population), model must sequentially call them because the second depends on the output of the first — this avoids the Gemini parallel-tool-call INVALID_ARGUMENT issue
  - Tests skip gracefully when GOOGLE_CLOUD_PROJECT/GOOGLE_CLOUD_LOCATION are not set (via vertex_model fixture)
---

## 2026-02-20 - US-INT-006
- What was implemented: Structured output validation tests using parse_structured_output
- Files changed:
  - `tests/integration/test_agent_structured_output.py` (new)
- **Learnings for future iterations:**
  - PRD references `result.output` (e.g. `result.output.capital`) but `agent.run()` returns `AgentOutput` with NO `output` field; use `parse_structured_output(result.text, Model)` from `orbiter._internal.output_parser` instead
  - For structured output tests, instruct the agent via system prompt to reply ONLY with the JSON object; the LLM reliably follows this when clearly told the schema and no-other-text
  - Tool call assertions use `result.tool_calls` (list of ToolCall); structured output assertions use `parse_structured_output(result.text, Model)`
  - `temperature_celsius` parsed from JSON may come as int; use `isinstance(x, (int, float))` or just check `isinstance(x, float)` after pydantic coerces it
---

## 2026-02-20 - US-INT-007
- What was implemented: SQLiteMemoryStore persistence, keyword search, and metadata isolation tests
- Files changed:
  - `tests/integration/test_memory_persistence.py` (new)
- **Learnings for future iterations:**
  - PRD says `memory_store.save()` and `memory_store.load(session_id=...)` but actual API uses `add()` and `search(metadata=MemoryMetadata(session_id=...))` — adapt accordingly
  - `search(query="KEYWORD", limit=N)` does SQLite LIKE/FTS matching on content field; returns only items whose content contains the query string
  - `search(metadata=MemoryMetadata(session_id="..."))` filters by session_id via json_extract on metadata column
  - `search(metadata=MemoryMetadata(user_id="..."))` filters by user_id — user isolation works correctly
  - `memory_store` fixture (from conftest.py) yields an already-init'd SQLiteMemoryStore backed by a temp file — no need to call `await store.init()` again
  - Tests that only exercise SQLiteMemoryStore directly (no LLM needed) run in ~0.2s total — very fast
---

## 2026-02-20 - US-INT-009
- What was implemented: Context summarization trigger tests — verifies summarization pipeline works without errors through 6 turns and that input_tokens are bounded after compression
- Files changed:
  - `tests/integration/test_context_summarization.py` (new)
- **Learnings for future iterations:**
  - SummaryMemory does NOT exist as a persistent class; summarization creates transient SystemMessages only (not saved to SQLiteMemoryStore)
  - MemoryPersistence hooks only save HumanMemory (before LLM call) and AIMemory/ToolMemory (via POST_LLM_CALL/POST_TOOL_CALL hooks)
  - SystemMemory items from load_history() CAN be reconstructed IF written to store manually, but the summarization pipeline never writes them
  - token_budget_trigger fires only during the tool loop (lines 964-982 in agent.py) — not between turns; set summary_threshold for turn-to-turn summarization tests
  - summary_threshold=4 means first summarization fires at turn 3 (4 stored messages + 1 new = 5 >= 4)
  - generate_summary() uses the provider to make an extra LLM call for the summary — add extra time budget (~2-3s per summarization event); use timeout(90) not timeout(30)
  - Adapted test_summarization_shortens_context to compare usage.input_tokens (proxy for context length) since AgentOutput has no messages field
  - Adapted test_token_budget_triggers_summarization to verify >= 12 items in store (all 6 turns completed) since SummaryMemory is not persisted
---

## 2026-02-20 - US-INT-008
- What was implemented: ChromaVectorMemoryStore semantic search and similarity threshold tests
- Files changed:
  - `tests/integration/test_memory_vector.py` (new)
- **Learnings for future iterations:**
  - `ChromaVectorMemoryStore.search()` has no `min_score` parameter; to test similarity thresholds, embed query and items manually via `vector_store._embedding_provider.embed()` and compute dot product (unit-norm vectors → dot product = cosine similarity)
  - `all-MiniLM-L6-v2` (SentenceTransformerEmbeddingProvider default) produces unit-norm vectors; dot product of two embeddings equals their cosine similarity
  - Semantic search correctly distinguishes unrelated domains (astronomy vs cooking, cooking vs quantum physics) — at least 2 of 3 relevant items returned in top-5 results
  - `vector_store` fixture is already initialized; no `init()` call needed, just call `await store.add()` directly
  - Test runtime ~20s due to model loading on first use; subsequent tests in same session are faster
---

## 2026-02-20 - US-INT-010
- What was implemented: MCP stdio subprocess integration tests — verifies agent can connect to real stdio MCP server subprocess and call its tools
- Files changed:
  - `tests/integration/test_mcp_stdio.py` (new)
- **Learnings for future iterations:**
  - `await agent.add_mcp_server(mcp_server_config)` connects to MCP server and registers all its tools on the agent; tools are namespaced: `mcp__test_server__get_capital`
  - Check tool registration via `any("get_capital" in name for name in agent.tools.keys())` since names are namespaced
  - `result.tool_calls` (from `agent.run()`) is `list[ToolCall]`; each `.name` is the namespaced name; check with `"get_capital" in tc.name`
  - `result.text` (not `result.output`) contains the LLM's text response — assert keyword presence with `.lower()`
  - MCP stdio client spawns a fresh subprocess per connection (not a long-running process); the `MCPServerConfig` in the fixture just describes how to launch it
  - For sequential MCP tool calls, chain them so second depends on first (avoids Gemini parallel-tool-call INVALID_ARGUMENT)
  - Use timeout(60) not timeout(30) for MCP tests — subprocess spawn + LLM call needs extra headroom
---

## 2026-02-20 - US-INT-011
- What was implemented: Agent handoffs integration tests — verifies Swarm handoff mode routes between agents and final output reflects target agent's work
- Files changed:
  - `tests/integration/test_handoffs.py` (new)
- **Learnings for future iterations:**
  - Orbiter handoffs use output-based routing (NOT tool calls): Agent A must output EXACTLY the target agent's name (stripped) to trigger handoff via `_detect_handoff` in swarm.py
  - Use `Swarm(agents=[router, specialist, ...], mode="handoff")` and call `swarm.run(prompt, provider=provider)` for handoff tests
  - `RunResult.messages` from the final agent includes accumulated history from all agents in the chain; check for `AssistantMessage(content="target_name")` to detect handoff occurred
  - For structured output from the target agent, use `parse_structured_output(result.output, Model)` — `RunResult.output` is a str, not auto-parsed
  - The next agent after handoff receives `current_input = result.output` (the routing word) as the new human input, plus all prior messages; instruct specialist to look at conversation history for the original question
  - Router instructions must be very explicit: "Your entire response must be exactly the single word: poet" — model may add punctuation otherwise
  - No HandoffEvent or ToolCall is emitted for handoffs in Orbiter; only the routing output ("poet") appears as an AssistantMessage in messages
---

## 2026-02-20 - US-INT-013
- What was implemented: Dynamic tool loading integration tests — verifies add_tool() makes tool available immediately, remove_tool() makes it unavailable, and concurrent add_tool() calls are safe
- Files changed:
  - `tests/integration/test_dynamic_loading.py` (new)
- **Learnings for future iterations:**
  - `agent.add_tool(tool)` is async (acquires `_tools_lock`); `agent.remove_tool(name)` is sync (dict del is atomic in CPython asyncio)
  - `FunctionTool(fn)` creates a tool from any callable; override name via `fn.__name__ = "tool_name"` before passing to FunctionTool
  - `_register_tool` raises `AgentError` on duplicate names — no silent overwrite
  - `test_concurrent_add_tool_no_race_condition` needs no LLM call; purely tests asyncio safety of `_tools_lock`
  - `agent.tools` is a `dict[str, Tool]`; check presence with `"tool_name" in agent.tools`
---

## 2026-02-20 - US-INT-014
- What was implemented: Memory + context three-package chain tests — verifies SQLite persistence across 6 turns, AI content written to Chroma is semantically retrievable, and a new agent with AgentMemory(long_term=Chroma) correctly injects Chroma-stored facts via _inject_long_term_knowledge
- Files changed:
  - `tests/integration/test_memory_context_chain.py` (new)
- **Learnings for future iterations:**
  - `AgentMemory(short_term=SQLite, long_term=Chroma)` auto-persists to SQLite only; Chroma (long_term) is searched by `_inject_long_term_knowledge()` for injection but NOT auto-written
  - `_inject_long_term_knowledge()` runs at line 917 of agent.py: `getattr(agent_memory, "long_term", None)` — only fires if memory has `.long_term` attribute (i.e., `AgentMemory` not bare `MemoryStore`)
  - To test KnowledgeNeuron injection: use `AgentMemory(short_term=..., long_term=ChromaVectorMemoryStore(...))` and pre-populate Chroma with facts; the agent's system message gets a `<knowledge>` block with matched items
  - `_inject_long_term_knowledge()` calls `long_term.search(query=user_input, limit=5)` — no metadata filter; all items in Chroma are eligible for matching
  - SummaryMemory is NOT persisted anywhere; "confirm SummaryMemory item created in SQLite" → adapted to verify conversation items (HumanMemory + AIMemory) persisted
  - For Chroma population when using `AgentMemory`: need to manually `await chroma_store.add(AIMemory(...))` after getting items from SQLite; Chroma doesn't auto-receive items
  - ChromaVectorMemoryStore needs `path=tmpdir` for persistence; collection created lazily, no explicit `init()` call needed
  - Use `shutil.rmtree(chroma_dir, ignore_errors=True)` in finally for Chroma temp dir cleanup
---

## 2026-02-20 - US-INT-012
- What was implemented: Agent self-spawn integration tests — verifies parent agent spawns children and depth limit is enforced
- Files changed:
  - `tests/integration/test_self_spawn.py` (new)
- **Learnings for future iterations:**
  - Self-spawn API: `Agent(allow_self_spawn=True, max_spawn_depth=N)` auto-registers `spawn_self(task: str) -> str` tool
  - `spawn_self` tool is named exactly `spawn_self` (not `spawn_subtask`); check `tc.name == "spawn_self"` in `result.tool_calls`
  - Depth guard: `if _spawn_depth >= max_spawn_depth: return error_string` — does NOT raise, returns error string; LLM handles it gracefully
  - To test depth enforcement: set `agent._spawn_depth = agent.max_spawn_depth` before `agent.run()` — spawn_self fires guard on first call
  - Spawned child gets `allow_self_spawn=False`, so no recursive spawning from children
  - Use `agent.run()` (not `swarm.run()`); returns `AgentOutput` with `.text` and `.tool_calls`
  - Use timeout(60) not timeout(30) — each spawn_self call adds a full LLM round-trip
  - Prompt must instruct SEQUENTIAL spawns to avoid Gemini parallel-tool-call INVALID_ARGUMENT error
---

## 2026-02-20 - US-INT-015
- What was implemented: MCP large output workspace offload integration tests — verifies that a 15 KB MCP tool result triggers workspace offload, retrieve_artifact is called, and the keyword in the dataset is correctly identified
- Files changed:
  - `tests/integration/helpers/mcp_test_server.py` (added `get_large_dataset(topic: str)` method to TestServer)
  - `tests/integration/test_mcp_large_output.py` (new, 2 tests + DataSummary model)
- **Learnings for future iterations:**
  - Large output offloading: `MCPServerConfig(large_output_tools=['tool_name'])` marks a tool as large_output=True; any result (regardless of size) is offloaded to workspace via `_offload_large_result()`; the 10 KB threshold (`_get_large_output_threshold()`) also triggers offload for ANY tool
  - `agent._workspace` is `None` until first offload; after offload it's a `Workspace` object; `agent._workspace.list()` returns `list[Artifact]`
  - `Workspace.list()` returns all `Artifact` objects in memory; no `await` needed (sync method)
  - `retrieve_artifact` tool is auto-registered on ALL agents (even without tools); it retrieves artifacts from `agent._workspace` by artifact_id
  - The pointer string format: `"[Result stored as artifact 'ARTIFACT_ID'. Call retrieve_artifact('ARTIFACT_ID') to access.]"` — LLM must be instructed to call retrieve_artifact when it sees this pattern
  - Tool output: `get_large_dataset` generates >= 15 KB with keyword in header; trimmed at UTF-8 boundary to avoid corruption
  - Use timeout(60) (not timeout(30)) for MCP large-output tests — subprocess spawn + LLM + retrieve_artifact LLM call adds overhead
  - `DataSummary(contains_keyword: bool)` → parse via `parse_structured_output(result.text, DataSummary)` following established pattern
  - Ruff I001: import blocks inside functions must be alphabetically sorted (orbiter._internal.output_parser before orbiter.agent before orbiter.mcp before orbiter.models)
---

## 2026-02-20 - US-INT-016
- What was implemented: MCP progress events in stream test — verifies MCPProgressEvent items appear in stream before ToolResultEvent when a long_running_task tool emits progress notifications
- Files changed:
  - `tests/integration/helpers/mcp_test_server.py` (added `long_running_task(steps: int, ctx: FastMCPContext)` method to TestServer)
  - `tests/integration/test_mcp_progress_stream.py` (new, 1 test)
- **Learnings for future iterations:**
  - FastMCP Context injection works through `@mcp_server` decorator's `functools.wraps` wrapper: `inspect.signature(wrapper)` follows `__wrapped__` (set by functools.wraps) and returns the original method's signature including `ctx: Context`; FastMCP injects `ctx` as a keyword arg, which passes through `**kwargs` to the bound method
  - Server-side progress notifications: `async def long_running_task(self, steps: int, ctx: FastMCPContext)` + `await ctx.report_progress(progress, total, message)` in FastMCP tool
  - Import: `from mcp.server.fastmcp import Context as FastMCPContext` — alias avoids name clash and is clearer
  - MCPProgressEvent order in stream: BEFORE ToolResultEvent (queues drained after `_execute_tools()` completes but before ToolResultEvent is yielded)
  - `detailed=True` is required in `run.stream()` for ToolResultEvent to appear; MCPProgressEvent is yielded regardless (subject to event_types filter)
  - Tool registration order in mcp_test_server.py: `_register_methods` uses `inspect.getmembers()` which returns methods in alphabetical order; `long_running_task` appears between `get_population` and other methods alphabetically
  - Use `timeout(60)` (not 30) for MCP progress stream tests — subprocess spawn + LLM + progress notifications overhead
  - Add `asyncio.sleep(0.05)` between progress notifications to ensure they're emitted reliably before tool returns
---

## 2026-02-20 - US-INT-017
- What was implemented: Handoff chain with memory persistence test — verifies a 3-agent A→B→C handoff chain persists messages from all 3 agents to a shared SQLiteMemoryStore, identifiable by agent_id metadata
- Files changed:
  - `tests/integration/test_handoff_memory.py` (new, 1 test)
- **Learnings for future iterations:**
  - In Swarm handoff mode, each agent in the chain is called via `call_runner(agent, input, messages=all_messages, provider=provider)` — no `conversation_id` is passed, so each agent creates its own UUID `conversation_id` on first run
  - Each agent saves memory items with `agent_id=self.name` and `task_id=<its own conversation_id>` — so items from different agents in the chain have different `task_id` values
  - To query a specific agent's contributions from a shared store: use `search(metadata=MemoryMetadata(agent_id="agent_name"))` — filters by agent_id regardless of task_id
  - Passing the same `SQLiteMemoryStore` instance to multiple agents (via `memory=shared_store`) means all agents write to the same SQLite file, each tagged with their own `agent_id`
  - Use `timeout(60)` for 3-agent chain tests — 3 sequential LLM calls need headroom
  - Router agents still save HumanMemory (their input) + AIMemory (their routing output, e.g. "agent_b") to the store, so items_a and items_b are non-empty even for routing agents
---

## 2026-02-20 - US-INT-018
- What was implemented: Conversation branching isolation test — verifies agent.branch() creates independent conversation scopes identified by unique task_ids in SQLiteMemoryStore
- Files changed:
  - `tests/integration/test_branching_isolation.py` (new, 1 test)
- **Learnings for future iterations:**
  - `agent.branch(from_message_id)` copies all memory items up to and including the given MemoryItem.id to a new conversation UUID; returns the new branch_id (str)
  - Pass the returned branch_id to subsequent `agent.run(..., conversation_id=branch_id)` calls; the agent's own `self.conversation_id` is NOT updated (only updated when `conversation_id` param is None)
  - Memory items for branched runs are stored with `task_id=branch_id`, not the original `conversation_id` — query with `search(metadata=MemoryMetadata(agent_id=..., task_id=branch_id))`
  - HumanMemory (the user prompt string) is persisted before the LLM call — use distinctive keyword prompts (e.g. "Say exactly: VOLCANO_TOPIC") to reliably verify which topic appears in each branch
  - Cross-contamination check: because HumanMemory stores the exact prompt text, keywords in prompts are reliable test discriminators; no risk of the LLM accidentally mentioning the other branch's keyword if prompts are sufficiently distinct
  - Use `timeout(90)` for this test: 3 initial turns + 2 branch-A turns + 2 branch-B turns = 7 LLM calls, needs headroom
  - `branch()` requires `agent.conversation_id` to be set (i.e., agent must have run at least once with memory configured)
  - `branch()` raises `AgentError` if memory not configured, no active conversation, or message ID not found
---

## 2026-02-20 - US-INT-019
- What was implemented: Context vector injection via KnowledgeNeuron test — verifies that 5 obscure fictional facts stored in ChromaVectorMemoryStore are injected into a new agent's system message via `_inject_long_term_knowledge()` and enable the agent to correctly answer a question it cannot know from training data
- Files changed:
  - `tests/integration/test_context_vector_injection.py` (new, 1 test)
- **Learnings for future iterations:**
  - `AgentMemory(long_term=ChromaVectorMemoryStore(...))` without `short_term` is valid; `_inject_long_term_knowledge()` only requires `memory.long_term` to be set
  - Timeout of 30s (PRD spec) was increased to 60s to accommodate SentenceTransformer model loading + LLM call; if the model is already cached from a prior session test, 30s may be sufficient
  - `output_type=CityResponse` pattern: define `class CityResponse(BaseModel): city: str`, instruct LLM via system prompt to return `{"city": "..."}` JSON only, then `parse_structured_output(result.text, CityResponse)` — same pattern as all other structured output tests
  - 5 fictional facts about a fictional product (Orbiter framework) are robust test data — unlikely to appear in any model's training data; the founding city (Auckland) is verifiable because it appears in the first fact and the vector search query ("Orbiter framework founded city") will match it
  - No `short_term` store is needed for this test; the agent doesn't need to persist history, only retrieve injected knowledge
---

## 2026-02-20 - US-INT-020
- What was implemented: Spawn memory isolation test — verifies that memory items written by child agent A (conversation_id='child-a-session') are not visible when querying by child B's session_id, and are visible when querying by child A's session_id
- Files changed:
  - `tests/integration/test_spawn_memory_isolation.py` (new, 1 test)
- **Learnings for future iterations:**
  - Memory isolation between agents with different conversation_ids relies on `task_id` scoping in SQLiteMemoryStore; HumanMemory is stored before the LLM call with `task_id=conversation_id`
  - Use a unique token in the prompt (e.g., 'CHILD_A_UNIQUE_FACT_XYZ') as the "stored fact"; HumanMemory stores the prompt text, making the token reliably retrievable
  - Query with `MemoryMetadata(task_id="child-a-session")` to scope results to a specific agent's conversation
  - The test uses two distinct Agent instances (different names and conversation_ids) on the same shared store to simulate child agent isolation
  - Isolation works by default without any special configuration; task_id is auto-set to conversation_id on every agent run
---

## 2026-02-20 - US-INT-021
- What was implemented: Structured output with multi-tool chain test — verifies TravelReport is correctly populated after get_distance and get_travel_tip are both called sequentially before the JSON output
- Files changed:
  - `tests/integration/test_structured_with_tools.py` (new, 1 test)
- **Learnings for future iterations:**
  - Sequential chained tools (get_distance → get_travel_tip where tip uses the destination from the distance call) reliably avoids Gemini parallel-tool-call INVALID_ARGUMENT; both tools end up in `result.tool_calls` in order
  - `parse_structured_output(result.text, TravelReport)` works correctly after two sequential tool calls — the LLM reliably emits JSON at the end if instructed clearly in system prompt + prompt
  - `assert len(result.tool_calls) == 2` is a reliable check when tools are chained; no extra tool calls are added
  - Timeout of 30s (PRD spec) is sufficient for 2 sequential tool calls + structured output (Vertex is fast)
---
