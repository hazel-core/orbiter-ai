{
  "project": "Orbiter",
  "branchName": "ralph/orbiter-framework",
  "description": "Orbiter Framework — Complete remaining implementation (Phases 5-22) of the modern multi-agent framework rewrite from AWorld",
  "userStories": [
    {
      "id": "US-001",
      "title": "Implement message builder",
      "description": "As a framework developer, I want a message builder that constructs correctly ordered LLM message lists from system instructions, conversation history, and tool results, so that agents can communicate with LLM providers without message ordering bugs.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/__init__.py (empty)",
        "Create packages/orbiter-core/src/orbiter/_internal/message_builder.py",
        "Implement build_messages(instructions, history, tool_results) function",
        "Handle message ordering: system -> user -> assistant -> tool result cycles",
        "Handle edge cases: empty history, no tool results, multiple tool results",
        "~120 lines of source code",
        "Create/update packages/orbiter-core/tests/test_message_builder.py",
        "Tests cover: empty history, single turn, multi-turn, tool result insertion, message ordering validation",
        "uv run ruff check packages/ -- zero lint errors",
        "uv run ruff format --check packages/ -- all formatted",
        "uv run pyright packages/orbiter-core/ -- zero type errors",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Implement output parser",
      "description": "As a framework developer, I want an output parser that extracts text, tool calls, and structured output from LLM responses, so that the agent can decide its next action.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/output_parser.py",
        "Implement parse_response(response: ModelResponse) -> ParsedOutput",
        "ParsedOutput contains: text, tool_calls, structured_output, finish_reason",
        "Handle text-only responses, tool-call-only responses, and mixed responses",
        "Handle structured output via Pydantic model validation when output_type is provided",
        "~100 lines of source code",
        "Create/update packages/orbiter-core/tests/test_output_parser.py",
        "Tests cover: text-only, tool-call-only, mixed, structured output validation, malformed responses",
        "uv run ruff check packages/ -- zero lint errors",
        "uv run pyright packages/orbiter-core/ -- zero type errors",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Already implemented and committed in US-001 batch. 19 tests passing."
    },
    {
      "id": "US-003",
      "title": "Implement Agent class — init and configuration",
      "description": "As a framework user, I want to create Agent instances with a clean constructor that accepts name, model, instructions, tools, hooks, memory, handoffs, and output_type, so that I can define agents declaratively.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/agent.py",
        "Agent.__init__ accepts: name (required), model, instructions, tools, handoffs, hooks, output_type, max_steps, temperature, max_tokens, memory, context",
        "All parameters are keyword-only except name",
        "Sensible defaults: model='openai:gpt-4o', max_steps=10, temperature=1.0",
        "Tool registration validates tool types",
        "describe() method returns agent metadata (name, model, tools, capabilities)",
        "~100 lines of source code",
        "Create packages/orbiter-core/tests/test_agent.py",
        "Tests cover: minimal agent, full-featured agent, config validation, describe() output",
        "uv run ruff check packages/ -- zero lint errors",
        "uv run pyright packages/orbiter-core/ -- zero type errors",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Agent class was mostly pre-existing from Phase 5 sessions. Added missing memory and context parameters."
    },
    {
      "id": "US-004",
      "title": "Implement Agent run method with retry logic",
      "description": "As a framework developer, I want the Agent to have a run() method that builds messages, calls the LLM, parses the response, and returns the result with retry logic for transient errors, so that single-turn agent execution is reliable.",
      "acceptanceCriteria": [
        "Add async run(input, messages, provider) method to Agent",
        "Wires message_builder -> LLM call -> output_parser -> return",
        "Single-turn execution only (no tool loop yet)",
        "Hooks: PRE_LLM_CALL, POST_LLM_CALL fired at correct points",
        "Retry logic: configurable max_retries (default 3) with exponential backoff",
        "Context-length errors (ModelError with specific code) fail immediately, no retry",
        "Transient errors (rate limit, timeout, server error) retry with backoff",
        "~130 lines of source code",
        "Tests cover: successful run, retry on transient error, immediate fail on context-length error, hook invocation order",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Agent.run() method with retry logic, 15 new tests (364 total). ModelError gained code parameter for error classification."
    },
    {
      "id": "US-005",
      "title": "Implement Agent tool execution loop",
      "description": "As a framework developer, I want the Agent to execute tool calls returned by the LLM, feed results back, and re-call the LLM in a loop until a text response is produced or max_steps is reached, so that agents can use tools to accomplish tasks.",
      "acceptanceCriteria": [
        "Extend Agent.run(): if LLM returns tool calls -> execute tools -> feed results back -> re-call LLM",
        "max_steps guard prevents infinite loops",
        "PRE_TOOL_CALL and POST_TOOL_CALL hooks fired for each tool execution",
        "Parallel tool execution via asyncio.TaskGroup when multiple tool calls are returned",
        "Tool errors caught and returned as ToolResult(error=...), not propagated",
        "~120 lines of source code",
        "Tests cover: single tool call, multi-tool parallel call, tool error handling, max_steps enforcement",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Tool loop with parallel execution via asyncio.TaskGroup, PRE/POST_TOOL_CALL hooks, error handling. 8 new tests (372 total)."
    },
    {
      "id": "US-006",
      "title": "Agent edge case tests",
      "description": "As a framework developer, I want comprehensive tests for Agent edge cases so that the implementation is robust.",
      "acceptanceCriteria": [
        "Update packages/orbiter-core/tests/test_agent.py",
        "Tests cover: multi-tool calls in sequence, parallel tool calls, tool returning error, max_steps reached, retry behavior across tool loop, agent with no tools, agent with handoffs declared",
        "~120 lines of test code",
        "Total agent tests: ~15-20",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "8 new edge case tests in TestAgentEdgeCases class (380 total tests). Covers retry during tool loop, handoff agents, sequential message accumulation, max_steps=1, empty input, usage tracking, no-arg tools."
    },
    {
      "id": "US-007",
      "title": "Implement Human-in-the-loop tool",
      "description": "As a framework user, I want a HITL tool that pauses agent execution to request user confirmation or input, so that I can build agents with human oversight.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/human.py",
        "HumanInputTool -- async tool that blocks for user input",
        "HumanInputHandler protocol with async get_input(prompt: str) -> str",
        "ConsoleHandler -- default interactive console handler (reads from stdin)",
        "Tool schema generated correctly (prompt parameter, optional choices)",
        "~120 lines of source code",
        "Create packages/orbiter-core/tests/test_human.py",
        "Tests cover: tool schema, handler invocation with mocked input, timeout behavior",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "HumanInputHandler ABC, ConsoleHandler, HumanInputTool with timeout support. 21 tests (401 total)."
    },
    {
      "id": "US-008",
      "title": "Implement run state tracking",
      "description": "As a framework developer, I want internal state tracking for run execution (messages, iterations, status) so that the runner can manage agent lifecycle.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/state.py",
        "RunState -- tracks messages, tool calls, iterations, current agent, status",
        "RunNodeStatus enum: INIT, RUNNING, SUCCESS, FAILED, TIMEOUT",
        "RunNode -- per-step state with agent_id, group tracking, timestamps",
        "~120 lines of source code",
        "Create packages/orbiter-core/tests/test_state.py",
        "Tests cover: state transitions, node lifecycle, message accumulation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "RunNodeStatus (StrEnum), RunNode (Pydantic BaseModel with lifecycle transitions), RunState (mutable execution tracker with message/node/usage accumulation). 29 tests (430 total)."
    },
    {
      "id": "US-009",
      "title": "Implement call runner core loop",
      "description": "As a framework developer, I want the internal call runner that orchestrates the LLM->tool->LLM loop with state tracking and loop detection, so that the public run() function has a reliable execution engine.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/call_runner.py",
        "async call_runner(agent, input, state) -- the core execution loop",
        "Integrates with Agent.run(), RunState tracking",
        "Endless loop detection with configurable threshold",
        "~120 lines of source code",
        "Create packages/orbiter-core/tests/test_call_runner.py",
        "Tests cover: single-turn, multi-turn, loop detection, state updates",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "CallRunnerError, call_runner() with RunState tracking, loop detection via tool-call signatures, node lifecycle management. 15 tests (445 total)."
    },
    {
      "id": "US-010",
      "title": "Implement public run() entry point",
      "description": "As a framework user, I want run(agent, input) and run.sync(agent, input) as the primary API for executing agents, so that running agents is simple and intuitive.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/runner.py",
        "async run(agent_or_swarm, input, messages=None) -> RunResult",
        "run.sync(agent_or_swarm, input) -- sync wrapper using asyncio.run()",
        "Wraps bare Agent in single-agent Swarm internally if needed",
        "Returns RunResult with output, messages, usage, steps",
        "~100 lines of source code",
        "Create packages/orbiter-core/tests/test_runner.py",
        "Tests cover: run() async, run.sync(), multi-turn via messages param, error propagation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Public run() async + run.sync() wrapper, auto-provider resolution, 13 new tests (458 total)."
    },
    {
      "id": "US-011",
      "title": "Implement streaming run",
      "description": "As a framework user, I want run.stream(agent, input) that yields StreamEvent objects in real-time, so that I can build streaming UIs and show progressive agent output.",
      "acceptanceCriteria": [
        "Add run.stream() as an async generator yielding StreamEvent",
        "Yields TextEvent for text chunks, ToolCallEvent for tool invocations",
        "Integrates with provider streaming (provider.stream())",
        "~120 lines of source code",
        "Tests cover: streaming text events in order, tool call events, stream completion",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": "run.stream() async generator yielding TextEvent and ToolCallEvent. Integrates with provider.stream(), tool execution loop, max_steps. 10 new tests (468 total)."
    },
    {
      "id": "US-012",
      "title": "Implement handler system — base + agent handler",
      "description": "As a framework developer, I want a handler abstraction and an AgentHandler that routes between agents in a swarm, so that multi-agent execution is composable.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/handlers.py",
        "Handler[IN, OUT] ABC with async handle() -> AsyncGenerator",
        "AgentHandler -- routes between agents, handles handoff dispatch",
        "Swarm topology-aware stop checks (workflow/handoff/team modes)",
        "~130 lines of source code",
        "Tests cover: handler dispatch, agent routing, handoff detection",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Handler[IN, OUT] ABC, AgentHandler with workflow/handoff/team modes, SwarmMode enum, handoff detection, stop checks. 28 tests (496 total)."
    },
    {
      "id": "US-013",
      "title": "Implement tool + group handlers",
      "description": "As a framework developer, I want ToolHandler and GroupHandler for dynamic tool loading/execution and parallel agent group execution, so that the runner can handle complex execution patterns.",
      "acceptanceCriteria": [
        "Add ToolHandler to handlers.py -- dynamic tool loading, execution, result aggregation",
        "Add GroupHandler -- parallel agent/tool group execution with dependency resolution",
        "~130 lines of source code",
        "Tests cover: parallel tool execution, group execution ordering, dependency resolution",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": "ToolHandler (dynamic tool loading, parallel execution, result aggregation) and GroupHandler (parallel/serial agent groups with dependency resolution via topological sort). 23 new tests (519 total)."
    },
    {
      "id": "US-014",
      "title": "Implement background task handler",
      "description": "As a framework developer, I want a background task handler that supports hot-merge and wake-up-merge patterns, so that long-running tasks can integrate background results.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/background.py",
        "BackgroundTaskHandler with hot-merge and wake-up-merge patterns",
        "Pending message queue for background results",
        "Integration point with checkpoint system",
        "~120 lines of source code",
        "Tests cover: hot-merge, wake-up-merge, pending message handling",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": "BackgroundTaskHandler with hot-merge and wake-up-merge patterns, PendingQueue, BackgroundTask lifecycle, merge callbacks. 28 new tests (547 total)."
    },
    {
      "id": "US-015",
      "title": "Runner integration tests",
      "description": "As a framework developer, I want end-to-end integration tests for the runner system so that Agent + Tool + run() works correctly together.",
      "acceptanceCriteria": [
        "End-to-end tests: Agent + @tool + run() with mocked LLM provider",
        "Handler pipeline tests",
        "Background task scenario tests",
        "~100 lines of test code",
        "Update packages/orbiter-core/src/orbiter/__init__.py with exports: Agent, run, tool, Tool",
        "uv run pytest -- all tests pass for orbiter-core",
        "Typecheck passes"
      ],
      "priority": 15,
      "passes": true,
      "notes": "22 integration tests: 6 end-to-end (Agent+@tool+run), 4 public API imports, 3 handler pipeline, 2 ToolHandler, 2 GroupHandler, 5 background task scenarios. Updated __init__.py with Agent, run, tool, Tool, FunctionTool exports. 569 total tests."
    },
    {
      "id": "US-016",
      "title": "Implement graph utilities",
      "description": "As a framework developer, I want graph utilities (topological sort, cycle detection, flow DSL parsing) so that swarm orchestration can define agent execution order.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/graph.py",
        "Simple adjacency list graph implementation",
        "topological_sort() using Kahn's algorithm",
        "Cycle detection (raises on cyclic graphs)",
        "parse_flow_dsl('a >> b >> c') -> list of edges",
        "~100 lines of source code",
        "Create packages/orbiter-core/tests/test_graph.py",
        "Tests cover: topo sort, cycle detection, DSL parsing, complex DAGs",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 16,
      "passes": true,
      "notes": "Graph dataclass with adjacency lists, topological_sort() via Kahn's algorithm, cycle detection, parse_flow_dsl() with parallel group support. 31 new tests (600 total)."
    },
    {
      "id": "US-017",
      "title": "Implement Swarm — workflow mode",
      "description": "As a framework user, I want Swarm(agents=[...], flow='a >> b >> c') that runs agents sequentially, passing output as input, so that I can build agent pipelines.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/swarm.py",
        "Swarm.__init__ accepts: agents, flow (DSL string), mode (default 'workflow')",
        "mode='workflow': execute agents in topological order from flow DSL",
        "Output of each agent becomes input for the next",
        "~120 lines of source code",
        "Create packages/orbiter-core/tests/test_swarm.py",
        "Tests cover: sequential execution, output->input chaining, 3-agent pipeline",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 17,
      "passes": true,
      "notes": "Swarm class with workflow mode, flow DSL parsing via parse_flow_dsl/topological_sort, output→input chaining, run() integration. 20 new tests (620 total)."
    },
    {
      "id": "US-018",
      "title": "Implement Swarm — handoff mode",
      "description": "As a framework user, I want agents to be able to hand off to other agents dynamically (like triage -> billing), so that I can build agent-driven delegation patterns.",
      "acceptanceCriteria": [
        "Add mode='handoff' support to Swarm",
        "Agents can return a handoff action targeting another agent in the swarm",
        "Handoff transfers control and conversation history",
        "Endless loop detection with configurable threshold",
        "~120 lines of source code",
        "Tests cover: agent A hands off to agent B, loop detection triggers, handoff with conversation history",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 18,
      "passes": true,
      "notes": "Handoff mode with _run_handoff(), _detect_handoff(), max_handoffs config, loop detection. 15 new tests (635 total)."
    },
    {
      "id": "US-019",
      "title": "Implement Swarm — team mode",
      "description": "As a framework user, I want a team mode where a lead agent coordinates worker agents, so that I can build leader-delegation patterns.",
      "acceptanceCriteria": [
        "Add mode='team' support to Swarm",
        "First agent in list is the lead; others are workers",
        "Lead can delegate to workers and synthesize their results",
        "~120 lines of source code",
        "Tests cover: lead delegates to worker, worker responds, lead synthesizes final output",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 19,
      "passes": true,
      "notes": "Team mode with _DelegateTool (auto-generated worker delegation tools), lead agent runs with workers as callable tools, tools restored after run. 13 new tests (648 total)."
    },
    {
      "id": "US-020",
      "title": "Implement parallel + serial agent groups",
      "description": "As a framework developer, I want parallel and serial agent group primitives so that swarm flow DSL can express '(a | b) >> c' for concurrent-then-sequential execution.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/agent_group.py",
        "ParallelGroup -- concurrent agent execution via asyncio.TaskGroup, custom result aggregation",
        "SerialGroup -- dependency-based sequential execution with output->input chaining",
        "Both integrate as nodes in Swarm flow DSL",
        "~120 lines of source code",
        "Tests cover: parallel execution, serial chaining, mixed topologies",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 20,
      "passes": true,
      "notes": "ParallelGroup (concurrent via asyncio.TaskGroup, custom aggregation) and SerialGroup (output→input chaining) in agent_group.py. Both integrate as Swarm workflow nodes via is_group marker. 27 new tests (675 total)."
    },
    {
      "id": "US-021",
      "title": "Implement nested swarms",
      "description": "As a framework user, I want to use a Swarm as an agent within another Swarm, enabling hierarchical multi-agent composition.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/_internal/nested.py",
        "Swarm can be used as a node in another Swarm's agent list",
        "Recursive execution with context isolation",
        "~100 lines of source code",
        "Tests cover: 2-level nested swarm, context isolation between levels",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 21,
      "passes": true,
      "notes": "SwarmNode wrapper with is_swarm duck-typing marker, context isolation (inner swarm gets clean message history). Swarm._run_workflow updated to detect is_swarm. 14 new tests (689 total)."
    },
    {
      "id": "US-022",
      "title": "Swarm integration + public API wiring",
      "description": "As a framework user, I want run(swarm, input) to work seamlessly and Swarm exported from orbiter, so that multi-agent orchestration is accessible from the top-level API.",
      "acceptanceCriteria": [
        "Wire Swarm into run() -- detect Agent vs Swarm input",
        "Update packages/orbiter-core/src/orbiter/__init__.py -- export Swarm",
        "Integration tests for all swarm modes: workflow, handoff, team",
        "Integration tests for parallel/serial groups and nesting",
        "~100 lines of source code + tests",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 22,
      "passes": true,
      "notes": "Swarm, SwarmNode, ParallelGroup, SerialGroup exported from orbiter __init__.py. 21 integration tests (workflow, handoff, team, parallel/serial groups, nested swarms, public API exports, tools in workflows). 710 total tests."
    },
    {
      "id": "US-023",
      "title": "Implement ContextConfig + ContextState",
      "description": "As a framework developer, I want ContextConfig (automation settings) and ContextState (hierarchical key-value state with parent inheritance) as the foundation of the context engine.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/config.py",
        "ContextConfig -- Pydantic v2 frozen model: mode (pilot|copilot|navigator), history_rounds, summary_threshold, offload_threshold, enable_retrieval, neuron_names",
        "Create packages/orbiter-context/src/orbiter/context/state.py",
        "ContextState -- get(key, default) searches local then parent chain, set(key, value) writes local only, local_dict(), to_dict()",
        "~120 lines of source code total",
        "Create packages/orbiter-context/tests/test_context_config.py and test_context_state.py",
        "Tests cover: config creation/validation/defaults, state inheritance, local vs parent, merge",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 23,
      "passes": true,
      "notes": "Created orbiter-context package with workspace registration. ContextConfig (Pydantic v2 frozen, AutomationMode enum, make_config factory) and ContextState (hierarchical key-value with parent chain). 57 new tests (767 total)."
    },
    {
      "id": "US-024",
      "title": "Implement Context class — core lifecycle",
      "description": "As a framework developer, I want the Context class with fork/merge for hierarchical task decomposition, so that multi-agent contexts are isolated but composable.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/context.py",
        "Context.__init__: task_id (required), config, parent",
        "fork(task_id) -- create child context with state inheritance",
        "merge(child) -- consolidate child state back into parent with net token calculation",
        "~130 lines of source code",
        "Create packages/orbiter-context/tests/test_context.py",
        "Tests cover: context creation, fork, merge, hierarchical state isolation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 24,
      "passes": true,
      "notes": "Context class with fork/merge, ContextError, token snapshot for net-delta merge. 35 new tests (802 total)."
    },
    {
      "id": "US-025",
      "title": "Implement TokenTracker",
      "description": "As a framework developer, I want per-agent, per-step token tracking so that cost analysis and budget enforcement are possible.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/token_tracker.py",
        "TokenTracker with add_step(agent_id, prompt_tokens, output_tokens)",
        "TokenStep dataclass: prompt_tokens, output_tokens, step index",
        "get_trajectory(agent_id) -> list of steps",
        "total_usage() -> aggregated prompt/output/total tokens",
        "~100 lines of source code",
        "Create packages/orbiter-context/tests/test_token_tracker.py",
        "Tests cover: step tracking, trajectory retrieval, multi-agent aggregation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 25,
      "passes": true,
      "notes": "TokenStep (frozen dataclass), TokenUsageSummary, TokenTracker with add_step/get_trajectory/total_usage/agent_usage. 24 new tests (826 total)."
    },
    {
      "id": "US-026",
      "title": "Implement Neuron base + core built-in neurons",
      "description": "As a framework developer, I want a Neuron ABC and core built-in neurons (system, task, history) so that prompts can be composed from modular components.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/neuron.py",
        "Neuron ABC with async format(ctx) -> str and priority: int",
        "neuron_registry -- Registry[Neuron] for neuron discovery",
        "SystemNeuron (priority 100) -- date/time/platform dynamic variables",
        "TaskNeuron (priority 1) -- task ID, input, output, subtask plan",
        "HistoryNeuron (priority 10) -- conversation history with windowing",
        "~130 lines of source code",
        "Create packages/orbiter-context/tests/test_neuron.py",
        "Tests cover: neuron formatting, priority ordering, registry, each built-in neuron output",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 26,
      "passes": true,
      "notes": "Neuron ABC with __slots__, neuron_registry (Registry[Neuron]), SystemNeuron (date/time/platform), TaskNeuron (task_id/input/output/subtasks), HistoryNeuron (windowed via config.history_rounds). 32 new tests (858 total)."
    },
    {
      "id": "US-027",
      "title": "Implement extended neurons",
      "description": "As a framework developer, I want additional neurons (skills, workspace, todo, facts, entities, knowledge) and a dynamic variable system, so that the prompt builder has rich context sources.",
      "acceptanceCriteria": [
        "Add to neuron.py or create packages/orbiter-context/src/orbiter/context/_internal/neurons.py",
        "SkillNeuron (priority 40), WorkspaceNeuron (priority 30), TodoNeuron (priority 2), FactNeuron (priority 50), EntityNeuron (priority 60), KnowledgeNeuron (priority 20)",
        "Dynamic variable system: DynamicVariableRegistry with nested path resolution",
        "~130 lines of source code",
        "Tests cover: each neuron type produces correct prompt fragment",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 27,
      "passes": true,
      "notes": "Six extended neurons (TodoNeuron p2, KnowledgeNeuron p20, WorkspaceNeuron p30, SkillNeuron p40, FactNeuron p50, EntityNeuron p60) added to neuron.py. DynamicVariableRegistry with nested path resolution, resolver registration, and template substitution in variables.py. 46 new tests (904 total)."
    },
    {
      "id": "US-028",
      "title": "Implement PromptBuilder",
      "description": "As a framework user, I want a PromptBuilder that composes neurons in priority order to build rich system prompts, so that agent prompts are constructed from modular, reusable components.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/prompt_builder.py",
        "PromptBuilder.__init__(ctx: Context)",
        "add(neuron_name, **kwargs) -- register a neuron for building",
        "async build() -> str -- resolve all neurons in priority order, compose final prompt",
        "Template variable resolution with hierarchical context traversal",
        "~120 lines of source code",
        "Create packages/orbiter-context/tests/test_prompt_builder.py",
        "Tests cover: add neurons, build prompt, priority ordering, variable resolution",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 28,
      "passes": true,
      "notes": "PromptBuilder with priority-ordered neuron composition, variable resolution via DynamicVariableRegistry, add/add_neuron/build/clear/chaining API. 30 new tests (934 total)."
    },
    {
      "id": "US-029",
      "title": "Implement ContextProcessor pipeline",
      "description": "As a framework developer, I want event-driven context processors that intervene at specific points in the LLM execution cycle, so that context can be dynamically transformed.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/processor.py",
        "ContextProcessor ABC with event: str and async process(ctx, payload) -> None",
        "ProcessorPipeline -- registers processors, fires them by event type",
        "Built-in: SummarizeProcessor (pre_llm_call), ToolResultOffloader (post_tool_call)",
        "~130 lines of source code",
        "Create packages/orbiter-context/tests/test_context_processor.py",
        "Tests cover: processor registration, event filtering, pipeline execution order, built-in processors",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 29,
      "passes": true,
      "notes": "ContextProcessor ABC with event-based dispatch, ProcessorPipeline (register/unregister/fire/chaining), SummarizeProcessor (pre_llm_call), ToolResultOffloader (post_tool_call). 39 new tests (973 total)."
    },
    {
      "id": "US-030",
      "title": "Implement Workspace + artifact system",
      "description": "As a framework user, I want a Workspace for persistent artifact storage during execution, with versioning and observer notifications, so that agents can read/write files and large tool results can be offloaded.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/workspace.py",
        "Workspace -- write(name, content), read(name), list(), delete(name)",
        "ArtifactType enum: TEXT, CODE, MARKDOWN, JSON, CSV, IMAGE",
        "Artifact versioning: version_history(name), revert_to_version(name, version)",
        "Observer pattern: on_create, on_update, on_delete callbacks",
        "Local filesystem backend",
        "~130 lines of source code",
        "Create packages/orbiter-context/tests/test_workspace.py",
        "Tests cover: write/read/list/delete, versioning, observer notifications",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 30,
      "passes": true,
      "notes": "Workspace with ArtifactType enum, Artifact versioning, observer pattern (on_create/on_update/on_delete), local filesystem backend. 40 new tests (1013 total)."
    },
    {
      "id": "US-031",
      "title": "Implement workspace-retriever integration",
      "description": "As a framework developer, I want artifacts added to workspace to be auto-indexed in the knowledge store for RAG retrieval, so that agents can search their own artifacts semantically.",
      "acceptanceCriteria": [
        "Wire workspace into RAG pipeline: workspace writes auto-index in KnowledgeStore",
        "Chunk range queries for large artifacts",
        "~100 lines of source code",
        "Tests cover: artifact -> chunk -> search round-trip",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 31,
      "passes": true,
      "notes": "KnowledgeStore with chunking + keyword search in _internal/knowledge.py. Workspace auto-indexes on write/update and de-indexes on delete. 41 new tests (1054 total)."
    },
    {
      "id": "US-032",
      "title": "Implement Checkpoint",
      "description": "As a framework user, I want to save and restore complete execution state for long-running tasks, so that work can be resumed after interruption.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/checkpoint.py",
        "Checkpoint -- serialized context snapshot (values, metadata, version)",
        "Context.snapshot() -> Checkpoint",
        "Context.restore(checkpoint) -> Context",
        "Checkpoints are versioned per session",
        "~100 lines of source code",
        "Create packages/orbiter-context/tests/test_checkpoint.py",
        "Tests cover: snapshot, restore, version incrementing, state preservation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 32,
      "passes": true,
      "notes": "Checkpoint (frozen dataclass with to_dict/from_dict serialization), CheckpointStore (versioned per-session store), Context.snapshot()/Context.restore() classmethod. 35 new tests (1089 total)."
    },
    {
      "id": "US-033",
      "title": "Implement Knowledge store + RAG basics",
      "description": "As a framework developer, I want a KnowledgeStore with artifact indexing and semantic search, so that the context engine can retrieve relevant knowledge.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/_internal/__init__.py",
        "Create packages/orbiter-context/src/orbiter/context/_internal/knowledge.py",
        "KnowledgeStore -- add(name, content), search(query, top_k), get(name), get_range(name, start, end)",
        "Basic chunking + in-memory vector store for testing",
        "~130 lines of source code",
        "Tests cover: add/search/get artifacts, range queries, chunking",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 33,
      "passes": true,
      "notes": "Already fully implemented in US-031 (KnowledgeStore with chunking + keyword search in _internal/knowledge.py). 41 tests already covering add/search/get/get_range/chunking. All acceptance criteria verified."
    },
    {
      "id": "US-034",
      "title": "Implement context tools",
      "description": "As a framework user, I want context tools (planning, knowledge, file) that let agents manipulate their own context during execution, so that agents can plan, search knowledge, and manage files.",
      "acceptanceCriteria": [
        "Create packages/orbiter-context/src/orbiter/context/tools.py",
        "planning_tool -- add_todo(item), get_todo() for task planning checklist",
        "knowledge_tool -- get_knowledge(name), grep_knowledge(name, pattern) for artifact search",
        "file_tool -- read_file(path) from working directory",
        "All implemented as @tool-decorated functions",
        "~120 lines of source code",
        "Create packages/orbiter-context/tests/test_context_tools.py",
        "Tests cover: each tool's execution, context state mutation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 34,
      "passes": true,
      "notes": "_ContextTool subclass with bind(ctx) pattern, planning tools (add_todo, complete_todo, get_todo), knowledge tools (get_knowledge, grep_knowledge, search_knowledge), file tool (read_file with path traversal protection). 38 new tests (1127 total)."
    },
    {
      "id": "US-035",
      "title": "Context package __init__ + integration tests",
      "description": "As a framework user, I want the context package to have a clean public API and pass integration tests end-to-end.",
      "acceptanceCriteria": [
        "Update packages/orbiter-context/src/orbiter/context/__init__.py -- exports: Context, ContextConfig, ContextState, PromptBuilder, ContextProcessor, Neuron, Workspace",
        "Integration tests: Context + PromptBuilder + Processor + Workspace end-to-end",
        "Wire context into Agent (agent.py context parameter)",
        "~100 lines of source code + tests",
        "uv run pytest -- all tests pass for orbiter-context",
        "Typecheck passes"
      ],
      "priority": 35,
      "passes": true,
      "notes": "Updated __init__.py with 23 public exports (Context, ContextConfig, ContextState, PromptBuilder, ContextProcessor, Neuron, Workspace, etc.). 26 integration tests covering: public API imports, Context+PromptBuilder E2E, Context+Processor E2E, Context+Workspace E2E, full lifecycle (create→populate→process→build→checkpoint), fork/merge with workspace, context tools with workspace, custom processors, Agent context wiring. 1153 total tests."
    },
    {
      "id": "US-036",
      "title": "Implement memory interface + types",
      "description": "As a framework developer, I want a MemoryStore protocol and typed MemoryItem hierarchy so that memory backends are pluggable and type-safe.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/base.py",
        "MemoryStore protocol: add(), get(), search(), clear()",
        "MemoryItem with subtypes: SystemMemory, HumanMemory, AIMemory, ToolMemory",
        "MemoryMetadata with user_id, session_id, task_id, agent_id scoping",
        "Status lifecycle: DRAFT -> ACCEPTED -> DISCARD",
        "~120 lines of source code",
        "Tests cover: types, status transitions, protocol conformance",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 36,
      "passes": true,
      "notes": "MemoryStatus (StrEnum: draft/accepted/discard), MemoryMetadata (frozen Pydantic: user_id/session_id/task_id/agent_id/extra), MemoryItem (base with transition()), SystemMemory/HumanMemory/AIMemory/ToolMemory subtypes, MemoryStore (runtime_checkable Protocol: add/get/search/clear). 38 new tests (1191 total)."
    },
    {
      "id": "US-037",
      "title": "Implement short-term memory",
      "description": "As a framework developer, I want short-term memory that manages conversation context with scope-based filtering and truncation.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/short_term.py",
        "ShortTermMemory implementing MemoryStore",
        "Scope-based filtering: user, session, task",
        "Incomplete message pair filtering (tool call/response integrity)",
        "~130 lines of source code",
        "Tests cover: add messages, truncation, windowing, scope filtering",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 37,
      "passes": true,
      "notes": "ShortTermMemory implementing MemoryStore with scope-based filtering (user/session/task), configurable windowing (max_rounds), and tool call/response integrity filtering. 32 new tests (1223 total)."
    },
    {
      "id": "US-038",
      "title": "Implement summary + compression",
      "description": "As a framework developer, I want summary trigger logic and multi-template summary generation so that long conversations can be compressed while preserving key information.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/summary.py",
        "Summary trigger: message count threshold, token count threshold",
        "Multi-template summaries: conversation, facts, profiles",
        "SummaryConfig -- prompts, thresholds, compression rules",
        "~130 lines of source code",
        "Tests cover: trigger detection, summary generation with mocked LLM",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 38,
      "passes": true,
      "notes": "SummaryTemplate (StrEnum: conversation/facts/profiles), SummaryConfig (frozen dataclass: thresholds, templates, prompts, keep_recent, token_estimate_ratio), check_trigger() with message/token thresholds, Summarizer protocol, generate_summary() with multi-template LLM-powered compression. 33 new tests (1256 total)."
    },
    {
      "id": "US-039",
      "title": "Implement long-term memory orchestrator",
      "description": "As a framework developer, I want long-term memory with async LLM extraction of user profiles, agent experiences, and facts, so that agents build persistent knowledge across sessions.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/long_term.py",
        "LongTermMemory -- persistent memory across sessions",
        "MemoryOrchestrator -- async extraction of UserProfile, AgentExperience, Facts",
        "Processing task queue with status tracking",
        "~130 lines of source code",
        "Tests cover: extraction with mocked LLM, task queue lifecycle",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 39,
      "passes": true,
      "notes": "LongTermMemory (MemoryStore protocol, deduplication, namespace isolation), ExtractionType (StrEnum: user_profile/agent_experience/facts), ExtractionTask (lifecycle: pending→running→completed/failed), Extractor protocol, MemoryOrchestrator (submit/process/process_all with task queue), OrchestratorConfig. 47 new tests (1303 total)."
    },
    {
      "id": "US-040",
      "title": "Implement SQLite memory backend",
      "description": "As a framework developer, I want a SQLite-backed memory store so that memory persists to disk without external dependencies.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/backends/sqlite.py",
        "SQLiteMemoryStore implementing MemoryStore",
        "JSON indexes for metadata fields, soft deletes, version field",
        "~120 lines of source code",
        "Tests with temp SQLite database",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 40,
      "passes": true,
      "notes": "SQLiteMemoryStore implementing MemoryStore protocol with JSON metadata indexes (json_extract), soft deletes, upsert with version bumping, async context manager lifecycle, subtype reconstruction (System/Human/AI/Tool). 34 new tests (1337 total)."
    },
    {
      "id": "US-041",
      "title": "Implement Postgres memory backend",
      "description": "As a framework developer, I want a Postgres-backed memory store for production deployments.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/backends/postgres.py",
        "PostgresMemoryStore implementing MemoryStore",
        "~120 lines of source code",
        "Tests (may need mock or skip if no postgres available)",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 41,
      "passes": true,
      "notes": "PostgresMemoryStore implementing MemoryStore protocol with asyncpg connection pool, JSONB metadata indexes, soft deletes, upsert with version bumping, $N parameterized queries, ILIKE search, subtype reconstruction. 33 unit tests (mocked asyncpg) + 6 integration tests (skipped without POSTGRES_DSN). 1370 total tests."
    },
    {
      "id": "US-042",
      "title": "Implement embeddings + vector search",
      "description": "As a framework developer, I want embedding-based vector search for semantic memory retrieval.",
      "acceptanceCriteria": [
        "Create packages/orbiter-memory/src/orbiter/memory/backends/vector.py",
        "VectorMemoryStore wrapping embedding + vector DB",
        "Embeddings ABC with sync + async variants",
        "OpenAI-compatible embedding provider with dimension support",
        "~130 lines of source code",
        "Tests with mocked embeddings",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 42,
      "passes": true,
      "notes": "Embeddings ABC (sync embed + async aembed + dimension property), OpenAIEmbeddings (lazy openai import, dimension support, asyncio.to_thread for async), VectorMemoryStore (in-memory cosine similarity search, post-filter by metadata/type/status, semantic ranking). 35 new tests (1405 total)."
    },
    {
      "id": "US-043",
      "title": "Memory package __init__ + integration",
      "description": "As a framework user, I want the memory package to have a clean public API and integrate with Agent.",
      "acceptanceCriteria": [
        "Update packages/orbiter-memory/src/orbiter/memory/__init__.py -- exports",
        "Wire memory into Agent",
        "Memory event integration (emit memory events for async processing)",
        "~80 lines of source code",
        "uv run pytest -- all tests pass for orbiter-memory",
        "Typecheck passes"
      ],
      "priority": 43,
      "passes": true,
      "notes": "Updated __init__.py with 28 public exports (base types, short-term, long-term, summary, events). Created MemoryEventEmitter wrapping MemoryStore + EventBus for async memory:added/searched/cleared events. 25 integration tests (public API imports, event emitter CRUD, Agent wiring, E2E pipelines). 1430 total tests."
    },
    {
      "id": "US-044",
      "title": "MCP client — server connection",
      "description": "As a framework developer, I want an MCP client that connects to MCP servers via SSE, stdio, and streamable-http transports.",
      "acceptanceCriteria": [
        "Create packages/orbiter-mcp/src/orbiter/mcp/client.py",
        "Multiple transport types: SSE, stdio, streamable-http",
        "Server instance caching/reuse with session isolation",
        "~130 lines of source code",
        "Tests cover: connection lifecycle, transport selection",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 44,
      "passes": true,
      "notes": "MCPClientError, MCPTransport (StrEnum), MCPServerConfig (validated), MCPServerConnection (connect/list_tools/call_tool/cleanup with async context manager, tool caching), MCPClient (multi-server manager with caching/reuse, auto-connect). Three transport backends: stdio, SSE, streamable-http. 45 new tests (1475 total)."
    },
    {
      "id": "US-045",
      "title": "MCP tools — loading + conversion",
      "description": "As a framework developer, I want MCP tool schema extraction and conversion to Orbiter Tool format.",
      "acceptanceCriteria": [
        "Create packages/orbiter-mcp/src/orbiter/mcp/tools.py",
        "Tool schema extraction from MCP server",
        "mcp__ namespace mapping for tool names",
        "Tool black/white-list filtering",
        "~120 lines of source code",
        "Tests cover: schema conversion, namespace mapping, filtering",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 45,
      "passes": true,
      "notes": "MCPToolError, MCPToolFilter (include/exclude), namespace_tool_name/parse_namespaced_name, extract_schema, MCPToolWrapper (Tool subclass with call_fn delegation), convert_mcp_tools, load_tools_from_connection, load_tools_from_client. 44 new tests (1519 total)."
    },
    {
      "id": "US-046",
      "title": "MCP server decorator",
      "description": "As a framework user, I want an @mcp_server() class decorator that converts Python methods to MCP tools, so that I can expose tools as MCP servers.",
      "acceptanceCriteria": [
        "Create packages/orbiter-mcp/src/orbiter/mcp/server.py",
        "@mcp_server() class decorator converting methods to MCP tools",
        "MCPServerRegistry for singleton server instances",
        "~120 lines of source code",
        "Tests cover: decorator, method->tool conversion, registry",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 46,
      "passes": true,
      "notes": "MCPServerError, MCPServerRegistry (singleton class/instance store), @mcp_server() class decorator (method→MCP tool conversion via FastMCP, run/stop methods, global registry). 32 new tests (1551 total)."
    },
    {
      "id": "US-047",
      "title": "MCP execution + tests",
      "description": "As a framework developer, I want MCP tool execution with retry logic and integration tests.",
      "acceptanceCriteria": [
        "Retry logic with configurable timeout",
        "Environment variable substitution in mcp.json config",
        "Integration tests for full MCP lifecycle",
        "~100 lines of source code",
        "uv run pytest -- all tests pass for orbiter-mcp",
        "Typecheck passes"
      ],
      "priority": 47,
      "passes": true,
      "notes": "MCPExecutionError, call_tool_with_retry (exponential backoff, per-attempt timeout, non-retryable MCPClientError), substitute_env_vars (${VAR} pattern), load_mcp_config/load_mcp_client (mcp.json loader with env var substitution). 31 new tests (1582 total)."
    },
    {
      "id": "US-048",
      "title": "YAML agent loader",
      "description": "As a framework user, I want to define agents and swarms in YAML files with variable substitution, so that I can create agents without writing Python code.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/loader.py",
        "Load agent/swarm definitions from YAML",
        "${ENV_VAR} and ${vars.KEY} substitution",
        "Swarm topology patterns: workflow, handoff, team",
        "Agent factory dispatch (builtin vs. custom classes)",
        "~130 lines of source code",
        "Tests cover: YAML parsing, variable substitution, swarm creation from YAML",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 48,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-049",
      "title": "Skill registry",
      "description": "As a framework user, I want a multi-source skill registry that loads skills from local paths and GitHub URLs, so that agents can discover and use reusable skill libraries.",
      "acceptanceCriteria": [
        "Create packages/orbiter-core/src/orbiter/skills.py",
        "SkillRegistry -- multi-source skill management",
        "GitHub URL parsing & shallow clone with branch support, cached at ~/.orbiter/skills/",
        "YAML front-matter extraction (name, desc, tool_list, type, active)",
        "Conflict resolution strategies (keep_first, keep_last, raise)",
        "Search + filtering capabilities",
        "~130 lines of source code",
        "Tests cover: local skill loading, registry operations, search, conflict resolution",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 49,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-050",
      "title": "Sandbox interface + local sandbox",
      "description": "As a framework developer, I want a Sandbox ABC and local implementation so that agents can execute code in controlled environments.",
      "acceptanceCriteria": [
        "Create packages/orbiter-sandbox/src/orbiter/sandbox/base.py",
        "SandboxStatus enum: INIT, RUNNING, IDLE, ERROR, CLOSED",
        "Sandbox ABC with workspace, MCP integration, agent configuration",
        "LocalSandbox implementation",
        "~130 lines of source code",
        "Tests cover: lifecycle, status transitions",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 50,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-051",
      "title": "Built-in sandbox tools",
      "description": "As a framework user, I want sandboxed filesystem and terminal tools so that agents can interact with the local environment safely.",
      "acceptanceCriteria": [
        "Create packages/orbiter-sandbox/src/orbiter/sandbox/tools.py",
        "FilesystemTool with allowed_directories sandboxing, safe path validation",
        "TerminalTool with dangerous command blacklist, platform detection, timeout",
        "~130 lines of source code",
        "Tests cover: path validation, command filtering, sandbox boundaries",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 51,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-052",
      "title": "Sandbox builder",
      "description": "As a framework user, I want a fluent builder API for constructing sandboxes with method chaining.",
      "acceptanceCriteria": [
        "Create packages/orbiter-sandbox/src/orbiter/sandbox/builder.py",
        "Fluent API with method chaining for sandbox construction",
        "Lazy evaluation: auto-build on first API call",
        "~120 lines of source code",
        "Tests cover: builder chaining, lazy evaluation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 52,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-053",
      "title": "Kubernetes sandbox",
      "description": "As a framework developer, I want a Kubernetes-based sandbox for remote execution environments.",
      "acceptanceCriteria": [
        "Create packages/orbiter-sandbox/src/orbiter/sandbox/kubernetes.py",
        "KubernetesSandbox implementing Sandbox",
        "~120 lines of source code",
        "Tests (may need mock)",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 53,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-054",
      "title": "Trace config + base",
      "description": "As a framework developer, I want trace configuration and semantic conventions for agent/tool observability attributes.",
      "acceptanceCriteria": [
        "Create packages/orbiter-trace/src/orbiter/trace/config.py",
        "TraceConfig with backend selection, sampling, export settings",
        "Semantic conventions for gen_ai.*, agent.*, tool.* attributes",
        "~120 lines of source code",
        "Tests cover: config creation, convention constants",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 54,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-055",
      "title": "Span decorator + context manager",
      "description": "As a framework user, I want a @traced decorator that instruments sync/async functions with span creation and metadata extraction.",
      "acceptanceCriteria": [
        "Create packages/orbiter-trace/src/orbiter/trace/decorator.py",
        "@traced decorator supporting sync, async, generators, async generators",
        "Function metadata extraction (qualname, module, line number, parameters)",
        "Stack frame analysis with user-code filtering",
        "~120 lines of source code",
        "Tests cover: sync/async decoration, metadata extraction",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 55,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-056",
      "title": "Agent/tool instrumentation",
      "description": "As a framework developer, I want automatic agent and tool metrics (duration, counter, token usage) so that execution is observable.",
      "acceptanceCriteria": [
        "Create packages/orbiter-trace/src/orbiter/trace/instrumentation.py",
        "Agent metrics: agent_run_duration (histogram), agent_run_counter, agent_token_usage",
        "Tool metrics: tool_step_duration, tool_step_counter",
        "~120 lines of source code",
        "Tests cover: metric recording, histogram buckets",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 56,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-057",
      "title": "Trace context propagation",
      "description": "As a framework developer, I want W3C Baggage standard propagation for cross-service trace correlation.",
      "acceptanceCriteria": [
        "Create packages/orbiter-trace/src/orbiter/trace/propagation.py",
        "W3C Baggage standard (RFC 9110) propagation",
        "Span consumer plugin system with @register_span_consumer decorator",
        "~100 lines of source code",
        "Tests cover: propagation headers, consumer registration",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 57,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-058",
      "title": "Prompt execution logger",
      "description": "As a framework developer, I want structured LLM execution logging with token breakdown and context window usage analysis.",
      "acceptanceCriteria": [
        "Create packages/orbiter-trace/src/orbiter/trace/prompt_logger.py",
        "Structured logging: token breakdown by role, context window usage",
        "Multi-modal content logging (text, images, tool_use)",
        "~130 lines of source code",
        "Tests cover: log formatting, token breakdown",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 58,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-059",
      "title": "Eval types + base evaluator",
      "description": "As a framework developer, I want an Evaluator with parallel execution and pass@k metrics so that agent quality can be measured systematically.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/base.py",
        "EvalTarget ABC, EvalCriteria with threshold-based pass/fail",
        "Evaluator with parallel execution, repeat_times, pass@k metrics",
        "EvalResult, EvalCaseResult, ScorerResult dataclasses",
        "~100 lines of source code",
        "Tests cover: evaluator lifecycle, pass@k calculation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 59,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-060",
      "title": "Rule-based scorers",
      "description": "As a framework user, I want rule-based scorers for format validation, schema compliance, and output correctness checks.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/scorers.py",
        "FormatValidationScorer -- JSON, XML, YAML, Markdown, CSV",
        "SchemaValidationScorer -- JSON schema compliance",
        "OutputCorrectnessScorer -- ground truth matching, keyword checking",
        "OutputLengthScorer, OutputRelevanceScorer, OutputCompletenessScorer",
        "~130 lines of source code",
        "Tests cover: each scorer with pass/fail cases",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 60,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-061",
      "title": "LLM-as-Judge + quality scorers",
      "description": "As a framework user, I want LLM-based scoring with configurable judge prompts and weighted multi-dimensional quality assessment.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/llm_scorer.py or add to scorers.py",
        "LLMAsJudgeScorer -- configurable judge prompts",
        "OutputQualityScorer -- weighted 5-dimensional: correctness (40%), relevance (20%), completeness (20%), clarity (10%), professionalism (10%)",
        "LogicConsistencyScorer, ReasoningValidityScorer, ConstraintSatisfactionScorer",
        "~130 lines of source code",
        "Tests with mocked LLM judge",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 61,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-062",
      "title": "Trajectory + time scorers",
      "description": "As a framework developer, I want trajectory validation, time cost, and accuracy scorers with a scorer registry.",
      "acceptanceCriteria": [
        "TrajectoryValidators -- trajectory step validation",
        "TimeCostScorer, AnswerAccuracyLLMScorer, LabelDistributionScorer",
        "Scorer registry with @scorer_register() decorator",
        "~100 lines of source code",
        "Tests cover: each scorer, registry decorator",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 62,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-063",
      "title": "Reflection framework",
      "description": "As a framework developer, I want a reflection framework with LLM-powered analysis, insight extraction, and suggestion generation, so that agents can learn from their executions.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/reflection.py",
        "Reflector ABC with three-step template: analyze(), insight(), suggest()",
        "GeneralReflector using LLM: summary, key_findings, root_causes, insights, suggestions",
        "ReflectionType enum (SUCCESS, FAILURE, OPTIMIZATION, PATTERN, INSIGHT)",
        "ReflectionLevel enum (SHALLOW, MEDIUM, DEEP, META)",
        "ReflectionHistory tracking with summarization",
        "~130 lines of source code",
        "Tests with mocked LLM reflection",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 63,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-064",
      "title": "Ralph loop — state + config",
      "description": "As a framework developer, I want Ralph loop configuration (validation, reflection, stop conditions) and iteration state tracking, so that iterative refinement has a structured lifecycle.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/ralph/config.py",
        "RalphConfig unifying: ValidationConfig, ReflectionConfig, StopConditionConfig",
        "LoopState -- iteration tracking, score history, reflection history",
        "~120 lines of source code",
        "Tests cover: config creation, state transitions",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 64,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-065",
      "title": "Ralph loop — stop detectors",
      "description": "As a framework developer, I want pluggable stop condition detectors for the Ralph loop (max iterations, timeout, cost limit, score threshold).",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/ralph/detectors.py",
        "StopDetector ABC with pluggable implementations",
        "Built-in: MaxIterationDetector, TimeoutDetector, CostLimitDetector, ConsecutiveFailureDetector, ScoreThresholdDetector",
        "~100 lines of source code",
        "Tests cover: each detector type, composite detection",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 65,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-066",
      "title": "Ralph loop — runner",
      "description": "As a framework user, I want a RalphRunner that implements the 5-phase Run->Analyze->Learn->Plan->Halt loop for iterative agent refinement.",
      "acceptanceCriteria": [
        "Create packages/orbiter-eval/src/orbiter/eval/ralph/runner.py",
        "RalphRunner -- Run -> Analyze (score) -> Learn (reflect) -> Plan (re-prompt) -> Halt (detect stop)",
        "Integration with Evaluator scorers and Reflection framework",
        "~130 lines of source code",
        "Tests cover: full loop with mocked agent, early stopping, score improvement",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 66,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-067",
      "title": "A2A types + agent card",
      "description": "As a framework developer, I want A2A protocol types including AgentCard with skills and transport capabilities.",
      "acceptanceCriteria": [
        "Create packages/orbiter-a2a/src/orbiter/a2a/types.py",
        "AgentCard with skills, transport modes, streaming capabilities",
        "ServingConfig, ClientConfig Pydantic models",
        "Task event types: TaskArtifactUpdateEvent, TaskStatusUpdateEvent",
        "~130 lines of source code",
        "Tests cover: type creation, serialization",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 67,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-068",
      "title": "A2A server",
      "description": "As a framework user, I want a FastAPI-based A2A server that exposes agents via the A2A protocol with agent card discovery.",
      "acceptanceCriteria": [
        "Create packages/orbiter-a2a/src/orbiter/a2a/server.py",
        "FastAPI server with .well-known/agent-card endpoint",
        "Agent executor wrapping, streaming support",
        "TaskStore abstraction (in-memory default)",
        "~130 lines of source code",
        "Tests cover: agent card endpoint, task execution",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 68,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-069",
      "title": "A2A client + remote agent",
      "description": "As a framework user, I want an A2A client and RemoteAgent that calls remote A2A agents, so that agents can communicate across services.",
      "acceptanceCriteria": [
        "Create packages/orbiter-a2a/src/orbiter/a2a/client.py",
        "Thread-safe client manager with per-thread instances",
        "Agent card resolution from URL/file",
        "RemoteAgent -- Agent subclass for calling remote A2A agents",
        "Task->A2A message conversion, streaming event handling",
        "~130 lines of source code",
        "Tests cover: client lifecycle, remote agent invocation (mocked)",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 69,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-070",
      "title": "CLI entry point + config",
      "description": "As a framework user, I want an orbiter CLI command with config file support.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/main.py",
        "CLI entry point with argument parsing",
        "Config file loading (.orbiter.yaml, orbiter.config.yaml)",
        "~130 lines of source code",
        "Tests cover: arg parsing, config loading",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 70,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-071",
      "title": "Agent discovery + loading",
      "description": "As a CLI user, I want agents to be auto-discovered from the current directory and loaded for execution.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/loader.py",
        "Agent file scanning (.py, .yaml, .md agent definitions)",
        "Agent registration and validation",
        "~130 lines of source code",
        "Tests cover: discovery, loading, validation",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 71,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-072",
      "title": "Interactive console",
      "description": "As a CLI user, I want an interactive REPL console for chatting with agents.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/console.py",
        "Interactive REPL with prompt, streaming output, command handling",
        "~130 lines of source code",
        "Tests cover: command parsing, output formatting",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 72,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-073",
      "title": "Local executor",
      "description": "As a CLI user, I want local agent execution with streaming output and error handling.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/executor.py",
        "Local execution wrapping run() / run.stream()",
        "~120 lines of source code",
        "Tests cover: execution lifecycle",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 73,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-074",
      "title": "Plugin system",
      "description": "As a CLI user, I want a plugin system for extending CLI functionality.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/plugins.py",
        "Plugin discovery, loading, lifecycle",
        "~100 lines of source code",
        "Tests cover: plugin loading, hook points",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 74,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-075",
      "title": "Batch execution",
      "description": "As a CLI user, I want batch execution mode for running agents against multiple inputs from files or stdin.",
      "acceptanceCriteria": [
        "Create packages/orbiter-cli/src/orbiter_cli/batch.py",
        "Batch input loading (JSON, CSV, JSONL)",
        "Parallel execution with concurrency control",
        "Result aggregation and output",
        "~120 lines of source code",
        "Tests cover: batch loading, parallel execution",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 75,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-076",
      "title": "FastAPI app + chat route",
      "description": "As a framework user, I want a web server with a chat API endpoint for running agents via HTTP.",
      "acceptanceCriteria": [
        "Create packages/orbiter-server/src/orbiter_server/app.py",
        "FastAPI app with /chat endpoint",
        "Request/response models, streaming SSE support",
        "~130 lines of source code",
        "Tests cover: chat endpoint, streaming",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 76,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-077",
      "title": "Session management route",
      "description": "As a server user, I want session management APIs for creating, listing, and resuming chat sessions.",
      "acceptanceCriteria": [
        "Create packages/orbiter-server/src/orbiter_server/sessions.py",
        "CRUD routes for sessions",
        "~120 lines of source code",
        "Tests cover: session lifecycle",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 77,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-078",
      "title": "Agent management + workspace routes",
      "description": "As a server user, I want API routes for listing agents and accessing workspace artifacts.",
      "acceptanceCriteria": [
        "Create packages/orbiter-server/src/orbiter_server/agents.py",
        "Agent listing, workspace file listing/reading",
        "~120 lines of source code",
        "Tests cover: agent listing, workspace access",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 78,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-079",
      "title": "Streaming + WebSocket support",
      "description": "As a server user, I want WebSocket support for real-time streaming agent output.",
      "acceptanceCriteria": [
        "WebSocket endpoint for streaming",
        "SSE fallback for non-WebSocket clients",
        "~100 lines of source code",
        "Tests cover: WebSocket connection, streaming events",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 79,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-080",
      "title": "Trajectory dataset",
      "description": "As a training user, I want trajectory capture with strategy patterns and export to JSON/CSV.",
      "acceptanceCriteria": [
        "Create packages/orbiter-train/src/orbiter/train/trajectory.py",
        "TrajectoryItem model, TrajectoryDataset with strategy pattern",
        "append_trajectory(), from_messages(), save_task_trajectory()",
        "Export to JSON/CSV",
        "~130 lines of source code",
        "Tests cover: trajectory capture, export",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 80,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-081",
      "title": "Base trainer",
      "description": "As a training user, I want a base Trainer abstraction for fine-tuning agents.",
      "acceptanceCriteria": [
        "Create packages/orbiter-train/src/orbiter/train/trainer.py",
        "Trainer ABC with train/eval lifecycle",
        "~120 lines of source code",
        "Tests cover: trainer lifecycle",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 81,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-082",
      "title": "Data synthesis",
      "description": "As a training user, I want data synthesis utilities for generating training data from agent executions.",
      "acceptanceCriteria": [
        "Create packages/orbiter-train/src/orbiter/train/synthesis.py",
        "Data synthesis from trajectories",
        "~130 lines of source code",
        "Tests cover: synthesis pipeline",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 82,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-083",
      "title": "Agent evolution",
      "description": "As a training user, I want agent evolution utilities for iterative improvement.",
      "acceptanceCriteria": [
        "Create packages/orbiter-train/src/orbiter/train/evolution.py",
        "Agent evolution strategies",
        "~120 lines of source code",
        "Tests cover: evolution lifecycle",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 83,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-084",
      "title": "VeRL integration",
      "description": "As a training user, I want VeRL integration for reinforcement learning from human feedback.",
      "acceptanceCriteria": [
        "Create packages/orbiter-train/src/orbiter/train/verl.py",
        "VeRL integration layer",
        "~130 lines of source code",
        "Tests cover: integration (mocked)",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 84,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-085",
      "title": "Basic examples — define_agent, use_llm",
      "description": "As a new user, I want simple examples showing how to define an agent and use LLM providers.",
      "acceptanceCriteria": [
        "Create examples/quickstart/define_agent.py",
        "Create examples/quickstart/use_llm.py",
        "~80 lines total",
        "Examples run without error (with valid API keys)",
        "Typecheck passes"
      ],
      "priority": 85,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-086",
      "title": "Tool examples — local_tool, mcp_tool",
      "description": "As a new user, I want examples showing how to create local tools and use MCP tools.",
      "acceptanceCriteria": [
        "Create examples/quickstart/local_tool.py",
        "Create examples/quickstart/mcp_tool.py",
        "~80 lines total",
        "Typecheck passes"
      ],
      "priority": 86,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-087",
      "title": "Swarm examples — workflow, handoff, hybrid_swarm",
      "description": "As a new user, I want examples showing multi-agent orchestration patterns.",
      "acceptanceCriteria": [
        "Create examples/quickstart/workflow.py",
        "Create examples/quickstart/handoff.py",
        "Create examples/quickstart/hybrid_swarm.py",
        "~80 lines total",
        "Typecheck passes"
      ],
      "priority": 87,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-088",
      "title": "Memory + trace examples",
      "description": "As a new user, I want examples showing memory and tracing usage.",
      "acceptanceCriteria": [
        "Create examples/quickstart/use_memory.py",
        "Create examples/quickstart/use_trace.py",
        "~80 lines total",
        "Typecheck passes"
      ],
      "priority": 88,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-089",
      "title": "Advanced quickstart — parallel_task, HITL, multi_root_agent, serving",
      "description": "As a user, I want examples showing advanced patterns like parallel tasks, human-in-the-loop, multi-root agents, and serving.",
      "acceptanceCriteria": [
        "Create examples/quickstart/parallel_task.py",
        "Create examples/quickstart/hitl.py",
        "Create examples/quickstart/multi_root_agent.py",
        "Create examples/quickstart/serving.py",
        "~100 lines total",
        "Typecheck passes"
      ],
      "priority": 89,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-090",
      "title": "Config-driven + CLI examples",
      "description": "As a user, I want examples showing YAML-based agent definitions and CLI usage.",
      "acceptanceCriteria": [
        "Create examples/quickstart/config_driven.py",
        "Create examples/quickstart/agents.yaml",
        "Create examples/quickstart/cli_usage.py",
        "~80 lines total",
        "Typecheck passes"
      ],
      "priority": 90,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-091",
      "title": "Collaborative examples — debate, travel",
      "description": "As a user, I want examples showing collaborative multi-agent patterns.",
      "acceptanceCriteria": [
        "Create examples/multi_agent/debate.py",
        "Create examples/multi_agent/travel.py",
        "~100 lines total",
        "Typecheck passes"
      ],
      "priority": 91,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-092",
      "title": "Coordination examples — custom_agent, deepresearch, master_worker",
      "description": "As a user, I want examples showing coordination patterns.",
      "acceptanceCriteria": [
        "Create examples/multi_agent/custom_agent.py",
        "Create examples/multi_agent/deepresearch.py",
        "Create examples/multi_agent/master_worker.py",
        "~100 lines total",
        "Typecheck passes"
      ],
      "priority": 92,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-093",
      "title": "Workflow examples — search patterns",
      "description": "As a user, I want examples showing workflow search patterns.",
      "acceptanceCriteria": [
        "Create examples/multi_agent/search_patterns.py",
        "~60 lines total",
        "Typecheck passes"
      ],
      "priority": 93,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-094",
      "title": "GAIA benchmark",
      "description": "As a benchmark user, I want a GAIA benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/gaia/ with benchmark runner",
        "~150 lines",
        "Typecheck passes"
      ],
      "priority": 94,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-095",
      "title": "IMO benchmark",
      "description": "As a benchmark user, I want an IMO benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/imo/ with benchmark runner",
        "~150 lines",
        "Typecheck passes"
      ],
      "priority": 95,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-096",
      "title": "OSWorld benchmark",
      "description": "As a benchmark user, I want an OSWorld benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/osworld/ with benchmark runner",
        "~150 lines",
        "Typecheck passes"
      ],
      "priority": 96,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-097",
      "title": "VisualWebArena benchmark",
      "description": "As a benchmark user, I want a VisualWebArena benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/visualwebarena/ with benchmark runner",
        "~150 lines",
        "Typecheck passes"
      ],
      "priority": 97,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-098",
      "title": "XBench benchmark",
      "description": "As a benchmark user, I want an XBench benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/xbench/ with benchmark runner",
        "~150 lines",
        "Typecheck passes"
      ],
      "priority": 98,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-099",
      "title": "BFCL + other benchmarks",
      "description": "As a benchmark user, I want a BFCL benchmark runner example.",
      "acceptanceCriteria": [
        "Create examples/benchmarks/bfcl/ with benchmark runner",
        "~120 lines",
        "Typecheck passes"
      ],
      "priority": 99,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-100",
      "title": "Skill agent + web deployment examples",
      "description": "As a user, I want advanced examples showing skill agents and web deployment.",
      "acceptanceCriteria": [
        "Create examples/advanced/skill_agent.py",
        "Create examples/advanced/web_deploy.py",
        "~120 lines total",
        "Typecheck passes"
      ],
      "priority": 100,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-101",
      "title": "Common tools library",
      "description": "As a user, I want a common tools library with reusable tool implementations.",
      "acceptanceCriteria": [
        "Create examples/common/tools/ with reusable tool implementations",
        "Browser, search API, document tools",
        "~150 lines total",
        "Typecheck passes"
      ],
      "priority": 101,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-102",
      "title": "Training examples",
      "description": "As a training user, I want training pipeline examples.",
      "acceptanceCriteria": [
        "Create examples/training/ with training pipeline examples",
        "~120 lines total",
        "Typecheck passes"
      ],
      "priority": 102,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-103",
      "title": "Public API finalization",
      "description": "As a framework developer, I want all package __init__.py files to export the correct public API surface with __all__ declarations.",
      "acceptanceCriteria": [
        "Audit all __init__.py files across all 13 packages",
        "Ensure exports match design spec section 1.6",
        "Add __all__ where needed",
        "~50 lines of changes",
        "uv run pytest -- all tests pass",
        "Typecheck passes"
      ],
      "priority": 103,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-104",
      "title": "GitHub Actions CI",
      "description": "As a framework developer, I want GitHub Actions CI that runs lint, format, typecheck, and tests on every PR.",
      "acceptanceCriteria": [
        "Create .github/workflows/ci.yml",
        "Jobs: ruff check, ruff format, pyright, pytest",
        "Matrix: Python 3.11",
        "Cache UV dependencies",
        "~80 lines",
        "CI passes on push",
        "Typecheck passes"
      ],
      "priority": 104,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-105",
      "title": "README + migration guide",
      "description": "As an AWorld user, I want a README and migration guide so that I can understand Orbiter and migrate from AWorld.",
      "acceptanceCriteria": [
        "Create README.md -- project overview, installation, quick start, API examples",
        "Create docs/migration-guide.md -- AWorld -> Orbiter mapping",
        "~150 lines total",
        "Typecheck passes"
      ],
      "priority": 105,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-106",
      "title": "Delete old/ + final verification",
      "description": "As a framework developer, I want to remove the old/ reference directory and run final verification that everything works without it.",
      "acceptanceCriteria": [
        "Delete old/ directory",
        "uv sync succeeds",
        "uv run ruff check packages/ -- zero lint errors",
        "uv run ruff format --check packages/ -- all formatted",
        "uv run pyright packages/ -- zero type errors",
        "uv run pytest -- all tests pass",
        "All examples run (with valid API keys)",
        "Typecheck passes"
      ],
      "priority": 106,
      "passes": false,
      "notes": ""
    }
  ]
}
