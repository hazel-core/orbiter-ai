"""Agent class: the core autonomous unit in Orbiter."""

from __future__ import annotations

import asyncio
from collections.abc import Callable, Sequence
from typing import Any

from pydantic import BaseModel

from orbiter._internal.message_builder import build_messages
from orbiter._internal.output_parser import parse_response, parse_tool_arguments
from orbiter.config import parse_model_string
from orbiter.hooks import Hook, HookManager, HookPoint
from orbiter.observability.logging import get_logger  # pyright: ignore[reportMissingImports]
from orbiter.tool import Tool, ToolError
from orbiter.types import (
    AgentOutput,
    AssistantMessage,
    Message,
    OrbiterError,
    ToolResult,
    UserMessage,
)

_log = get_logger(__name__)


class AgentError(OrbiterError):
    """Raised for agent-level errors (duplicate tools, invalid config, etc.)."""


class Agent:
    """An autonomous LLM-powered agent with tools and lifecycle hooks.

    Agents are the core building block in Orbiter. Each agent wraps an LLM
    model, a set of tools, optional handoff targets, and lifecycle hooks.
    The ``run()`` method (added in a later session) executes the agent's
    tool loop.

    All parameters are keyword-only; only ``name`` is required.

    Args:
        name: Unique identifier for this agent.
        model: Model string in ``"provider:model_name"`` format.
        instructions: System prompt. Can be a string or an async callable
            that receives a context dict and returns a string.
        tools: Tools available to this agent.
        handoffs: Other agents this agent can delegate to via handoff.
        hooks: Lifecycle hooks as ``(HookPoint, Hook)`` tuples.
        output_type: Pydantic model class for structured output validation.
        max_steps: Maximum LLM-tool round-trips before stopping.
        temperature: LLM sampling temperature.
        max_tokens: Maximum output tokens per LLM call.
        memory: Optional memory store for persistent memory across sessions.
        context: Optional context engine for hierarchical state and prompt building.
    """

    def __init__(
        self,
        *,
        name: str,
        model: str = "openai:gpt-4o",
        instructions: str | Callable[..., str] = "",
        tools: list[Tool] | None = None,
        handoffs: list[Agent] | None = None,
        hooks: list[tuple[HookPoint, Hook]] | None = None,
        output_type: type[BaseModel] | None = None,
        max_steps: int = 10,
        temperature: float = 1.0,
        max_tokens: int | None = None,
        memory: Any = None,
        context: Any = None,
    ) -> None:
        self.name = name
        self.model = model
        self.provider_name, self.model_name = parse_model_string(model)
        self.instructions = instructions
        self.output_type = output_type
        self.max_steps = max_steps
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.memory = memory
        self.context = context

        # Tools indexed by name for O(1) lookup during execution
        self.tools: dict[str, Tool] = {}
        if tools:
            for t in tools:
                self._register_tool(t)

        # Handoff targets indexed by name
        self.handoffs: dict[str, Agent] = {}
        if handoffs:
            for agent in handoffs:
                self._register_handoff(agent)

        # Lifecycle hooks
        self.hook_manager = HookManager()
        if hooks:
            for point, hook in hooks:
                self.hook_manager.add(point, hook)

    def _register_tool(self, t: Tool) -> None:
        """Add a tool, raising on duplicate names.

        Args:
            t: The tool to register.

        Raises:
            AgentError: If a tool with the same name is already registered.
        """
        if t.name in self.tools:
            raise AgentError(f"Duplicate tool name '{t.name}' on agent '{self.name}'")
        self.tools[t.name] = t

    def _register_handoff(self, agent: Agent) -> None:
        """Add a handoff target, raising on duplicate names.

        Args:
            agent: The target agent.

        Raises:
            AgentError: If a handoff with the same name is already registered.
        """
        if agent.name in self.handoffs:
            raise AgentError(f"Duplicate handoff agent '{agent.name}' on agent '{self.name}'")
        self.handoffs[agent.name] = agent

    def get_tool_schemas(self) -> list[dict[str, Any]]:
        """Return OpenAI-format tool schemas for all registered tools.

        Returns:
            A list of tool schema dicts suitable for LLM function calling.
        """
        return [t.to_schema() for t in self.tools.values()]

    def describe(self) -> dict[str, Any]:
        """Return a summary of the agent's capabilities.

        Useful for debugging, logging, and capability advertisement
        in multi-agent systems.

        Returns:
            A dict with the agent's name, model, tools, and configuration.
        """
        return {
            "name": self.name,
            "model": self.model,
            "tools": list(self.tools.keys()),
            "handoffs": list(self.handoffs.keys()),
            "max_steps": self.max_steps,
            "output_type": (self.output_type.__name__ if self.output_type else None),
        }

    async def run(
        self,
        input: str,
        *,
        messages: Sequence[Message] | None = None,
        provider: Any = None,
        max_retries: int = 3,
    ) -> AgentOutput:
        """Execute the agent's LLM-tool loop with retry logic.

        Builds the message list, calls the LLM, and if tool calls are
        returned, executes them in parallel, feeds results back, and
        re-calls the LLM. The loop continues until a text-only response
        is produced or ``max_steps`` is reached.

        Args:
            input: User query string for this turn.
            messages: Prior conversation history.
            provider: An object with an ``async complete()`` method
                (e.g. a ``ModelProvider`` instance).
            max_retries: Maximum retry attempts for transient errors.

        Returns:
            Parsed ``AgentOutput`` from the final LLM response.

        Raises:
            AgentError: If no provider is supplied or all retries are exhausted.
        """
        if provider is None:
            raise AgentError(f"Agent '{self.name}' requires a provider for run()")

        # Resolve instructions
        instructions = self.instructions
        if callable(instructions):
            instructions = instructions(self.name)

        # Build initial message list
        history: list[Message] = list(messages) if messages else []
        history.append(UserMessage(content=input))
        msg_list = build_messages(instructions, history)

        # Tool schemas
        tool_schemas = self.get_tool_schemas() or None

        # Tool loop — iterate up to max_steps
        for _step in range(self.max_steps):
            output = await self._call_llm(msg_list, tool_schemas, provider, max_retries)

            # No tool calls — return the final text response
            if not output.tool_calls:
                return output

            # Execute tool calls and collect results
            actions = parse_tool_arguments(output.tool_calls)
            tool_results = await self._execute_tools(actions)

            # Append assistant message (with tool calls) and results to history
            msg_list.append(AssistantMessage(content=output.text, tool_calls=output.tool_calls))
            msg_list.extend(tool_results)

        # max_steps exhausted — return last output as-is
        return output

    async def _call_llm(
        self,
        msg_list: list[Message],
        tool_schemas: list[dict[str, Any]] | None,
        provider: Any,
        max_retries: int,
    ) -> AgentOutput:
        """Single LLM call with retry logic and lifecycle hooks."""
        last_error: Exception | None = None
        for attempt in range(max_retries):
            try:
                await self.hook_manager.run(HookPoint.PRE_LLM_CALL, agent=self, messages=msg_list)

                response = await provider.complete(
                    msg_list,
                    tools=tool_schemas,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                )

                await self.hook_manager.run(HookPoint.POST_LLM_CALL, agent=self, response=response)

                return parse_response(
                    content=response.content,
                    tool_calls=response.tool_calls,
                    usage=response.usage,
                )

            except Exception as exc:
                if _is_context_length_error(exc):
                    _log.error("Context length exceeded on '%s'", self.name)
                    raise AgentError(
                        f"Context length exceeded on agent '{self.name}': {exc}"
                    ) from exc

                last_error = exc
                if attempt < max_retries - 1:
                    _log.warning(
                        "Retry %d/%d for '%s': %s", attempt + 1, max_retries, self.name, exc
                    )
                    delay = 2**attempt
                    await asyncio.sleep(delay)

        _log.error("Agent '%s' failed after %d retries", self.name, max_retries)
        raise AgentError(
            f"Agent '{self.name}' failed after {max_retries} retries: {last_error}"
        ) from last_error

    async def _execute_tools(
        self,
        actions: list[Any],
    ) -> list[ToolResult]:
        """Execute tool calls in parallel, catching errors per-tool."""
        results: list[ToolResult] = [ToolResult(tool_call_id="", tool_name="")] * len(actions)

        async def _run_one(idx: int) -> None:
            action = actions[idx]
            tool = self.tools.get(action.tool_name)

            # PRE_TOOL_CALL hook
            await self.hook_manager.run(
                HookPoint.PRE_TOOL_CALL,
                agent=self,
                tool_name=action.tool_name,
                arguments=action.arguments,
            )

            if tool is None:
                result = ToolResult(
                    tool_call_id=action.tool_call_id,
                    tool_name=action.tool_name,
                    error=f"Unknown tool '{action.tool_name}'",
                )
            else:
                try:
                    output = await tool.execute(**action.arguments)
                    content = output if isinstance(output, str) else str(output)
                    result = ToolResult(
                        tool_call_id=action.tool_call_id,
                        tool_name=action.tool_name,
                        content=content,
                    )
                except (ToolError, Exception) as exc:
                    _log.warning("Tool '%s' failed on '%s': %s", action.tool_name, self.name, exc)
                    result = ToolResult(
                        tool_call_id=action.tool_call_id,
                        tool_name=action.tool_name,
                        error=str(exc),
                    )

            # POST_TOOL_CALL hook
            await self.hook_manager.run(
                HookPoint.POST_TOOL_CALL,
                agent=self,
                tool_name=action.tool_name,
                result=result,
            )

            results[idx] = result

        async with asyncio.TaskGroup() as tg:
            for i in range(len(actions)):
                tg.create_task(_run_one(i))

        return results

    def __repr__(self) -> str:
        parts = [f"name={self.name!r}", f"model={self.model!r}"]
        if self.tools:
            parts.append(f"tools={list(self.tools.keys())}")
        if self.handoffs:
            parts.append(f"handoffs={list(self.handoffs.keys())}")
        return f"Agent({', '.join(parts)})"


def _is_context_length_error(exc: Exception) -> bool:
    """Check if an exception represents a context-length overflow.

    Detects errors with a ``code`` attribute of ``"context_length"``
    (set by ``ModelError``) or common provider error messages.
    """
    code = getattr(exc, "code", "")
    if code == "context_length":
        return True
    msg = str(exc).lower()
    return "context_length" in msg or "context length" in msg
