{
  "project": "sukuna-compat",
  "branchName": "ralph/sukuna-compat",
  "description": "Distributed execution, rich streaming events, and observability for Sukuna compatibility — orbiter-distributed package, AWorld-compatible event types, and distributed metrics/tracing",
  "userStories": [
    {
      "id": "US-001",
      "title": "Add StepEvent type",
      "description": "As a developer consuming streaming events, I want a StepEvent emitted at the start and end of each agent step so that my frontend can show step-by-step progress.",
      "acceptanceCriteria": [
        "StepEvent added to orbiter/types.py with fields: type: Literal['step'], step_number: int, agent_name: str, status: Literal['started', 'completed'], started_at: float, completed_at: float | None, usage: Usage | None",
        "StepEvent is a frozen Pydantic BaseModel with .model_dump() JSON serialization",
        "StreamEvent union updated to include StepEvent",
        "Existing TextEvent and ToolCallEvent unchanged",
        "All existing tests pass",
        "Pyright passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Add to packages/orbiter-core/src/orbiter/types.py. Follow frozen BaseModel pattern used by TextEvent/ToolCallEvent. StreamEvent is a Union type alias at the bottom of the file. Keep all existing models untouched."
    },
    {
      "id": "US-002",
      "title": "Add ToolResultEvent type",
      "description": "As a developer, I want a ToolResultEvent emitted after each tool execution so that my frontend can display tool results with success/failure status.",
      "acceptanceCriteria": [
        "ToolResultEvent added to orbiter/types.py with fields: type: Literal['tool_result'], tool_name: str, tool_call_id: str, arguments: dict[str, Any], result: str, error: str | None, success: bool, duration_ms: float, agent_name: str",
        "StreamEvent union updated to include ToolResultEvent",
        "Serializable to JSON via .model_dump()",
        "All existing tests pass",
        "Pyright passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Add to packages/orbiter-core/src/orbiter/types.py alongside StepEvent. Same frozen BaseModel pattern. arguments field uses dict[str, Any] for arbitrary tool args."
    },
    {
      "id": "US-003",
      "title": "Add ReasoningEvent, ErrorEvent, StatusEvent, UsageEvent types",
      "description": "As a developer, I want additional event types for reasoning content, errors, status changes, and per-step usage so that my frontend has full visibility into agent execution.",
      "acceptanceCriteria": [
        "ReasoningEvent with fields: type: Literal['reasoning'], text: str, agent_name: str",
        "ErrorEvent with fields: type: Literal['error'], error: str, error_type: str, agent_name: str, step_number: int | None, recoverable: bool",
        "StatusEvent with fields: type: Literal['status'], status: Literal['starting', 'running', 'waiting_for_tool', 'completed', 'cancelled', 'error'], agent_name: str, message: str",
        "UsageEvent with fields: type: Literal['usage'], usage: Usage, agent_name: str, step_number: int, model: str",
        "All four added to StreamEvent union",
        "All existing tests pass",
        "Pyright passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Add all four to packages/orbiter-core/src/orbiter/types.py. All are frozen BaseModel. StreamEvent union becomes: TextEvent | ToolCallEvent | StepEvent | ToolResultEvent | ReasoningEvent | ErrorEvent | StatusEvent | UsageEvent. Add tests for serialization of each new type."
    },
    {
      "id": "US-004",
      "title": "Emit rich events from run.stream()",
      "description": "As a developer, I want run.stream() to accept a detailed=True parameter that enables emission of all rich event types during agent execution.",
      "acceptanceCriteria": [
        "run.stream() signature updated: async def _stream(agent, input, *, messages=None, provider=None, max_steps=None, detailed=False) -> AsyncIterator[StreamEvent]",
        "When detailed=False (default): only TextEvent and ToolCallEvent emitted (backward compatible)",
        "When detailed=True: emits StatusEvent('starting') at start, StepEvent(status='started') before each LLM call, UsageEvent after each LLM call, ToolResultEvent after each tool execution, StepEvent(status='completed') at step end, StatusEvent('completed') at finish",
        "ErrorEvent emitted on errors regardless of detailed flag",
        "ReasoningEvent emitted when model returns reasoning content (if supported by provider)",
        "Tests verify both detailed=False and detailed=True behavior",
        "Pyright passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Modify packages/orbiter-core/src/orbiter/runner.py _stream() function. The existing streaming logic accumulates text deltas and tool call deltas — wrap each loop iteration with StepEvent start/end and add UsageEvent/ToolResultEvent yields when detailed=True. Use time.time() for started_at/completed_at. ErrorEvent should be yielded in except blocks."
    },
    {
      "id": "US-005",
      "title": "Emit rich events for Swarm streaming",
      "description": "As a developer, I want Swarm execution to support detailed=True streaming with events that identify which sub-agent produced each event.",
      "acceptanceCriteria": [
        "Swarm gets a stream() method (or run.stream() detects Swarm and delegates) accepting detailed=True",
        "All events include the correct agent_name of the sub-agent that produced them",
        "StatusEvent emitted for each agent handoff/delegation with the new agent name",
        "Workflow, handoff, and team modes all emit rich events",
        "Tests verify multi-agent streaming events",
        "Pyright passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Modify packages/orbiter-core/src/orbiter/swarm.py to add streaming support. run.stream() in runner.py already detects Swarm via hasattr(agent, 'flow_order') — add stream delegation similar to how run() delegates to agent.run(). Each mode (workflow/handoff/team) needs a streaming variant that yields events with correct agent_name."
    },
    {
      "id": "US-006",
      "title": "Event filtering and subscription",
      "description": "As a developer, I want to filter which event types I receive from the stream so I can subscribe only to events my application cares about.",
      "acceptanceCriteria": [
        "run.stream() accepts optional event_types: set[str] | None parameter",
        "When event_types is provided, only events whose type field matches are yielded",
        "When event_types is None (default), all events pass through (respecting detailed flag)",
        "Example: event_types={'text', 'tool_result'} yields only TextEvent and ToolResultEvent",
        "Tests verify filtering works correctly",
        "Pyright passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "Small change to runner.py _stream() — add event_types parameter and a simple filter before each yield. Also add to Swarm stream method if created in US-005."
    },
    {
      "id": "US-007",
      "title": "Create orbiter-distributed package scaffold",
      "description": "As a developer, I need the orbiter-distributed package set up in the monorepo with proper namespace packaging and dependencies.",
      "acceptanceCriteria": [
        "Package at packages/orbiter-distributed/ with pyproject.toml following existing patterns (hatchling, src/orbiter/ layout)",
        "Namespace package: orbiter.distributed using pkgutil.extend_path()",
        "Dependencies: orbiter-core, redis[hiredis]>=5.0",
        "Optional dependencies: [temporal] with temporalio>=1.7, [test] with pytest, pytest-asyncio, fakeredis[lua]",
        "Added to root pyproject.toml workspace members, dev dependencies, and tool.uv.sources",
        "__init__.py with __all__ exposing public API placeholders",
        "README.md exists (required by hatchling)",
        "Package installs cleanly with uv sync",
        "Pyright passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Follow exact same pyproject.toml pattern as packages/orbiter-observability/pyproject.toml. Use hatchling with packages = ['src/orbiter']. temporalio is optional dep group [temporal], not in base deps. Add fakeredis[lua] to test deps for Redis Stream support. Pattern: orbiter/__init__.py has pkgutil.extend_path(), orbiter/distributed/__init__.py is the sub-namespace."
    },
    {
      "id": "US-009",
      "title": "TaskPayload and TaskStatus models",
      "description": "As a developer, I need data models that define the task payload (what gets queued) and task status (tracking progress).",
      "acceptanceCriteria": [
        "TaskPayload Pydantic model in orbiter/distributed/models.py with fields: task_id (str, auto-generated UUID), agent_config (dict[str, Any]), input (str), messages (list[dict[str, Any]]), model (str | None), detailed (bool, default False), metadata (dict[str, Any]), created_at (float), timeout_seconds (float, default 300.0)",
        "TaskStatus StrEnum: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, RETRYING",
        "TaskResult Pydantic model: task_id (str), status (TaskStatus), result (dict[str, Any] | None), error (str | None), started_at (float | None), completed_at (float | None), worker_id (str | None), retries (int)",
        "All models JSON-serializable via .model_dump()",
        "Tests verify serialization round-trips",
        "Pyright passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/models.py. Use StrEnum (not str, Enum) per ruff UP042. Use uuid4().hex for task_id default_factory. Use frozen=True on Pydantic models where appropriate (TaskPayload and TaskResult are data carriers)."
    },
    {
      "id": "US-008",
      "title": "Redis task queue — TaskBroker",
      "description": "As a developer, I need a Redis-backed task broker that can enqueue agent execution tasks and distribute them to workers.",
      "acceptanceCriteria": [
        "TaskBroker class in orbiter/distributed/broker.py",
        "Constructor: TaskBroker(redis_url: str, *, queue_name: str = 'orbiter:tasks', max_retries: int = 3)",
        "async connect() and async disconnect() for lifecycle management",
        "async submit(task: TaskPayload) -> str enqueues task, returns task_id",
        "async claim(worker_id: str, timeout: float = 5.0) -> TaskPayload | None pops task from queue (blocking with timeout)",
        "async ack(task_id: str) -> None acknowledges task completion",
        "async nack(task_id: str) -> None returns task to queue for retry",
        "Uses Redis Streams for durable task queue (XADD/XREADGROUP/XACK)",
        "Consumer groups for multiple workers",
        "Tests with fakeredis",
        "Pyright passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/broker.py. Use redis.asyncio for async Redis client. Redis Streams pattern: XADD to submit, XREADGROUP with consumer group for claim, XACK for ack. Create consumer group on connect() with XGROUP CREATE (use MKSTREAM). Use fakeredis[lua] for tests — it supports Streams."
    },
    {
      "id": "US-010",
      "title": "Task state store",
      "description": "As a developer, I need a Redis-backed store that tracks task status, so callers can query task progress and workers can update state.",
      "acceptanceCriteria": [
        "TaskStore class in orbiter/distributed/store.py",
        "Constructor: TaskStore(redis_url: str, *, prefix: str = 'orbiter:task:', ttl_seconds: int = 86400)",
        "async set_status(task_id: str, status: TaskStatus, **kwargs) -> None updates task state in Redis hash",
        "async get_status(task_id: str) -> TaskResult | None retrieves current task state",
        "async list_tasks(status: TaskStatus | None = None, limit: int = 100) -> list[TaskResult] lists tasks, optionally filtered",
        "Task state stored as Redis hash with TTL for auto-cleanup",
        "Tests with fakeredis",
        "Pyright passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/store.py. Use HSET/HGETALL for hash operations. Use EXPIRE for TTL. For list_tasks, use a secondary index: SADD orbiter:tasks:index task_id on set_status, SMEMBERS + filter for list_tasks. Reuse TaskResult model from US-009."
    },
    {
      "id": "US-015",
      "title": "Agent and Swarm serialization/deserialization",
      "description": "As a developer, I need Agent and Swarm to be serializable to dict (for task payloads) and reconstructable from dict (on worker side).",
      "acceptanceCriteria": [
        "Agent.to_dict() -> dict[str, Any] serializes agent config (name, model, instructions, tool references, handoffs, max_steps, temperature, max_tokens, output_type reference)",
        "Agent.from_dict(data: dict) -> Agent reconstructs agent (tools resolved by name from importable references)",
        "Swarm.to_dict() -> dict[str, Any] serializes swarm config including all agents",
        "Swarm.from_dict(data: dict) -> Swarm reconstructs swarm",
        "Tools serialized as importable dotted paths (e.g., 'myapp.tools.search') and resolved via import on deserialization",
        "Round-trip test: Agent.from_dict(agent.to_dict()) produces functionally equivalent agent",
        "Pyright passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": "Add to_dict/from_dict to packages/orbiter-core/src/orbiter/agent.py and packages/orbiter-core/src/orbiter/swarm.py. For tool serialization: use f'{tool.__module__}.{tool.__qualname__}' for dotted path. For deserialization: use importlib.import_module + getattr. Closure-based tools can't be serialized — raise ValueError with clear message."
    },
    {
      "id": "US-011",
      "title": "Result streaming via Redis Pub/Sub and Streams",
      "description": "As a developer, I want real-time streaming of agent events from worker to caller via Redis Pub/Sub (for live events) and Redis Streams (for persistence/replay).",
      "acceptanceCriteria": [
        "EventPublisher class in orbiter/distributed/events.py",
        "async publish(task_id: str, event: StreamEvent) -> None publishes to both Redis Pub/Sub channel (orbiter:events:{task_id}) and appends to Redis Stream (orbiter:stream:{task_id})",
        "EventSubscriber class with two consumption modes: async subscribe(task_id) -> AsyncIterator[StreamEvent] for live Pub/Sub, async replay(task_id, from_id='0') -> AsyncIterator[StreamEvent] for persistent Stream",
        "Events serialized as JSON, deserialized back to proper StreamEvent subtype via discriminated union on type field",
        "Stream TTL configurable (default 1 hour)",
        "Tests with fakeredis",
        "Pyright passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/events.py. For discriminated union deserialization: parse JSON, read 'type' field, instantiate correct event class. Pub/Sub for live streaming, Streams for replay. Use EXPIRE on stream key for TTL cleanup."
    },
    {
      "id": "US-012",
      "title": "Worker process — task execution loop",
      "description": "As a developer, I need a worker process that claims tasks from the queue, executes agents, publishes streaming events, and updates task status.",
      "acceptanceCriteria": [
        "Worker class in orbiter/distributed/worker.py",
        "Constructor: Worker(redis_url: str, *, worker_id: str | None = None, concurrency: int = 1, queue_name: str = 'orbiter:tasks')",
        "async start() enters claim-execute loop; async stop() gracefully shuts down",
        "On task claim: sets status to RUNNING, reconstructs Agent from agent_config via Agent.from_dict(), calls run.stream(agent, input, detailed=task.detailed), publishes each StreamEvent via EventPublisher, on completion sets status to COMPLETED with RunResult",
        "On failure: sets status to FAILED with error, nacks task if retries remain",
        "Worker generates unique worker_id if not provided (hostname + PID + random suffix)",
        "Heartbeat: worker publishes heartbeat to orbiter:workers:{worker_id} Redis key with TTL (30s default)",
        "Handles SIGINT/SIGTERM for graceful shutdown",
        "Tests verify task execution lifecycle",
        "Pyright passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/worker.py. Depends on TaskBroker (US-008), TaskStore (US-010), EventPublisher (US-011), Agent.from_dict (US-015), and run.stream detailed (US-004). Use asyncio.Event for shutdown signaling. Heartbeat via periodic SET with EX (TTL). For tests, mock the agent execution and verify the full lifecycle: claim -> reconstruct -> execute -> publish events -> update status."
    },
    {
      "id": "US-013",
      "title": "Task cancellation",
      "description": "As a developer, I want to cancel a running distributed task so that the worker stops execution and frees resources.",
      "acceptanceCriteria": [
        "CancellationToken class in orbiter/distributed/cancel.py with cancelled: bool property and cancel() method",
        "TaskBroker.cancel(task_id: str) -> None publishes cancel signal to orbiter:cancel:{task_id} Pub/Sub channel and sets task status to CANCELLED",
        "Worker subscribes to cancel channel for active tasks; on cancel signal, sets CancellationToken.cancelled = True",
        "Agent execution loop checks CancellationToken between steps (cooperative cancellation)",
        "StatusEvent(status='cancelled') emitted when cancellation takes effect",
        "Tests verify cancellation flow",
        "Pyright passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/cancel.py. Add cancel() method to TaskBroker in broker.py. Update Worker to subscribe to cancel channel per active task. Cooperative cancellation: the worker checks token.cancelled before each step in the execution loop."
    },
    {
      "id": "US-014",
      "title": "run.distributed() API",
      "description": "As a developer, I want a run.distributed() function that submits agent execution to the distributed queue and returns a handle for monitoring results.",
      "acceptanceCriteria": [
        "distributed() function in orbiter/distributed/client.py with signature: async def distributed(agent, input, *, messages=None, redis_url=None, detailed=False, timeout=300.0, metadata=None) -> TaskHandle",
        "TaskHandle class with: task_id property, async result() -> RunResult blocks until completion, async stream() -> AsyncIterator[StreamEvent] subscribes to live events, async cancel() cancels the task, async status() -> TaskResult returns current status",
        "redis_url defaults to ORBITER_REDIS_URL environment variable",
        "Agent/Swarm serialized to TaskPayload.agent_config via to_dict()",
        "Works with both Agent and Swarm instances",
        "Tests verify submit -> execute -> result flow",
        "Pyright passes"
      ],
      "priority": 15,
      "passes": false,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/client.py. TaskHandle wraps TaskBroker, TaskStore, EventSubscriber. result() polls TaskStore until COMPLETED/FAILED. stream() delegates to EventSubscriber.subscribe(). cancel() delegates to TaskBroker.cancel(). This is the main public API for distributed execution."
    },
    {
      "id": "US-016",
      "title": "CLI command — orbiter start worker",
      "description": "As an operator, I want to start a distributed worker via CLI so I can scale agent execution horizontally.",
      "acceptanceCriteria": [
        "orbiter start worker command added to orbiter-cli via Typer subcommand",
        "Options: --redis-url (default: ORBITER_REDIS_URL env var), --concurrency (default: 1), --queue (default: orbiter:tasks), --worker-id (auto-generated if not set)",
        "Starts Worker instance and runs until SIGINT/SIGTERM",
        "Prints startup banner with worker ID, Redis URL (masked), queue name, concurrency",
        "orbiter-cli adds orbiter-distributed as a dependency in pyproject.toml",
        "Tests verify CLI command registration",
        "Pyright passes"
      ],
      "priority": 16,
      "passes": false,
      "notes": "Add to packages/orbiter-cli/src/orbiter_cli/main.py. Use Typer subcommand group: start = typer.Typer(), app.add_typer(start, name='start'). Mask Redis URL in banner: show host only. Use asyncio.run(worker.start()) to run the worker."
    },
    {
      "id": "US-017",
      "title": "CLI command — orbiter task status/cancel/list",
      "description": "As an operator, I want CLI commands to inspect and manage distributed tasks.",
      "acceptanceCriteria": [
        "orbiter task status <task_id> shows task status, worker, timing, result preview",
        "orbiter task cancel <task_id> cancels a running task",
        "orbiter task list lists recent tasks with status, optionally filtered by --status",
        "Output formatted with Rich tables",
        "--redis-url option (defaults to ORBITER_REDIS_URL)",
        "Tests verify CLI commands",
        "Pyright passes"
      ],
      "priority": 17,
      "passes": false,
      "notes": "Add task subcommand group to packages/orbiter-cli/src/orbiter_cli/main.py. Uses TaskStore for status/list and TaskBroker for cancel. Rich tables for formatted output (rich is already a dependency of typer)."
    },
    {
      "id": "US-019",
      "title": "Worker health checks and monitoring endpoint",
      "description": "As an operator, I want workers to expose health information so I can monitor fleet health.",
      "acceptanceCriteria": [
        "Worker publishes health data to orbiter:workers:{worker_id} Redis hash: status, tasks_processed, tasks_failed, current_task_id, started_at, last_heartbeat, concurrency, hostname",
        "WorkerHealthCheck class implementing orbiter.observability.health.HealthCheck protocol",
        "async get_worker_fleet_status(redis_url) -> list[WorkerHealth] function returns all active workers",
        "Workers with expired heartbeat TTL (>60s) considered dead",
        "CLI: orbiter worker list shows all workers and their health",
        "Tests verify health data publishing",
        "Pyright passes"
      ],
      "priority": 18,
      "passes": false,
      "notes": "Add health data publishing to Worker class from US-012. Create WorkerHealth dataclass and get_worker_fleet_status() in orbiter/distributed/health.py. Add 'orbiter worker list' CLI command. Uses orbiter.observability.health.HealthCheck protocol."
    },
    {
      "id": "US-020",
      "title": "Distributed task metrics",
      "description": "As an operator, I want metrics for distributed task processing so I can monitor queue health and worker performance.",
      "acceptanceCriteria": [
        "New metric constants in orbiter/observability/semconv.py: METRIC_DIST_TASKS_SUBMITTED, METRIC_DIST_TASKS_COMPLETED, METRIC_DIST_TASKS_FAILED, METRIC_DIST_TASKS_CANCELLED, METRIC_DIST_QUEUE_DEPTH, METRIC_DIST_TASK_DURATION, METRIC_DIST_TASK_WAIT_TIME",
        "New semantic convention attributes: DIST_TASK_ID, DIST_WORKER_ID, DIST_QUEUE_NAME, DIST_TASK_STATUS",
        "record_task_submitted(), record_task_completed(), record_task_failed() helper functions in orbiter/distributed/metrics.py",
        "Metrics recorded using existing orbiter.observability.metrics infrastructure (both OTel and in-memory fallback)",
        "Worker automatically records metrics during task lifecycle",
        "Tests verify metrics are recorded",
        "Pyright passes"
      ],
      "priority": 19,
      "passes": false,
      "notes": "Add constants to packages/orbiter-observability/src/orbiter/observability/semconv.py following existing patterns. Create packages/orbiter-distributed/src/orbiter/distributed/metrics.py with recording helpers that use orbiter.observability.metrics. Update Worker to call recording helpers."
    },
    {
      "id": "US-021",
      "title": "Distributed task tracing",
      "description": "As an operator, I want distributed traces that span from task submission through queue to worker execution so I can trace requests end-to-end.",
      "acceptanceCriteria": [
        "Task submission creates a trace span orbiter.distributed.submit with task_id, agent_name attributes",
        "Trace context propagated in TaskPayload.metadata['trace_context'] using W3C baggage from orbiter.observability.propagation",
        "Worker extracts trace context and creates child span orbiter.distributed.execute linked to submission span",
        "Tool executions within worker create nested spans (existing @traced decorator)",
        "Uses existing orbiter.observability.tracing infrastructure (aspan, traced)",
        "Tests verify trace context propagation",
        "Pyright passes"
      ],
      "priority": 20,
      "passes": false,
      "notes": "Update client.py (run.distributed) to create submission span and inject trace context into metadata. Update worker.py to extract trace context and create execution span. Uses BaggagePropagator from orbiter.observability.propagation and aspan from orbiter.observability.tracing."
    },
    {
      "id": "US-022",
      "title": "Streaming event metrics",
      "description": "As an operator, I want metrics on streaming events so I can monitor event throughput and latency.",
      "acceptanceCriteria": [
        "New metric constants: METRIC_STREAM_EVENTS_EMITTED, METRIC_STREAM_EVENT_PUBLISH_DURATION",
        "New attribute: STREAM_EVENT_TYPE for event type breakdown",
        "EventPublisher records metrics on each publish (event count by type, publish duration)",
        "run.stream() with detailed=True records total events emitted per run",
        "Metrics recorded using existing infrastructure",
        "Tests verify event metrics",
        "Pyright passes"
      ],
      "priority": 21,
      "passes": false,
      "notes": "Add constants to semconv.py. Update EventPublisher in events.py to record metrics via orbiter.observability.metrics. Update runner.py _stream() to count and record total events."
    },
    {
      "id": "US-023",
      "title": "Distributed alert rules",
      "description": "As an operator, I want pre-defined alert rules for distributed system health so I get notified of problems.",
      "acceptanceCriteria": [
        "register_distributed_alerts() function in orbiter/distributed/alerts.py",
        "Pre-defined alert rules: queue depth > 100 (WARNING), queue depth > 500 (CRITICAL), task failure rate > 10% (WARNING), worker count = 0 (CRITICAL), task wait time > 60s (WARNING)",
        "Uses existing orbiter.observability.alerts.AlertManager and AlertRule",
        "Alert rules registered with global alert manager",
        "Tests verify alert rules trigger correctly",
        "Pyright passes"
      ],
      "priority": 22,
      "passes": false,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/alerts.py. Uses AlertManager, AlertRule, AlertSeverity, Comparator from orbiter.observability.alerts. Register rules for distributed metric names from US-020."
    },
    {
      "id": "US-018",
      "title": "Temporal integration for durable execution",
      "description": "As a developer, I want optional Temporal integration for durable workflow execution, so tasks survive worker crashes and can be retried with full state recovery.",
      "acceptanceCriteria": [
        "TemporalExecutor class in orbiter/distributed/temporal.py as an alternative execution backend",
        "Temporal workflow wraps agent execution: receives TaskPayload, executes agent in an activity, publishes events",
        "Temporal activity runs run.stream() with heartbeating (Temporal activity heartbeat)",
        "Worker accepts executor: Literal['local', 'temporal'] = 'local' parameter",
        "When executor='temporal': worker registers Temporal workflows/activities instead of direct execution",
        "Temporal connection configured via TEMPORAL_HOST and TEMPORAL_NAMESPACE env vars",
        "temporalio is an optional dependency (not required for Redis-only mode)",
        "Graceful degradation: if temporalio not installed and executor='temporal' requested, raise clear error",
        "Tests for Temporal workflow/activity logic (mocked Temporal client)",
        "Pyright passes"
      ],
      "priority": 23,
      "passes": false,
      "notes": "Create packages/orbiter-distributed/src/orbiter/distributed/temporal.py. temporalio is in [temporal] optional deps from US-007. Use try/except ImportError with HAS_TEMPORAL flag pattern (same as OTel in orbiter-observability). Update Worker to accept executor param. Temporal workflow is a @workflow.defn class, activity is a @activity.defn function."
    },
    {
      "id": "US-024",
      "title": "Package README for orbiter-distributed",
      "description": "As a developer, I want a README in the orbiter-distributed package that explains what it does and shows basic usage.",
      "acceptanceCriteria": [
        "packages/orbiter-distributed/README.md with: package description, installation, quick start (5-minute guide), basic run.distributed() example, worker startup command, link to full docs",
        "Covers both Agent and Swarm usage",
        "Shows environment variable configuration"
      ],
      "priority": 24,
      "passes": false,
      "notes": "Update the placeholder README.md created in US-007 with full content. Include code examples for: basic agent distributed execution, swarm distributed execution, worker startup via CLI, environment variables (ORBITER_REDIS_URL, TEMPORAL_HOST, TEMPORAL_NAMESPACE)."
    },
    {
      "id": "US-025",
      "title": "Architecture documentation",
      "description": "As a developer, I want architecture docs explaining how distributed execution components connect.",
      "acceptanceCriteria": [
        "docs/distributed/architecture.md with: system architecture diagram (Mermaid), component descriptions (TaskBroker, Worker, EventPublisher, TaskStore), data flow (submit -> queue -> worker -> events -> caller), Redis data structures used, Temporal integration explanation",
        "Covers both Redis-only and Redis+Temporal modes"
      ],
      "priority": 25,
      "passes": false,
      "notes": "Create docs/distributed/ directory. Use Mermaid diagrams for architecture visualization. Document all Redis keys/streams/channels used by the system."
    },
    {
      "id": "US-026",
      "title": "Worker setup and scaling guide",
      "description": "As an operator, I want a guide on configuring and scaling workers.",
      "acceptanceCriteria": [
        "docs/distributed/workers.md with: worker startup options, concurrency tuning, multiple worker deployment, environment variables reference, Redis configuration recommendations, health monitoring setup",
        "Includes Docker and docker-compose examples for worker deployment"
      ],
      "priority": 26,
      "passes": false,
      "notes": "Create docs/distributed/workers.md. Include docker-compose.yml snippet with Redis + workers. Document all CLI options and env vars."
    },
    {
      "id": "US-027",
      "title": "Migration guide — local to distributed",
      "description": "As a developer, I want a guide showing how to move from local run() to distributed run.distributed().",
      "acceptanceCriteria": [
        "docs/distributed/migration.md with: side-by-side comparison of local vs distributed code, step-by-step migration walkthrough, handling streaming events in both modes, error handling differences, when to use local vs distributed"
      ],
      "priority": 27,
      "passes": false,
      "notes": "Create docs/distributed/migration.md. Focus on the 2-line difference between local run() and run.distributed(). Show event consumption patterns for both modes."
    },
    {
      "id": "US-028",
      "title": "Rich streaming events documentation",
      "description": "As a developer, I want documentation for all streaming event types and how to use them.",
      "acceptanceCriteria": [
        "docs/streaming-events.md with: all event types with field descriptions, detailed=True usage guide, event filtering examples, SSE integration example (FastAPI/Django), frontend consumption patterns, migration from AWorld output types"
      ],
      "priority": 28,
      "passes": false,
      "notes": "Create docs/streaming-events.md. Document all 8 event types: TextEvent, ToolCallEvent, StepEvent, ToolResultEvent, ReasoningEvent, ErrorEvent, StatusEvent, UsageEvent. Include FastAPI SSE example."
    },
    {
      "id": "US-029",
      "title": "Example applications",
      "description": "As a developer, I want example applications demonstrating distributed execution.",
      "acceptanceCriteria": [
        "examples/distributed/simple_chat.py — simple chatbot agent running distributed with result streaming",
        "examples/distributed/multi_agent.py — Swarm with multiple agents running distributed",
        "examples/distributed/docker-compose.yml — Docker Compose with Redis, Temporal (optional), and 2 workers",
        "examples/distributed/fastapi_sse.py — FastAPI endpoint that submits distributed task and streams SSE events to frontend",
        "Each example has inline comments explaining the code"
      ],
      "priority": 29,
      "passes": false,
      "notes": "Create examples/distributed/ directory. Examples should be runnable with uv run. docker-compose.yml should include Redis 7+, optional Temporal, and worker containers."
    }
  ]
}
