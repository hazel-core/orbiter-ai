{
  "project": "Orbiter",
  "branchName": "ralph/orbiter-integration-tests",
  "description": "Intensive integration test suite for the Orbiter monorepo — 31 user stories covering all 14 packages using real Vertex AI (gemini-2.0-flash), real Redis (Docker), real Uvicorn, and real stdio MCP subprocesses across four tiers: two-package seams, three-package chains, full-stack scenarios, and marathon multi-feature runs.",
  "userStories": [
    {
      "id": "US-INT-001",
      "title": "Integration test infrastructure setup",
      "description": "As a developer, I need the test infrastructure wired up so all 30 integration tests can run against real dependencies.",
      "acceptanceCriteria": [
        "Create tests/integration/ directory at monorepo root with __init__.py",
        "Create tests/integration/conftest.py with all fixtures: vertex_model (session, skips if GOOGLE_CLOUD_PROJECT or GOOGLE_CLOUD_LOCATION absent), tmp_sqlite_db (function, tempfile.mkstemp + os.unlink teardown), memory_store (function, fresh SQLiteMemoryStore), vector_store (function, fresh ChromaVectorMemoryStore in tempfile.mkdtemp + shutil.rmtree teardown), mcp_server_process (session, launches mcp_test_server.py subprocess via sys.executable, yields MCPServerConfig with stdio transport, kills on teardown), uvicorn_server (session, subprocess.Popen uvicorn on port 8765, polls GET /health until 200 within 15s, kills on teardown), redis_container (session, docker run redis:7-alpine on port 6380, pytest.skip if Docker unavailable, docker stop teardown), http_client (function, httpx.AsyncClient base_url http://localhost:8765, closes on teardown)",
        "Create tests/integration/helpers/mcp_test_server.py with a @mcp_server() class exposing get_capital(country: str) -> str and get_population(city: str) -> str tools, runnable standalone via python mcp_test_server.py",
        "Update root pyproject.toml testpaths to include 'tests/integration'",
        "Update root pyproject.toml [dependency-groups].dev to add pytest-timeout, tenacity, docker",
        "Add markers section to pytest.ini_options registering 'integration' and 'marathon'",
        "Run uv lock to update lockfile",
        "pytest tests/integration/ --collect-only runs without import errors",
        "Ruff lint passes on all new files",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-002",
      "title": "Agent + memory persistence seam test",
      "description": "As a developer, I need a test that verifies an agent's conversation history persists to SQLite and is correctly loaded by a new agent instance on the next run.",
      "acceptanceCriteria": [
        "Create tests/integration/test_agent_memory.py",
        "test_memory_persists_across_agent_instances: Agent with SQLiteMemoryStore and conversation_id='test-session-001' runs with prompt about Antananarivo; new Agent instance with same SQLite file and conversation_id runs follow-up; output_type=CityResponse(city: str); assert result.output.city.lower() == 'antananarivo'",
        "test_memory_metadata_scoping: two agents with different user_id metadata and same session_id; assert each agent's stored messages only visible to their own user_id query",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-003",
      "title": "Agent + context windowing seam test",
      "description": "As a developer, I need a test that verifies the context engine correctly bounds message history when history_rounds is configured.",
      "acceptanceCriteria": [
        "Create tests/integration/test_agent_context.py",
        "test_history_rounds_bounds_messages: Agent with ContextConfig(mode='copilot', history_rounds=2) and max_steps=1; run 6 sequential turns; assert len(result.messages) <= history_rounds * 2 + 2 after final turn",
        "test_context_mode_pilot_disables_windowing: Agent with ContextConfig(mode='pilot'); run 4 sequential turns; assert all prior messages still present (no trimming)",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-004",
      "title": "Agent streaming event ordering test",
      "description": "As a developer, I need a test that verifies agent.stream() emits events in the correct order with no missing or duplicate event types.",
      "acceptanceCriteria": [
        "Create tests/integration/test_agent_streaming.py",
        "test_stream_event_ordering_single_step: Agent with no tools; exhaust agent.stream() into list; assert at least one TextEvent present, last event is UsageEvent, no ErrorEvent present",
        "test_stream_event_ordering_with_tool_call: Agent with one tool and constrained prompt; assert ToolCallEvent appears before ToolResultEvent, TextEvent appears after ToolResultEvent, UsageEvent is last",
        "test_stream_step_events_wrap_each_step: Agent forced to make exactly 2 tool calls; assert StepEvent count >= 2",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-005",
      "title": "Agent multi-tool selection test",
      "description": "As a developer, I need a test that verifies an agent with multiple tools calls the correct tool when given a constrained prompt.",
      "acceptanceCriteria": [
        "Create tests/integration/test_agent_tools.py",
        "test_agent_calls_correct_tool_of_three: Agent with get_weather(city), get_time(timezone), get_currency(country); prompt 'You MUST call ONLY the get_weather tool with city=Dublin. Do not call any other tool.'; assert result.messages contains exactly one ToolCall with name == 'get_weather' and 'Dublin' in arguments",
        "test_agent_chains_two_tools: Agent with get_capital(country) and get_population(city); prompt instructs calling get_capital first then get_population; assert two ToolCall objects in messages with names in correct order",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-006",
      "title": "Structured output validation test",
      "description": "As a developer, I need a test that verifies output_type=PydanticModel produces a fully-populated, correctly-typed response.",
      "acceptanceCriteria": [
        "Create tests/integration/test_agent_structured_output.py",
        "Define class CountryInfo(BaseModel): name: str; capital: str; population_millions: float; continent: str",
        "test_structured_output_all_fields_populated: Agent with output_type=CountryInfo; prompt 'Return structured information about France.'; assert result.output is CountryInfo instance, result.output.capital.lower() == 'paris', result.output.population_millions > 0, result.output.continent is non-empty string",
        "test_structured_output_with_tool_call: Agent with output_type=WeatherReport(city: str, temperature_celsius: float, condition: str) and a get_raw_weather(city) tool; constrained prompt forces tool call then structured output; assert all 3 fields populated with correct types",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-007",
      "title": "Memory persistence write/read/search test",
      "description": "As a developer, I need a test that verifies SQLiteMemoryStore correctly persists, retrieves, and searches items across different query patterns.",
      "acceptanceCriteria": [
        "Create tests/integration/test_memory_persistence.py",
        "test_sqlite_write_then_read_by_session: directly write 5 HumanMemory items via memory_store.save(); read back via memory_store.load(session_id=...); assert all 5 returned",
        "test_sqlite_keyword_search: save items with distinct keywords; call memory_store.search('specific_keyword'); assert matching items returned and non-matching excluded",
        "test_sqlite_metadata_isolation: save items with user_id='user-A' and user_id='user-B'; query with user_id='user-A'; assert only user-A items returned",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-008",
      "title": "Vector memory semantic search test",
      "description": "As a developer, I need a test that verifies ChromaVectorMemoryStore correctly embeds items and retrieves semantically related ones above a similarity threshold.",
      "acceptanceCriteria": [
        "Create tests/integration/test_memory_vector.py",
        "test_chroma_semantic_search_retrieves_relevant: store 5 facts (3 astronomy, 2 cooking); query 'planets and stars' — assert at least 2 astronomy facts returned; query 'recipes and ingredients' — assert at least 1 cooking fact returned",
        "test_chroma_similarity_threshold: store 3 items; query with completely unrelated topic using min_score=0.8; assert 0 results returned",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-009",
      "title": "Context summarization trigger test",
      "description": "As a developer, I need a test that verifies the context engine triggers summarization when the token budget threshold is hit, producing a SummaryMemory item in the store.",
      "acceptanceCriteria": [
        "Create tests/integration/test_context_summarization.py",
        "test_token_budget_triggers_summarization: Agent with ContextConfig(mode='copilot', summary_threshold=4, token_budget_trigger=0.5) and SQLite memory; run 6 turns with verbose prompts; assert at least one SummaryMemory item exists in memory store after 6 runs",
        "test_summarization_shortens_context: run agent to just before and just after summarization trigger; assert len(result.messages) after summarization is less than before",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-010",
      "title": "MCP stdio subprocess integration test",
      "description": "As a developer, I need a test that verifies an agent can connect to a real stdio MCP server subprocess and successfully call its tools.",
      "acceptanceCriteria": [
        "Create tests/integration/test_mcp_stdio.py",
        "Uses mcp_server_process session fixture from US-INT-001",
        "test_agent_calls_mcp_tool_get_capital: agent loads MCP server via await agent.add_mcp_server(mcp_server_process); prompt 'You MUST call the get_capital tool with country=Japan. Do not answer from memory.'; assert ToolCall with name == 'get_capital' in messages; assert 'tokyo' appears case-insensitively in result.output",
        "test_agent_calls_both_mcp_tools_sequentially: constrained prompt forces both get_capital and get_population calls; assert two ToolCall objects in messages with correct names",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-011",
      "title": "Agent handoffs test",
      "description": "As a developer, I need a test that verifies Agent A correctly delegates to Agent B via a handoff and the final output reflects Agent B's work.",
      "acceptanceCriteria": [
        "Create tests/integration/test_handoffs.py",
        "test_handoff_from_agent_a_to_agent_b: Agent B instructions='You are a poet. When asked anything, respond with a haiku.'; Agent A has B in handoffs with instructions to route poetry requests to the poet; prompt 'Write me a haiku about mountains.'; assert result.output non-empty and has roughly 3 lines; assert at least one ToolCall or handoff event in messages indicating delegation",
        "test_handoff_result_is_from_target_agent: Agent B with output_type=HandoffResponse(handled_by: str, content: str); assert result.output.handled_by contains agent B's name",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-012",
      "title": "Agent self-spawn test",
      "description": "As a developer, I need a test that verifies a parent agent can spawn child agents for parallel subtasks and aggregate their results.",
      "acceptanceCriteria": [
        "Create tests/integration/test_self_spawn.py",
        "test_parent_spawns_two_children_and_aggregates: Agent with allow_self_spawn=True, max_spawn_depth=2; spawn_subtask(task_description: str) tool creates and runs child Agent; prompt forces spawn for capitals of Australia and Brazil; assert result.output contains both 'canberra' and 'brasilia' case-insensitively; assert two spawn-related tool calls in messages",
        "test_spawn_depth_limit_enforced: Agent attempts to spawn beyond max_spawn_depth=1; assert ErrorEvent or exception raised, no infinite recursion",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-013",
      "title": "Dynamic tool loading test",
      "description": "As a developer, I need a test that verifies add_tool() after agent initialization makes the tool immediately available, and concurrent add_tool() calls are safe.",
      "acceptanceCriteria": [
        "Create tests/integration/test_dynamic_loading.py",
        "test_add_tool_after_init_available_immediately: Agent initialized with zero tools; await agent.add_tool(get_joke_tool); run with constrained prompt; assert ToolCall with name == 'get_joke' in messages",
        "test_remove_tool_unavailable_after_removal: Agent with one tool; agent.remove_tool('get_joke'); run with explicit instruction to use that tool; assert no ToolCall with that name in messages",
        "test_concurrent_add_tool_no_race_condition: 10 concurrent asyncio.gather calls each adding a uniquely-named tool; assert all 10 tools present in agent.tools after gather, no asyncio.Lock errors",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-014",
      "title": "Memory + context three-package chain test",
      "description": "As a developer, I need a test that verifies the full chain: long run triggers summarization stored to SQLite, vector search finds the summary, and a new run's KnowledgeNeuron injects it.",
      "acceptanceCriteria": [
        "Create tests/integration/test_memory_context_chain.py",
        "test_summary_stored_and_retrieved_by_vector_search: Agent with SQLite + Chroma + summarization configured; run 6 verbose turns until summarization fires; confirm SummaryMemory item created in SQLite; perform vector search on Chroma with query related to conversation topic; assert summary content appears in search results",
        "test_knowledge_neuron_injects_summary_into_next_run: new Agent instance loads same stores; run with prompt asking about conversation topic; assert KnowledgeNeuron fired (via system message or structured output field); agent gives correct answer using injected context",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-015",
      "title": "MCP large output workspace offload test",
      "description": "As a developer, I need a test that verifies an MCP tool returning a large blob triggers workspace offload and the agent correctly calls retrieve_artifact to access the content.",
      "acceptanceCriteria": [
        "Create tests/integration/test_mcp_large_output.py",
        "Add get_large_dataset(topic: str) tool to mcp_test_server.py returning exactly 15KB string (over 10KB default threshold)",
        "test_large_mcp_output_offloaded_to_workspace: MCPServerConfig with large_output_tools=['get_large_dataset']; constrained prompt forces get_large_dataset call; assert workspace has one artifact created; assert ToolCall named 'retrieve_artifact' in agent messages",
        "test_retrieved_artifact_content_in_final_output: output_type=DataSummary(contains_keyword: bool) with known keyword in dataset; assert result.output.contains_keyword == True",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 15,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-INT-016",
      "title": "MCP progress events in stream test",
      "description": "As a developer, I need a test that verifies MCPProgressEvent items appear in the stream during MCP tool execution, before the ToolResultEvent.",
      "acceptanceCriteria": [
        "Create tests/integration/test_mcp_progress_stream.py",
        "Add long_running_task(steps: int) tool to mcp_test_server.py that emits 'steps' progress notifications before returning",
        "test_mcp_progress_events_fire_before_tool_result: agent streams with MCP server attached; constrained prompt forces long_running_task(steps=3) call; collect all stream events in order; assert at least 1 MCPProgressEvent present; assert all MCPProgressEvent items appear before the corresponding ToolResultEvent",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 16,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-017",
      "title": "Handoff chain with memory persistence test",
      "description": "As a developer, I need a test that verifies a three-agent handoff chain (A→B→C) persists messages from all three agents to the shared memory store.",
      "acceptanceCriteria": [
        "Create tests/integration/test_handoff_memory.py",
        "test_three_agent_chain_all_messages_persisted: Agent C answers factual questions; Agent B routes math questions to C; Agent A routes all questions to B; shared SQLiteMemoryStore passed to all three; prompt to A triggers full A→B→C chain; after run, query memory store for all items; assert messages from all 3 agents present identified by agent_id metadata",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 17,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-018",
      "title": "Conversation branching isolation test",
      "description": "As a developer, I need a test that verifies agent.branch() creates isolated conversation histories where changes in one branch do not affect the other.",
      "acceptanceCriteria": [
        "Create tests/integration/test_branching_isolation.py",
        "test_branch_histories_are_independent: agent runs 3 turns; call branch_id_a = await agent.branch(from_message_id) and branch_id_b = await agent.branch(same from_message_id); run 2 more turns in branch A with topic X; run 2 more turns in branch B with topic Y; query memory with task_id=branch_id_a — assert only topic X messages; query memory with task_id=branch_id_b — assert only topic Y messages; no cross-contamination",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 18,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-019",
      "title": "Context vector injection via KnowledgeNeuron test",
      "description": "As a developer, I need a test that verifies vector-stored facts are injected into a new agent's context via KnowledgeNeuron and correctly influence the response.",
      "acceptanceCriteria": [
        "Create tests/integration/test_context_vector_injection.py",
        "test_knowledge_neuron_enables_correct_answer: save 5 obscure non-general-knowledge facts to ChromaVectorMemoryStore (e.g. 'The Orbiter framework was founded in 2024 in Auckland.'); create new Agent with ContextConfig(mode='copilot') and Chroma store as knowledge source; prompt 'Where was the Orbiter framework founded? Respond with just the city name.'; output_type=CityResponse(city: str); assert result.output.city.lower() == 'auckland'",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 19,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-020",
      "title": "Spawn memory isolation test",
      "description": "As a developer, I need a test that verifies memory items written by child agent A are not visible to child agent B when queried by their respective conversation_id.",
      "acceptanceCriteria": [
        "Create tests/integration/test_spawn_memory_isolation.py",
        "test_child_agents_have_isolated_memory: shared SQLiteMemoryStore; child A runs with conversation_id='child-a-session' and stores a unique fact; child B queries memory with conversation_id='child-b-session' — assert child A's fact NOT returned; child A queries with its own session — assert its fact IS returned",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 20,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-021",
      "title": "Structured output with multi-tool chain test",
      "description": "As a developer, I need a test that verifies output_type fields are correctly populated even after multiple tool calls are required to gather the necessary information.",
      "acceptanceCriteria": [
        "Create tests/integration/test_structured_with_tools.py",
        "Define class TravelReport(BaseModel): origin_city: str; destination_city: str; distance_km: float; travel_tip: str",
        "Agent has get_distance(city1, city2) and get_travel_tip(destination) tools",
        "test_structured_output_after_two_tool_calls: constrained prompt forces both tool calls then structured output; assert all 4 fields populated, distance_km > 0, origin_city and destination_city are non-empty strings; assert exactly 2 ToolCall objects in messages",
        "@pytest.mark.integration and @pytest.mark.timeout(30) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 21,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-022",
      "title": "Web API agent run via REST test",
      "description": "As a developer, I need a test that verifies the full web API flow: register user → login → create agent → run agent → get response.",
      "acceptanceCriteria": [
        "Create tests/integration/test_web_api_agent.py",
        "Uses uvicorn_server and http_client fixtures",
        "test_full_rest_agent_run_flow: POST /api/auth/register → 201; POST /api/auth/login → JWT token; POST /api/agents with vertex:gemini-2.0-flash config → agent id; POST /api/agents/{id}/run with input 'What is 2+2? Respond with just the number.' → 200; assert response JSON has output field containing '4'",
        "test_agent_run_returns_usage_stats: same flow; assert response JSON has usage.total_tokens > 0",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 22,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-023",
      "title": "Web API SSE streaming test",
      "description": "As a developer, I need a test that verifies the web API streaming endpoint emits correctly-structured SSE events including text, tool calls, and usage.",
      "acceptanceCriteria": [
        "Create tests/integration/test_web_streaming.py",
        "Uses uvicorn_server and http_client fixtures",
        "test_sse_stream_contains_required_event_types: authenticate; create agent with one tool; POST /api/agents/{id}/stream with constrained prompt; consume SSE stream; parse all data: lines as JSON; assert at least one event with type == 'text'; assert at least one event with type == 'tool_call'; assert last event has type == 'usage' with total_tokens > 0",
        "test_sse_stream_no_error_events: consume stream; assert no event with type == 'error'",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 23,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-024",
      "title": "Web API authentication flow test",
      "description": "As a developer, I need a test that verifies the authentication system correctly issues JWT tokens, protects routes, and rejects invalid tokens.",
      "acceptanceCriteria": [
        "Create tests/integration/test_web_auth_flow.py",
        "Uses uvicorn_server and http_client fixtures",
        "test_valid_jwt_accesses_protected_route: register → login → GET /api/agents with Authorization: Bearer {token} → 200",
        "test_invalid_jwt_rejected: GET /api/agents with Authorization: Bearer garbage.token.here → 401",
        "test_missing_auth_rejected: GET /api/agents with no Authorization header → 401",
        "test_password_wrong_login_rejected: register user; login with wrong password → 401 or 403",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 24,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-025",
      "title": "Distributed worker with Redis test",
      "description": "As a developer, I need a test that verifies a Worker picks up a task from the Redis queue, runs the agent, and stores the result back in Redis.",
      "acceptanceCriteria": [
        "Create tests/integration/test_distributed_worker.py",
        "Uses redis_container fixture",
        "test_worker_executes_task_and_stores_result: create RedisTaskBroker pointing to Docker Redis on port 6380; submit TaskPayload with vertex:gemini-2.0-flash and input 'What is 3+3? Respond with just the number.'; start Worker on same Redis; Worker processes task within asyncio.wait_for 25s timeout; query Redis for result; assert TaskResult.status == 'completed' and result contains '6'",
        "test_worker_handles_task_failure_gracefully: submit task with invalid model config; assert TaskResult.status == 'failed' and error field is non-empty string",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 25,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-026",
      "title": "Evaluation framework test",
      "description": "As a developer, I need a test that verifies the eval framework collects a trajectory from an agent run and produces a valid score from an LLM-as-judge scorer.",
      "acceptanceCriteria": [
        "Create tests/integration/test_eval_framework.py",
        "test_trajectory_collected_after_run: create Evaluator or trajectory collector; run agent with simple factual question; assert trajectory contains at least one step with input, output, and tool_calls fields",
        "test_llm_judge_scorer_returns_valid_score: use LLMJudgeScorer with rubric 'Did the agent answer the question correctly?'; run agent; pass trajectory to scorer; assert returned score is float in [0.0, 1.0]",
        "test_rule_based_scorer_exact_match: ExactMatchScorer(expected='Paris'); agent output containing 'Paris' → score == 1.0; agent output containing 'London' → score == 0.0",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 26,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-027",
      "title": "Agent-to-Agent (A2A) protocol test",
      "description": "As a developer, I need a test that verifies Agent A can delegate a task to Agent B running on a separate HTTP server and use the result.",
      "acceptanceCriteria": [
        "Create tests/integration/test_a2a_protocol.py",
        "Session fixtures spin up two Uvicorn servers on ports 8765 and 8766",
        "test_a2a_agent_delegates_and_receives_result: Agent B registered on port 8766 as translator; Agent A on port 8765 has A2A tool pointing to Agent B endpoint; prompt 'Translate Hello World to Spanish using the translator agent.'; assert result.output contains 'hola' or 'mundo' case-insensitively; assert at least one HTTP tool call to port 8766 in messages",
        "@pytest.mark.integration and @pytest.mark.timeout(60) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 27,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-028",
      "title": "Marathon — research agent multi-step test",
      "description": "As a developer, I need a marathon test that runs a research agent through 8+ steps using 3 tools, SQLite memory, and context windowing, producing a structured report.",
      "acceptanceCriteria": [
        "Create tests/integration/test_research_marathon.py",
        "Define class ResearchReport(BaseModel): topic: str; key_findings: list[str]; word_count: int; sources_consulted: int",
        "Tools: search_topic(query: str) -> str, fetch_detail(topic: str) -> str, count_words(text: str) -> int",
        "test_research_agent_produces_structured_report: Agent with all 3 tools, SQLite memory, context windowing, max_steps=12, output_type=ResearchReport; prompt forces research on Python language history using all tools; assert result.output.topic non-empty, len(result.output.key_findings) >= 3, result.output.word_count > 0, result.output.sources_consulted >= 2; assert at least 5 total tool calls across all steps; assert memory store contains > 10 items after run",
        "@pytest.mark.integration, @pytest.mark.marathon, and @pytest.mark.timeout(180) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 28,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-029",
      "title": "Marathon — multi-agent team test",
      "description": "As a developer, I need a marathon test that runs a 3-agent team with bidirectional handoffs, shared memory, and full streaming, producing a structured final output.",
      "acceptanceCriteria": [
        "Create tests/integration/test_team_marathon.py",
        "Define class TeamReport(BaseModel): coordinator_summary: str; worker_a_contribution: str; worker_b_contribution: str; total_steps: int",
        "Worker A: geography specialist; Worker B: mathematics specialist; Coordinator: routes and aggregates",
        "test_team_produces_aggregated_structured_output: prompt 'Find the capital of Japan AND calculate 17 x 23. Route each to the right specialist and combine the answers.'; stream all events from coordinator; assert at least 2 handoff-related tool calls; assert result.output.worker_a_contribution contains 'tokyo'; assert result.output.worker_b_contribution contains '391'; assert all 3 agents messages in shared SQLite memory store; assert TextEvent, ToolCallEvent, UsageEvent all appear in streamed events",
        "@pytest.mark.integration, @pytest.mark.marathon, and @pytest.mark.timeout(180) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 29,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-030",
      "title": "Marathon — platform end-to-end test",
      "description": "As a developer, I need a marathon test that exercises the full platform stack: Web API → agent with MCP tool (large output) → workspace offload → vector memory → structured REST response.",
      "acceptanceCriteria": [
        "Create tests/integration/test_platform_marathon.py",
        "Uses uvicorn_server, mcp_server_process, http_client, redis_container fixtures",
        "test_full_platform_stack_produces_structured_response: register/login via REST; create agent via REST with MCP config pointing to mcp_server_process with large_output_tools=['get_large_dataset']; POST /api/agents/{id}/run with prompt forcing get_large_dataset call; assert REST response has non-empty output field; assert workspace artifact created; assert retrieve_artifact tool call in response messages; assert vector memory has entries from the run",
        "@pytest.mark.integration, @pytest.mark.marathon, and @pytest.mark.timeout(180) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 30,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-INT-031",
      "title": "Marathon — learning agent across two runs test",
      "description": "As a developer, I need a marathon test that verifies an agent learns facts in run 1, persists them to vector memory, and a fresh agent in run 2 retrieves and correctly uses those facts via KnowledgeNeuron.",
      "acceptanceCriteria": [
        "Create tests/integration/test_learning_agent.py",
        "Define 5 obscure non-general-knowledge facts (fictional specs, invented dates — things the model cannot answer from training data)",
        "Define class FactCheckResult(BaseModel): fact_1_correct: bool; fact_2_correct: bool; fact_3_correct: bool; fact_4_correct: bool; fact_5_correct: bool",
        "test_agent_learns_in_run_1_recalls_in_run_2: Run 1 — Agent with learn_fact(fact: str) tool and shared Chroma store; prompt forces 5 learn_fact calls each embedding a fact; Run 2 — new Agent instance, no tools, same Chroma store as knowledge source, KnowledgeNeuron enabled; prompt asks 5 questions about stored facts; output_type=FactCheckResult; assert at least 4 of 5 fact_N_correct == True",
        "@pytest.mark.integration, @pytest.mark.marathon, and @pytest.mark.timeout(180) on all tests",
        "Ruff lint passes",
        "Typecheck passes"
      ],
      "priority": 31,
      "passes": false,
      "notes": ""
    }
  ]
}
